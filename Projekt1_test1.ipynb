{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1cb878-5da2-4a15-b73a-7dce723709e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def loss_d(y_true, y_pred):\n",
    "    return 2 * y_pred - 2 * y_true\n",
    "\n",
    "\n",
    "def sigm_fun(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigm_d(sigm_val):\n",
    "    return sigm_val * (1 - sigm_val)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return 0 if x < 0 else x\n",
    "\n",
    "\n",
    "def relu_d(relu_val):\n",
    "    return 0 if relu_val <= 0 else 1\n",
    "\n",
    "\n",
    "def softmax(outputs):\n",
    "    exp_outputs = [math.exp(o) for o in outputs]\n",
    "    total = sum(exp_outputs)\n",
    "    return [eo / total for eo in exp_outputs]\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_true = int(y_true - 1)\n",
    "    return -(\n",
    "        y_true * np.log(y_pred + 1e-10) + (1 - y_true) * np.log(1 - y_pred + 1e-10)\n",
    "    )\n",
    "\n",
    "\n",
    "def cross_entropy_loss_d(y_true, y_pred):\n",
    "    y_true = int(y_true - 1)\n",
    "    return y_pred - y_true\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, rng, input_dim, activation_f, activation_f_d):\n",
    "        self.activation_f = activation_f\n",
    "        self.activation_f_d = activation_f_d\n",
    "        weight_list = []\n",
    "        for i in range(input_dim):\n",
    "            weight_list.append(rng.uniform(-1, 1))\n",
    "        self.weights = np.asarray(weight_list)\n",
    "        self.b = rng.uniform(-1, 1)\n",
    "        self.b_grad = 0\n",
    "        self.w_grad = np.zeros(input_dim)\n",
    "        self.out_grad = 0\n",
    "\n",
    "    def work(self, inputs):\n",
    "        self.x = np.array(inputs)\n",
    "        sum = np.sum(inputs * self.weights, dtype=np.float128) + self.b\n",
    "\n",
    "        self.output = self.activation_f(sum)\n",
    "        return self.output\n",
    "\n",
    "    def x_grad(self, i):\n",
    "        return self.out_grad * self.activation_f_d(self.output) * self.weights[i]\n",
    "\n",
    "    def generate_param_grad(self):\n",
    "        self.w_grad = self.out_grad * self.activation_f_d(self.output) * self.x\n",
    "        self.b_grad = self.out_grad * self.activation_f_d(self.output)\n",
    "\n",
    "    def update_weights(self, errors, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * errors[i] * self.x[i]\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(\n",
    "        self,\n",
    "        number_of_neurons_in_layer,\n",
    "        hidden_layers,\n",
    "        input_dim,\n",
    "        number_of_outputs=1,\n",
    "        seed=10,\n",
    "        activation_f=\"sigmoid\",\n",
    "    ):\n",
    "        if activation_f == \"sigmoid\":\n",
    "            act_f = sigm_fun\n",
    "            act_f_d = sigm_d\n",
    "        elif activation_f == \"relu\":\n",
    "            act_f = relu\n",
    "            act_f_d = relu_d\n",
    "        else:\n",
    "            print(\"Allowed activation_function values are {'sigmoid', 'relu'}\")\n",
    "            return\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.layers = []\n",
    "        self.neurons: list[Neuron] = []\n",
    "        layer_1 = [\n",
    "            Neuron(self.rng, input_dim, act_f, act_f_d)\n",
    "            for _ in range(number_of_neurons_in_layer)\n",
    "        ]\n",
    "        self.layers.append(layer_1)\n",
    "        self.neurons.extend(layer_1)\n",
    "        for i in range(hidden_layers - 1):\n",
    "            current_layer = [\n",
    "                Neuron(self.rng, number_of_neurons_in_layer, act_f, act_f_d)\n",
    "                for _ in range(number_of_neurons_in_layer)\n",
    "            ]\n",
    "            self.neurons.extend(current_layer)\n",
    "            self.layers.append(current_layer)\n",
    "        output_layer = []\n",
    "        for i in range(number_of_outputs):\n",
    "            output_neuron = Neuron(\n",
    "                self.rng,\n",
    "                number_of_neurons_in_layer,\n",
    "                activation_f=lambda x: x,\n",
    "                activation_f_d=lambda x: 1,\n",
    "            )\n",
    "            output_layer.append(output_neuron)\n",
    "            self.neurons.append(output_neuron)\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "    def predict_regre(self, inputs):\n",
    "        outputs = inputs.copy()\n",
    "        for layer in self.layers:\n",
    "            outputs = [neuron.work(outputs) for neuron in layer]\n",
    "        return outputs[0]\n",
    "\n",
    "    def predict_class(self, inputs):\n",
    "        outputs = inputs.copy()\n",
    "        for layer in self.layers:\n",
    "            outputs = [neuron.work(outputs) for neuron in layer]\n",
    "        probabilitie = sigm_fun(outputs[0])\n",
    "\n",
    "        return probabilitie\n",
    "\n",
    "    def backprop_single_input(self, input, expected_value):\n",
    "        y_pred = self.predict_regre(input)\n",
    "        error = loss(expected_value, y_pred)\n",
    "        error_d = loss_d(y_pred=y_pred, y_true=expected_value)\n",
    "        output_neuron = self.layers[-1][0]\n",
    "        output_neuron.out_grad = error_d\n",
    "        output_neuron.generate_param_grad()\n",
    "        next_layer = self.layers[-1]\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            for i, neuron in enumerate(layer):\n",
    "                neuron.w_grad = 0\n",
    "                neuron.out_grad = 0\n",
    "                neuron.b_grad = 0\n",
    "                for next_neuron in next_layer:\n",
    "                    neuron.out_grad += next_neuron.x_grad(i)\n",
    "                neuron.generate_param_grad()\n",
    "        return error\n",
    "\n",
    "    def train_regre(\n",
    "        self, X, Y, learning_rate=0.05, epochs=50, print_logs=False, batch_size=50\n",
    "    ):\n",
    "        n = min(batch_size, len(X))\n",
    "        for k in range(epochs):\n",
    "            tot_error = 0\n",
    "            tot_grad = [(0, 0) for neuron in self.neurons]\n",
    "            batch_ind = self.rng.integers(low=0, high=len(X), size=n)\n",
    "            for x, y in zip(X[batch_ind], Y[batch_ind]):\n",
    "                tot_error += self.backprop_single_input(x, y)\n",
    "                tot_grad = [\n",
    "                    (grad[0] + neuron.w_grad, grad[1] + neuron.b_grad)\n",
    "                    for grad, neuron in zip(tot_grad, self.neurons)\n",
    "                ]\n",
    "            tot_error = tot_error / n\n",
    "            for grad, neuron in zip(tot_grad, self.neurons):\n",
    "                neuron.weights -= grad[0] / n * learning_rate\n",
    "                neuron.b -= grad[1] / n * learning_rate\n",
    "            if print_logs:\n",
    "                print(f\"Epoch: {k}, mean loss: {tot_error}\")\n",
    "\n",
    "    def backprop_binary_input(self, input, expected_value):\n",
    "        y_pred = self.predict_class(input)\n",
    "        error = cross_entropy_loss(expected_value, y_pred)\n",
    "        error_d = cross_entropy_loss_d(y_pred=y_pred, y_true=expected_value)\n",
    "        output_neuron = self.layers[-1][0]\n",
    "        output_neuron.out_grad = error_d\n",
    "        output_neuron.generate_param_grad()\n",
    "        next_layer = self.layers[-1]\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            for i, neuron in enumerate(layer):\n",
    "                neuron.w_grad = 0\n",
    "                neuron.out_grad = 0\n",
    "                neuron.b_grad = 0\n",
    "                for next_neuron in next_layer:\n",
    "                    neuron.out_grad += next_neuron.x_grad(i)\n",
    "                neuron.generate_param_grad()\n",
    "        return error\n",
    "\n",
    "    def train_class(\n",
    "        self, X, Y, learning_rate=0.05, epochs=50, print_logs=False, batch_size=50\n",
    "    ):\n",
    "        n = min(batch_size, len(X))\n",
    "        for k in range(epochs):\n",
    "            tot_error = 0\n",
    "            tot_grad = [(0, 0) for neuron in self.neurons]\n",
    "            batch_ind = self.rng.integers(low=0, high=len(X), size=n)\n",
    "            for x, y in zip(X[batch_ind], Y[batch_ind]):\n",
    "                tot_error += self.backprop_binary_input(x, y)\n",
    "                tot_grad = [\n",
    "                    (grad[0] + neuron.w_grad, grad[1] + neuron.b_grad)\n",
    "                    for grad, neuron in zip(tot_grad, self.neurons)\n",
    "                ]\n",
    "            tot_error = tot_error / n\n",
    "            for grad, neuron in zip(tot_grad, self.neurons):\n",
    "                neuron.weights -= grad[0] / n * learning_rate\n",
    "                neuron.b -= grad[1] / n * learning_rate\n",
    "            if print_logs:\n",
    "                print(f\"Epoch: {k}, mean loss: {tot_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def evaluate_model_regre(model, X, Y):\n",
    "    Y_pred = [model.predict_regre(x) for x in X]\n",
    "    MSE = sum([(y_pred - y)**2 for y_pred, y in zip(Y_pred,Y)]) / len(Y)\n",
    "    return math.sqrt(MSE)\n",
    "\n",
    "\n",
    "def evaluate_model_class(model, X, Y):\n",
    "    Y_pred = [model.predict_regre(x)  for x in X]\n",
    "    MSE = sum([(y_pred - y-1)**2 for y_pred, y in zip(Y_pred,Y)]) / len(Y)\n",
    "    return math.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f189deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X, min, max):\n",
    "    return (X.astype(float) - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(x, min, max):\n",
    "    return x * (max - min) + min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a3398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('./projekt1/regression/data.activation.train.1000.csv', delimiter=',')\n",
    "test_data = np.genfromtxt('./projekt1/regression/data.activation.test.1000.csv', delimiter=',')\n",
    "X_train = train_data[1:, 0]\n",
    "Y_train = train_data[1:, 1]\n",
    "X_test = test_data[1:, 0]\n",
    "Y_test = test_data[1:, 1]\n",
    "\n",
    "net = NeuralNet(hidden_layers=3,number_of_neurons_in_layer=30,input_dim=1, activation_f='relu')\n",
    "net.train_regre(X_train, Y_train, learning_rate=0.01, epochs=1000, print_logs=True)\n",
    "\n",
    "print(f\"RMSE on train data {evaluate_model_regre(net, X_train, Y_train)}\")\n",
    "print(f\"RMSE on test data {evaluate_model_regre(net, X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f411462",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt(\n",
    "    \"./projekt1/regression/data.activation.train.1000.csv\", delimiter=\",\"\n",
    ")\n",
    "test_data = np.genfromtxt(\n",
    "    \"./projekt1/regression/data.activation.test.1000.csv\", delimiter=\",\"\n",
    ")\n",
    "X_train = train_data[1:, 0]\n",
    "Y_train = train_data[1:, 1]\n",
    "X_test = test_data[1:, 0]\n",
    "Y_test = test_data[1:, 1]\n",
    "minx = min(X_train)\n",
    "maxx = max(X_train)\n",
    "\n",
    "net = NeuralNet(\n",
    "    hidden_layers=3, number_of_neurons_in_layer=30, input_dim=1, activation_f=\"relu\"\n",
    ")\n",
    "X_scaled = scale(X_train, minx, maxx)\n",
    "X_test_scaled = scale(X_test, minx, maxx)\n",
    "net.train_regre(X_scaled, Y_train, learning_rate=0.0001, epochs=500, print_logs=True)\n",
    "\n",
    "print(f\"RMSE on train data {evaluate_model_regre(net, X_scaled, Y_train)}\")\n",
    "print(f\"RMSE on test data {evaluate_model_regre(net, X_test_scaled, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449acd9",
   "metadata": {},
   "source": [
    "Plot scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ind = np.argsort(X_test_scaled)\n",
    "plt.plot(X_test_scaled, Y_test, label = 'test')\n",
    "Y_pred = [net.predict_regre(x) for x in X_test_scaled]\n",
    "plt.plot(X_test_scaled, Y_pred, label = 'pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85ba0e",
   "metadata": {},
   "source": [
    "Plot not scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9248d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ind = np.argsort(X_train)\n",
    "plt.plot(X_test, Y_test, label = 'test')\n",
    "Y_pred = [net.predict_regre(x) for x in X_test]\n",
    "plt.plot(X_test, Y_pred, label = 'pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac57271",
   "metadata": {},
   "source": [
    "# Classifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fab11e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('./projekt1/classification/data.simple.train.1000.csv', delimiter=',')\n",
    "test_data = np.genfromtxt('./projekt1/classification/data.simple.test.1000.csv', delimiter=',')\n",
    "X_train = train_data[1:, :2]\n",
    "Y_train = train_data[1:, 2]\n",
    "X_test = test_data[1:, :2]\n",
    "Y_test = test_data[1:, 2]\n",
    "\n",
    "net = NeuralNet(hidden_layers=3,number_of_neurons_in_layer=30,input_dim=2, number_of_outputs = 1)\n",
    "net.train_class(X_train, Y_train, learning_rate=0.01, epochs=1000, print_logs=True)\n",
    "\n",
    "print(f\"RMSE on train data {evaluate_model_class(net, X_train, Y_train)}\")\n",
    "print(f\"RMSE on test data {evaluate_model_class(net, X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8f742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "X_test = test_data[1:, 0]\n",
    "Y_test = test_data[1:, 1]\n",
    "C_test = test_data[1:, 2]\n",
    "\n",
    "# Create a scatter plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Use scatterplot with NumPy arrays\n",
    "Y_pred = [0 if net.predict_class(input) < 0.5 else 1 for input in test_data[1:, :2]]\n",
    "sns.scatterplot(x=X_test, y=Y_test, hue=Y_pred, palette='viridis', s=100, edgecolor='k', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend(title='Klasa')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91087390",
   "metadata": {},
   "source": [
    "# Wpływ liczby warstw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt(\n",
    "    \"./projekt1/regression/data.activation.train.1000.csv\", delimiter=\",\"\n",
    ")\n",
    "test_data = np.genfromtxt(\n",
    "    \"./projekt1/regression/data.activation.test.1000.csv\", delimiter=\",\"\n",
    ")\n",
    "X_train = train_data[1:, 0]\n",
    "Y_train = train_data[1:, 1]\n",
    "X_test = test_data[1:, 0]\n",
    "Y_test = test_data[1:, 1]\n",
    "minx = min(X_train)\n",
    "maxx = max(X_train)\n",
    "\n",
    "numbers_of_layer = [0,1,2,3,4]\n",
    "numbers_of_neurons = [1,5,20,50,100]\n",
    "\n",
    "nets = []\n",
    "for l in numbers_of_layer:\n",
    "    for n in numbers_of_neurons: \n",
    "        nets.append(NeuralNet(hidden_layers=l, number_of_neurons_in_layer=n, input_dim=1, activation_f=\"relu\"))\n",
    "    \n",
    "\n",
    "X_scaled = scale(X_train, minx, maxx)\n",
    "X_test_scaled = scale(X_test, minx, maxx)\n",
    "for net in nets:\n",
    "    net.train_regre(X_scaled, Y_train, learning_rate=0.0001, epochs=500, print_logs=True)\n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for net in nets:\n",
    "    plt.plot(X_test_scaled, Y_test, label = 'test')\n",
    "    Y_pred = [net.predict_regre(x) for x in X_test_scaled]\n",
    "    plt.plot(X_test_scaled, Y_pred, label = 'pred')\n",
    "    plt.show()\n",
    "             \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt(\n",
    "    \"./projekt1/regression/data.activation.train.1000.csv\", delimiter=\",\"\n",
    ")\n",
    "test_data = np.genfromtxt(\n",
    "    \"./projekt1/regression/data.activation.test.1000.csv\", delimiter=\",\"\n",
    ")\n",
    "X_train = train_data[1:, 0]\n",
    "Y_train = train_data[1:, 1]\n",
    "X_test = test_data[1:, 0]\n",
    "Y_test = test_data[1:, 1]\n",
    "minx = min(X_train)\n",
    "maxx = max(X_train)\n",
    "\n",
    "numbers_of_layer = [0,1,2,3,4]\n",
    "numbers_of_neurons = [1,5,20,50,100]\n",
    "\n",
    "net = NeuralNet(hidden_layers=4, number_of_neurons_in_layer=100, input_dim=1, activation_f=\"relu\")\n",
    "    \n",
    "\n",
    "X_scaled = scale(X_train, minx, maxx)\n",
    "X_test_scaled = scale(X_test, minx, maxx)\n",
    "net.train_regre(X_scaled, Y_train, learning_rate=0.0001, epochs=500, print_logs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d011bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X_test_scaled, Y_test, label = 'test')\n",
    "Y_pred = [net.predict_regre(x) for x in X_test_scaled]\n",
    "plt.plot(X_test_scaled, Y_pred, label = 'pred')\n",
    "plt.show()             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a41f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
