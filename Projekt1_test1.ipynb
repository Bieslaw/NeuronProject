{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e1cb878-5da2-4a15-b73a-7dce723709e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random.seed(10)\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred)**2)\n",
    "\n",
    "def loss_d(y_true, y_pred):\n",
    "    return 2 * y_pred - 2 * y_true\n",
    "\n",
    "def sigm_fun(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigm_d(sigm_val):\n",
    "    return sigm_val * (1 - sigm_val)\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_dim, activation_f = sigm_fun, activation_f_d = sigm_d):\n",
    "        self.activation_f = activation_f\n",
    "        self.activation_f_d = activation_f_d\n",
    "        weight_list = []\n",
    "        for i in range(input_dim):\n",
    "            weight_list.append(random.uniform(-1, 1))\n",
    "        self.weights = np.asarray(weight_list)\n",
    "        self.b = random.uniform(-1, 1)\n",
    "        self.b_grad = 0\n",
    "        self.w_grad = np.zeros(input_dim)\n",
    "        self.out_grad = 0\n",
    "    \n",
    "    def work(self, inputs):\n",
    "        self.x = np.array(inputs)\n",
    "        sum = np.sum(inputs * self.weights) + self.b\n",
    "        self.output = self.activation_f(sum)\n",
    "        return self.output\n",
    "\n",
    "    def x_grad(self, i):\n",
    "        return self.out_grad * self.activation_f_d(self.output) * self.weights[i]\n",
    "\n",
    "    def generate_param_grad(self):\n",
    "        self.w_grad = self.out_grad * self.activation_f_d(self.output) * self.x\n",
    "        self.b_grad = self.out_grad * self.activation_f_d(self.output)\n",
    "            \n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, number_of_neurons_in_layer, number_of_layers, input_dim):\n",
    "        self.input_layer = []\n",
    "        self.neurons: list[Neuron] = []\n",
    "        for i in range(number_of_neurons_in_layer):\n",
    "            neuron = Neuron(input_dim)\n",
    "            self.input_layer.append(neuron)\n",
    "            self.neurons.append(neuron)\n",
    "        self.net = []\n",
    "        for i in range(number_of_layers - 1):\n",
    "            current_layer =[]\n",
    "            for j in range(number_of_neurons_in_layer):\n",
    "                neuron = Neuron(number_of_neurons_in_layer)\n",
    "                current_layer.append(neuron)\n",
    "                self.neurons.append(neuron)\n",
    "            self.net.append(current_layer)\n",
    "        neuron = Neuron(number_of_neurons_in_layer, activation_f=lambda x:x, activation_f_d=lambda x:1)\n",
    "        self.output_layer = [neuron]\n",
    "        self.neurons.append(neuron)\n",
    "                                     \n",
    "    def predict(self, inputs):\n",
    "        temp_inputs = []\n",
    "        for neuron in self.input_layer:\n",
    "            temp_inputs.append(neuron.work(inputs))\n",
    "        temp_inputs2 = []\n",
    "        for layer in self.net:\n",
    "            for neuron in layer:\n",
    "                temp_inputs2.append(neuron.work(temp_inputs))\n",
    "            temp_inputs = temp_inputs2\n",
    "            temp_inputs2 = []\n",
    "        return self.output_layer[0].work(temp_inputs)\n",
    "\n",
    "    def backprop_single_input(self, input, expected_value):\n",
    "        y_pred = self.predict(input)\n",
    "        error = loss(y_pred, expected_value)\n",
    "        error_d = loss_d(y_pred=y_pred, y_true=expected_value)\n",
    "        self.output_layer[0].out_grad = error_d\n",
    "        self.output_layer[0].generate_param_grad()\n",
    "        next_layer = self.output_layer\n",
    "        layers = self.net.copy()\n",
    "        layers.append(self.input_layer)\n",
    "        for layer in reversed(layers):\n",
    "            for i, neuron in enumerate(layer):\n",
    "                neuron.w_grad = 0\n",
    "                neuron.out_grad = 0\n",
    "                neuron.b_grad = 0\n",
    "                for next_neuron in next_layer:\n",
    "                    neuron.out_grad += next_neuron.x_grad(i)\n",
    "                neuron.generate_param_grad()\n",
    "        return error\n",
    "    \n",
    "    def train(self, X, Y, learning_rate=0.05, max_iter = 100):\n",
    "        tot_error = 0\n",
    "        n = len(X)\n",
    "        tot_grad = [(neuron.w_grad, neuron.b_grad) for neuron in self.neurons]\n",
    "\n",
    "        for k in range(max_iter):\n",
    "            for x, y in zip(X, Y):\n",
    "                tot_error+=self.backprop_single_input(x, y)\n",
    "                print(tot_error)\n",
    "                tot_grad = [(grad[0] + neuron.w_grad, grad[1] + neuron.b_grad) \n",
    "                            for grad, neuron in zip(tot_grad, self.neurons)]\n",
    "            tot_error = tot_error/n\n",
    "            print(f\"Mean error {tot_error}\")\n",
    "            for grad, neuron in zip(tot_grad, self.neurons):\n",
    "                neuron.weights += -grad[0] / n * learning_rate\n",
    "                neuron.b += -grad[1] / n * learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e893919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.71062075495081\n",
      "Mean error 92.71062075495081\n",
      "157.07073751827124\n",
      "Mean error 157.07073751827124\n",
      "183.2963328393177\n",
      "Mean error 183.2963328393177\n",
      "184.03280505081324\n",
      "Mean error 184.03280505081324\n",
      "200.92376879793082\n",
      "Mean error 200.92376879793082\n",
      "257.51380308061124\n",
      "Mean error 257.51380308061124\n",
      "300.71181507586755\n",
      "Mean error 300.71181507586755\n",
      "317.59014461143835\n",
      "Mean error 317.59014461143835\n",
      "328.0601981288117\n",
      "Mean error 328.0601981288117\n",
      "332.7220466764819\n",
      "Mean error 332.7220466764819\n",
      "334.8267938655113\n",
      "Mean error 334.8267938655113\n",
      "335.4848739461601\n",
      "Mean error 335.4848739461601\n",
      "335.5052803353272\n",
      "Mean error 335.5052803353272\n",
      "335.7794494653469\n",
      "Mean error 335.7794494653469\n",
      "337.05726576073675\n",
      "Mean error 337.05726576073675\n",
      "339.6817227801722\n",
      "Mean error 339.6817227801722\n",
      "343.46509615846196\n",
      "Mean error 343.46509615846196\n",
      "347.76684810633486\n",
      "Mean error 347.76684810633486\n",
      "351.74545250560544\n",
      "Mean error 351.74545250560544\n",
      "354.6871466043694\n",
      "Mean error 354.6871466043694\n",
      "356.2841596938383\n",
      "Mean error 356.2841596938383\n",
      "356.754358499093\n",
      "Mean error 356.754358499093\n",
      "356.7558773143289\n",
      "Mean error 356.7558773143289\n",
      "357.13012130690083\n",
      "Mean error 357.13012130690083\n",
      "358.57339234762367\n",
      "Mean error 358.57339234762367\n",
      "361.36519060362787\n",
      "Mean error 361.36519060362787\n",
      "365.25913123586145\n",
      "Mean error 365.25913123586145\n",
      "369.5789965557906\n",
      "Mean error 369.5789965557906\n",
      "373.482446260428\n",
      "Mean error 373.482446260428\n",
      "376.289527325356\n",
      "Mean error 376.289527325356\n",
      "377.74785786122555\n",
      "Mean error 377.74785786122555\n",
      "378.13105960080037\n",
      "Mean error 378.13105960080037\n",
      "378.13204870426455\n",
      "Mean error 378.13204870426455\n",
      "378.5928009221464\n",
      "Mean error 378.5928009221464\n",
      "380.1759826801467\n",
      "Mean error 380.1759826801467\n",
      "383.10651213843397\n",
      "Mean error 383.10651213843397\n",
      "387.0838416218608\n",
      "Mean error 387.0838416218608\n",
      "391.3991714491736\n",
      "Mean error 391.3991714491736\n",
      "395.2118818926269\n",
      "Mean error 395.2118818926269\n",
      "397.877374882457\n",
      "Mean error 397.877374882457\n",
      "399.1984672360707\n",
      "Mean error 399.1984672360707\n",
      "399.5022920749669\n",
      "Mean error 399.5022920749669\n",
      "399.51271688622376\n",
      "Mean error 399.51271688622376\n",
      "400.06803518724695\n",
      "Mean error 400.06803518724695\n",
      "401.7940320441807\n",
      "Mean error 401.7940320441807\n",
      "404.8599278904547\n",
      "Mean error 404.8599278904547\n",
      "408.9123825548259\n",
      "Mean error 408.9123825548259\n",
      "413.2132979268683\n",
      "Mean error 413.2132979268683\n",
      "416.927676219609\n",
      "Mean error 416.927676219609\n",
      "419.44926910642107\n",
      "Mean error 419.44926910642107\n",
      "420.6370145685292\n",
      "Mean error 420.6370145685292\n",
      "420.87005108247854\n",
      "Mean error 420.87005108247854\n",
      "420.8998536144043\n",
      "Mean error 420.8998536144043\n",
      "421.5571583833255\n",
      "Mean error 421.5571583833255\n",
      "423.42797573578116\n",
      "Mean error 423.42797573578116\n",
      "426.62504611071114\n",
      "Mean error 426.62504611071114\n",
      "430.74387126828486\n",
      "Mean error 430.74387126828486\n",
      "435.0204686034419\n",
      "Mean error 435.0204686034419\n",
      "438.6293243618675\n",
      "Mean error 438.6293243618675\n",
      "441.0053440041754\n",
      "Mean error 441.0053440041754\n",
      "442.0642390762679\n",
      "Mean error 442.0642390762679\n",
      "442.2353997064327\n",
      "Mean error 442.2353997064327\n",
      "442.2944324553106\n",
      "Mean error 442.2944324553106\n",
      "443.06067375724456\n",
      "Mean error 443.06067375724456\n",
      "445.07764871088614\n",
      "Mean error 445.07764871088614\n",
      "448.40109629072094\n",
      "Mean error 448.40109629072094\n",
      "452.57723114701923\n",
      "Mean error 452.57723114701923\n",
      "456.8197198922305\n",
      "Mean error 456.8197198922305\n",
      "460.31635112212825\n",
      "Mean error 460.31635112212825\n",
      "462.5457978634581\n",
      "Mean error 462.5457978634581\n",
      "463.48093509350724\n",
      "Mean error 463.48093509350724\n",
      "463.5994184989623\n",
      "Mean error 463.5994184989623\n",
      "463.69739875810154\n",
      "Mean error 463.69739875810154\n",
      "464.5790227762665\n",
      "Mean error 464.5790227762665\n",
      "466.74281639277865\n",
      "Mean error 466.74281639277865\n",
      "470.187259303604\n",
      "Mean error 470.187259303604\n",
      "474.4113779799264\n",
      "Mean error 474.4113779799264\n",
      "478.61012534438595\n",
      "Mean error 478.61012534438595\n",
      "481.9883491312492\n",
      "Mean error 481.9883491312492\n",
      "484.0709012700453\n",
      "Mean error 484.0709012700453\n",
      "484.88794563304947\n",
      "Mean error 484.88794563304947\n",
      "484.9631941250959\n",
      "Mean error 484.9631941250959\n",
      "485.1096590406923\n",
      "Mean error 485.1096590406923\n",
      "486.1125782691618\n",
      "Mean error 486.1125782691618\n",
      "488.42317251780713\n",
      "Mean error 488.42317251780713\n",
      "491.98266923606235\n",
      "Mean error 491.98266923606235\n",
      "496.2452239102137\n",
      "Mean error 496.2452239102137\n",
      "500.390799423748\n",
      "Mean error 500.390799423748\n",
      "503.64498053279254\n",
      "Mean error 503.64498053279254\n",
      "505.5809958112258\n",
      "Mean error 505.5809958112258\n",
      "506.28615850686464\n",
      "Mean error 506.28615850686464\n",
      "506.3278143748156\n",
      "Mean error 506.3278143748156\n",
      "506.53207683298365\n",
      "Mean error 506.53207683298365\n",
      "507.661642729089\n",
      "Mean error 507.661642729089\n",
      "510.1183405700564\n",
      "Mean error 510.1183405700564\n",
      "513.7864174043142\n",
      "Mean error 513.7864174043142\n",
      "518.0776824729078\n",
      "Mean error 518.0776824729078\n",
      "522.1609016054914\n",
      "Mean error 522.1609016054914\n",
      "525.2859785467632\n",
      "Mean error 525.2859785467632\n",
      "527.076492496033\n",
      "Mean error 527.076492496033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Filip\\AppData\\Local\\Temp\\ipykernel_12856\\1397787267.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(9.22539705431861)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(10)\n",
    "net = NeuralNet(number_of_layers=3,number_of_neurons_in_layer=3,input_dim=3)\n",
    "\n",
    "\n",
    "x = [2,3,6]\n",
    "y = 10\n",
    "net.train([x], [y])\n",
    "net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a3398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
