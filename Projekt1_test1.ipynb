{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "2e1cb878-5da2-4a15-b73a-7dce723709e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "EPS = np.finfo(np.float32).eps\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def loss_d(y_true, y_pred):\n",
    "    return 2 * y_pred[0] - 2 * y_true\n",
    "\n",
    "\n",
    "def sigm_fun(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigm_d(sigm_val):\n",
    "    return sigm_val * (1 - sigm_val)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return 0.01 * x if x < 0 else x\n",
    "\n",
    "\n",
    "def relu_d(relu_val):\n",
    "    return 0.01 if relu_val <= 0 else 1\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "def linear_d(linear_val):\n",
    "    return 1\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_d(tanh_val):\n",
    "    return 1 - tanh_val**2\n",
    "\n",
    "def softmax(x):\n",
    "    _x = x.copy()\n",
    "    _x-=np.max(x)\n",
    "    return (np.exp(_x)/(np.exp(_x).sum())).tolist()\n",
    "\n",
    "def softmax_d(softmax_val, y_ind):\n",
    "    s_y = softmax_val[y_ind]\n",
    "    return [s_y*(1-s) if i == y_ind else -s * s_y for i, s in enumerate(softmax_val)]\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    return -(np.log(np.array(y_pred) + EPS)*np.array(y_true)).sum()\n",
    "\n",
    "\n",
    "def cross_entropy_loss_d(y_true, y_pred):\n",
    "    return -1/((np.array(y_pred)*np.array(y_true)).sum() + EPS)\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, rng, input_dim, activation_f, activation_f_d):\n",
    "        self.activation_f = activation_f\n",
    "        self.activation_f_d = activation_f_d\n",
    "        weight_list = []\n",
    "        for i in range(input_dim):\n",
    "            weight_list.append(rng.uniform(-1, 1) * 0.1)\n",
    "        self.weights = np.asarray(weight_list)\n",
    "        self.b = rng.uniform(-1, 1)\n",
    "        self.b_grad = 0\n",
    "        self.w_grad = np.zeros(input_dim)\n",
    "        self.out_grad = 0\n",
    "\n",
    "    def work(self, inputs):\n",
    "        self.x = np.array(inputs)\n",
    "        sum = np.sum(inputs * self.weights) + self.b\n",
    "\n",
    "        self.output = self.activation_f(sum)\n",
    "        return self.output\n",
    "\n",
    "    def x_grad(self, i):\n",
    "        return self.out_grad * self.activation_f_d(self.output) * self.weights[i]\n",
    "\n",
    "    def generate_param_grad(self):\n",
    "        self.w_grad = self.out_grad * self.activation_f_d(self.output) * self.x\n",
    "        self.b_grad = self.out_grad * self.activation_f_d(self.output)\n",
    "\n",
    "    def update_weights(self, errors, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * errors[i] * self.x[i]\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(\n",
    "        self,\n",
    "        number_of_neurons_in_layer,\n",
    "        hidden_layers,\n",
    "        input_dim,\n",
    "        number_of_outputs=1,\n",
    "        seed=10,\n",
    "        activation_f=\"sigmoid\",\n",
    "        out_activation_f=\"linear\",\n",
    "        loss_f=\"RSS\"\n",
    "    ):\n",
    "        if loss_f==\"RSS\":\n",
    "            self.loss_f=loss\n",
    "            self.loss_f_d=loss_d\n",
    "        elif loss_f == \"crossentropy\":\n",
    "            self.loss_f=cross_entropy_loss\n",
    "            self.loss_f_d=cross_entropy_loss_d\n",
    "        else:\n",
    "            print(\"Allowed loss_f values are {'RSS', 'crossentropy'}\")\n",
    "            return\n",
    "\n",
    "        if out_activation_f == \"linear\":\n",
    "            self.out_activation_f = linear\n",
    "            self.out_activation_f_d = linear_d\n",
    "        elif out_activation_f == \"softmax\":\n",
    "            self.out_activation_f = softmax\n",
    "            self.out_activation_f_d = softmax\n",
    "        else:\n",
    "            print(\"Allowed out_activation_f values are {'linear', 'softmax'}\")\n",
    "            return\n",
    "\n",
    "        if activation_f == \"sigmoid\":\n",
    "            act_f = sigm_fun\n",
    "            act_f_d = sigm_d\n",
    "        elif activation_f == \"relu\":\n",
    "            act_f = relu\n",
    "            act_f_d = relu_d\n",
    "        elif activation_f == \"tanh\":\n",
    "            act_f = tanh\n",
    "            act_f_d = tanh_d\n",
    "        else:\n",
    "            print(\"Allowed activation_function values are {'sigmoid', 'relu', 'tanh'}\")\n",
    "            return\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.layers = []\n",
    "        self.neurons: list[Neuron] = []\n",
    "        layer_1 = [\n",
    "            Neuron(self.rng, input_dim, act_f, act_f_d)\n",
    "            for _ in range(number_of_neurons_in_layer)\n",
    "        ]\n",
    "        self.layers.append(layer_1)\n",
    "        self.neurons.extend(layer_1)\n",
    "        for i in range(hidden_layers - 1):\n",
    "            current_layer = [\n",
    "                Neuron(self.rng, number_of_neurons_in_layer, act_f, act_f_d)\n",
    "                for _ in range(number_of_neurons_in_layer)\n",
    "            ]\n",
    "            self.neurons.extend(current_layer)\n",
    "            self.layers.append(current_layer)\n",
    "        output_layer = []\n",
    "        for i in range(number_of_outputs):\n",
    "            output_neuron = Neuron(\n",
    "                self.rng,\n",
    "                number_of_neurons_in_layer,\n",
    "                activation_f=linear,\n",
    "                activation_f_d=linear_d\n",
    "            )\n",
    "            output_layer.append(output_neuron)\n",
    "            self.neurons.append(output_neuron)\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        outputs = inputs.copy()\n",
    "        for layer in self.layers:\n",
    "            outputs = [neuron.work(outputs) for neuron in layer]\n",
    "        return self.out_activation_f(outputs)\n",
    "\n",
    "    def backprop_single_input(self, input, y_true):\n",
    "        y_pred = self.predict(input)\n",
    "        error = self.loss_f(y_true, y_pred)\n",
    "        error_d = self.loss_f_d(y_pred=y_pred, y_true=y_true)\n",
    "        softmax_d_val = [1 for i in range(len(y_pred))]\n",
    "        if self.out_activation_f == softmax:\n",
    "            y_true_ind = y_true.tolist().index(1)\n",
    "            softmax_d_val = softmax_d(y_pred, y_true_ind)\n",
    "        output_layer = self.layers[-1]\n",
    "        for i, output_neuron in enumerate(output_layer):\n",
    "            output_neuron.out_grad = error_d * softmax_d_val[i]\n",
    "            output_neuron.generate_param_grad()\n",
    "        next_layer = self.layers[-1]\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            for i, neuron in enumerate(layer):\n",
    "                neuron.w_grad = 0\n",
    "                neuron.out_grad = 0\n",
    "                neuron.b_grad = 0\n",
    "                for next_neuron in next_layer:\n",
    "                    neuron.out_grad += next_neuron.x_grad(i)\n",
    "                neuron.generate_param_grad()\n",
    "            next_layer = layer\n",
    "        return error\n",
    "\n",
    "    def backprop_binary_input(self, input, expected_value):\n",
    "        y_pred = self.predict_class(input)\n",
    "        error = cross_entropy_loss(expected_value, y_pred)\n",
    "        error_d = cross_entropy_loss_d(y_pred=y_pred, y_true=expected_value)\n",
    "        output_neuron = self.layers[-1][0]\n",
    "        output_neuron.out_grad = error_d\n",
    "        output_neuron.generate_param_grad()\n",
    "        next_layer = self.layers[-1]\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            for i, neuron in enumerate(layer):\n",
    "                neuron.w_grad = 0\n",
    "                neuron.out_grad = 0\n",
    "                neuron.b_grad = 0\n",
    "                for next_neuron in next_layer:\n",
    "                    neuron.out_grad += next_neuron.x_grad(i)\n",
    "                neuron.generate_param_grad()\n",
    "        return error\n",
    "\n",
    "    def train(\n",
    "        self, X, Y, learning_rate=0.05, epochs=50, print_logs=False, batch_size=50, momentum=0.9\n",
    "    ):\n",
    "        n = min(batch_size, len(X))\n",
    "        prev_grad = [(0., 0.) for neuron in self.neurons]\n",
    "        alpha = momentum\n",
    "        for k in range(epochs):\n",
    "            tot_error = 0\n",
    "            tot_grad = [(0, 0) for neuron in self.neurons]\n",
    "            batch_ind = self.rng.integers(low=0, high=len(X), size=n)\n",
    "            for x, y in zip(X[batch_ind], Y[batch_ind]):\n",
    "                tot_error += self.backprop_single_input(x, y)\n",
    "                tot_grad = [\n",
    "                    (grad[0] + neuron.w_grad, grad[1] + neuron.b_grad)\n",
    "                    for grad, neuron in zip(tot_grad, self.neurons)\n",
    "                ]\n",
    "            tot_error = tot_error / n\n",
    "            prev_grad = [(tot[0]/n + alpha * prev[0],tot[1]/n + alpha * prev[1] ) for tot, prev in zip(tot_grad, prev_grad)]\n",
    "            for grad, neuron in zip(prev_grad, self.neurons):\n",
    "                neuron.weights -= grad[0] * learning_rate\n",
    "                neuron.b -= grad[1] * learning_rate\n",
    "            if print_logs:\n",
    "                print(f\"Epoch: {k}, mean loss: {tot_error}\")\n",
    "\n",
    "\n",
    "def exportNeuralNet(nn: NeuralNet, path):\n",
    "    with open(path, 'w') as file:\n",
    "        file.write(f\"{len(nn.layers)}\\n\")\n",
    "        for layer in nn.layers:\n",
    "            for neuron in layer:\n",
    "                file.write(','.join(neuron.weights.astype(str)))\n",
    "                file.write(';')\n",
    "                file.write(f\"{neuron.b}\")\n",
    "                file.write('\\n')\n",
    "            file.write(\"|\\n\")\n",
    "def importNeuralNetWeights(nn: NeuralNet, path):\n",
    "    with open(path, 'r') as file:\n",
    "        num_layers = int(file.readline().rstrip())\n",
    "        layers = []\n",
    "        current_layer = []\n",
    "        for line in file:\n",
    "            line = line.rstrip()\n",
    "            if len(layers) == num_layers: break\n",
    "            if line == '|':\n",
    "                layers.append(current_layer)\n",
    "                current_layer = []\n",
    "                continue\n",
    "            neuron_data = line.split(';')\n",
    "            weights = np.fromstring(neuron_data[0], dtype=float, sep=',')\n",
    "            b = float(neuron_data[1])\n",
    "            current_layer.append((weights,b))\n",
    "        for neurons, layer_params in zip(nn.layers, layers):\n",
    "            print(f\"Layer neurons{len(neurons), len(layer_params)}\")\n",
    "            for neuron, neuron_params in zip(neurons, layer_params):\n",
    "                print(f\"neron weights{len(neuron.weights), len(neuron_params[0])}\")\n",
    "                neuron.weights = neuron_params[0]\n",
    "                neuron.b = neuron_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a0f519b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = NeuralNet(\n",
    "    hidden_layers=3, number_of_neurons_in_layer=3, input_dim=2, activation_f=\"relu\"\n",
    ")\n",
    "exportNeuralNet(export, 'test.txt')\n",
    "importNeuralNetWeights(export, 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "628eef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exportNeuralNet(export, 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00e7b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def evaluate_model_regre(model, X, Y):\n",
    "    Y_pred = [model.predict(x) for x in X]\n",
    "    MSE = sum([(y_pred - y)**2 for y_pred, y in zip(Y_pred,Y)]) / len(Y)\n",
    "    return math.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2f189deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X, min, max):\n",
    "    return (X.astype(float) - min) / (max - min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451d919",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0f411462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, mean loss: 11306.865320462282\n",
      "Epoch: 1, mean loss: 14148.753410118929\n",
      "Epoch: 2, mean loss: 13599.151215362384\n",
      "Epoch: 3, mean loss: 15893.00702725912\n",
      "Epoch: 4, mean loss: 15437.430167388753\n",
      "Epoch: 5, mean loss: 14257.950716308493\n",
      "Epoch: 6, mean loss: 15131.613700341164\n",
      "Epoch: 7, mean loss: 11664.606199259455\n",
      "Epoch: 8, mean loss: 9140.047816139479\n",
      "Epoch: 9, mean loss: 12667.906000550143\n",
      "Epoch: 10, mean loss: 10504.252979932031\n",
      "Epoch: 11, mean loss: 8707.162322085314\n",
      "Epoch: 12, mean loss: 6366.6420711997\n",
      "Epoch: 13, mean loss: 9226.832616495543\n",
      "Epoch: 14, mean loss: 8183.194054233859\n",
      "Epoch: 15, mean loss: 8375.539515230163\n",
      "Epoch: 16, mean loss: 8573.122013976381\n",
      "Epoch: 17, mean loss: 8914.353601866042\n",
      "Epoch: 18, mean loss: 5831.123501319749\n",
      "Epoch: 19, mean loss: 7824.544288556477\n",
      "Epoch: 20, mean loss: 5184.837978393996\n",
      "Epoch: 21, mean loss: 8147.191055044923\n",
      "Epoch: 22, mean loss: 7233.398061234035\n",
      "Epoch: 23, mean loss: 7244.434480805752\n",
      "Epoch: 24, mean loss: 7975.948901783312\n",
      "Epoch: 25, mean loss: 6890.649260139136\n",
      "Epoch: 26, mean loss: 6608.02491277937\n",
      "Epoch: 27, mean loss: 5984.987419914659\n",
      "Epoch: 28, mean loss: 6495.964244943212\n",
      "Epoch: 29, mean loss: 10420.792769304047\n",
      "Epoch: 30, mean loss: 6516.891085471646\n",
      "Epoch: 31, mean loss: 5305.037816557612\n",
      "Epoch: 32, mean loss: 6034.858796481331\n",
      "Epoch: 33, mean loss: 5909.2207048604905\n",
      "Epoch: 34, mean loss: 6584.098719223696\n",
      "Epoch: 35, mean loss: 6424.771759859638\n",
      "Epoch: 36, mean loss: 5478.311829331394\n",
      "Epoch: 37, mean loss: 4596.311326490442\n",
      "Epoch: 38, mean loss: 5619.950194642061\n",
      "Epoch: 39, mean loss: 5553.423946810586\n",
      "Epoch: 40, mean loss: 4948.1250629651895\n",
      "Epoch: 41, mean loss: 5237.283001305448\n",
      "Epoch: 42, mean loss: 4404.063778902245\n",
      "Epoch: 43, mean loss: 4562.30705986101\n",
      "Epoch: 44, mean loss: 3527.65225808805\n",
      "Epoch: 45, mean loss: 4865.451139138602\n",
      "Epoch: 46, mean loss: 5025.693434792476\n",
      "Epoch: 47, mean loss: 3569.0035610149725\n",
      "Epoch: 48, mean loss: 4137.637474451902\n",
      "Epoch: 49, mean loss: 3199.084545427195\n",
      "RMSE on train data 59.217462496951555\n",
      "RMSE on test data 78.15050301343715\n"
     ]
    }
   ],
   "source": [
    "train_data = np.genfromtxt(\n",
    "    \"./projekt1/regression/data.activation.train.1000.csv\", delimiter=\",\"\n",
    ")\n",
    "test_data = np.genfromtxt(\n",
    "    \"./projekt1/regression/data.activation.test.1000.csv\", delimiter=\",\"\n",
    ")\n",
    "X_train = train_data[1:, 0]\n",
    "Y_train = train_data[1:, 1]\n",
    "X_test = test_data[1:, 0]\n",
    "Y_test = test_data[1:, 1]\n",
    "minx = min(X_train)\n",
    "maxx = max(X_train)\n",
    "\n",
    "net = NeuralNet(\n",
    "    hidden_layers=3, number_of_neurons_in_layer=30, input_dim=1, activation_f=\"relu\"\n",
    ")\n",
    "X_scaled = scale(X_train, minx, maxx)\n",
    "X_test_scaled = scale(X_test, minx, maxx)\n",
    "net.train(X_scaled, Y_train, learning_rate=0.0001, epochs=50, print_logs=True)\n",
    "\n",
    "print(f\"RMSE on train data {evaluate_model_regre(net, X_scaled, Y_train)}\")\n",
    "print(f\"RMSE on test data {evaluate_model_regre(net, X_test_scaled, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449acd9",
   "metadata": {},
   "source": [
    "Plot scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a60f8035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2b9372a50>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ind = np.argsort(X_test_scaled)\n",
    "plt.plot(X_test_scaled, Y_test, label = 'test')\n",
    "Y_pred = [net.predict(x) for x in X_test_scaled]\n",
    "plt.plot(X_test_scaled, Y_pred, label = 'pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac57271",
   "metadata": {},
   "source": [
    "# Classifcation - multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e8fab11e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, mean loss: 2.7176565950126754\n",
      "Epoch: 1, mean loss: 3.2082090998733084\n",
      "Epoch: 2, mean loss: 3.176022001688029\n",
      "Epoch: 3, mean loss: 3.443322241143509\n",
      "Epoch: 4, mean loss: 3.540818269671037\n",
      "Epoch: 5, mean loss: 3.985345133068037\n",
      "Epoch: 6, mean loss: 2.9755446009608586\n",
      "Epoch: 7, mean loss: 2.7479244083432377\n",
      "Epoch: 8, mean loss: 2.5894122774458754\n",
      "Epoch: 9, mean loss: 2.719714944842349\n",
      "Epoch: 10, mean loss: 1.723461605048175\n",
      "Epoch: 11, mean loss: 1.9477420744655662\n",
      "Epoch: 12, mean loss: 2.411086462906102\n",
      "Epoch: 13, mean loss: 1.5129232206073746\n",
      "Epoch: 14, mean loss: 1.8768474914739803\n",
      "Epoch: 15, mean loss: 1.420445951592344\n",
      "Epoch: 16, mean loss: 1.1058398382967416\n",
      "Epoch: 17, mean loss: 0.8064604973425644\n",
      "Epoch: 18, mean loss: 0.7596259236259553\n",
      "Epoch: 19, mean loss: 0.5962234602281326\n",
      "Epoch: 20, mean loss: 0.6518854481742149\n",
      "Epoch: 21, mean loss: 0.7343017138452237\n",
      "Epoch: 22, mean loss: 0.749536860063578\n",
      "Epoch: 23, mean loss: 0.5244814082345611\n",
      "Epoch: 24, mean loss: 0.6785192712050744\n",
      "Epoch: 25, mean loss: 0.5213176061965306\n",
      "Epoch: 26, mean loss: 0.4922554978975107\n",
      "Epoch: 27, mean loss: 0.5918767696379214\n",
      "Epoch: 28, mean loss: 0.574442893192058\n",
      "Epoch: 29, mean loss: 0.6446013165404945\n",
      "Epoch: 30, mean loss: 0.5575275747256602\n",
      "Epoch: 31, mean loss: 0.5132425687191272\n",
      "Epoch: 32, mean loss: 0.5735750978195964\n",
      "Epoch: 33, mean loss: 0.613635628439732\n",
      "Epoch: 34, mean loss: 0.5528365047720416\n",
      "Epoch: 35, mean loss: 0.5642146438229612\n",
      "Epoch: 36, mean loss: 0.47309677307316017\n",
      "Epoch: 37, mean loss: 0.6143019990818932\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[273], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m Y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(num_classes)[Y_test]\n\u001b[0;32m     13\u001b[0m net \u001b[38;5;241m=\u001b[39m NeuralNet(hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,number_of_neurons_in_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, number_of_outputs \u001b[38;5;241m=\u001b[39m num_classes, activation_f\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, loss_f\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, out_activation_f\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[267], line 221\u001b[0m, in \u001b[0;36mNeuralNet.train\u001b[1;34m(self, X, Y, learning_rate, epochs, print_logs, batch_size, momentum)\u001b[0m\n\u001b[0;32m    219\u001b[0m batch_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng\u001b[38;5;241m.\u001b[39mintegers(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X), size\u001b[38;5;241m=\u001b[39mn)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X[batch_ind], Y[batch_ind]):\n\u001b[1;32m--> 221\u001b[0m     tot_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop_single_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m     tot_grad \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    223\u001b[0m         (grad[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m neuron\u001b[38;5;241m.\u001b[39mw_grad, grad[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m neuron\u001b[38;5;241m.\u001b[39mb_grad)\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m grad, neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tot_grad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons)\n\u001b[0;32m    225\u001b[0m     ]\n\u001b[0;32m    226\u001b[0m tot_error \u001b[38;5;241m=\u001b[39m tot_error \u001b[38;5;241m/\u001b[39m n\n",
      "Cell \u001b[1;32mIn[267], line 187\u001b[0m, in \u001b[0;36mNeuralNet.backprop_single_input\u001b[1;34m(self, input, y_true)\u001b[0m\n\u001b[0;32m    185\u001b[0m     neuron\u001b[38;5;241m.\u001b[39mb_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m next_neuron \u001b[38;5;129;01min\u001b[39;00m next_layer:\n\u001b[1;32m--> 187\u001b[0m         neuron\u001b[38;5;241m.\u001b[39mout_grad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnext_neuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m     neuron\u001b[38;5;241m.\u001b[39mgenerate_param_grad()\n\u001b[0;32m    189\u001b[0m next_layer \u001b[38;5;241m=\u001b[39m layer\n",
      "Cell \u001b[1;32mIn[267], line 77\u001b[0m, in \u001b[0;36mNeuron.x_grad\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_f(\u001b[38;5;28msum\u001b[39m)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mx_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_grad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_f_d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i]\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_param_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = np.genfromtxt('./projekt1/classification/data.three_gauss.train.1000.csv', delimiter=',')\n",
    "test_data = np.genfromtxt('./projekt1/classification/data.three_gauss.test.1000.csv', delimiter=',')\n",
    "X_train = train_data[1:, :2]\n",
    "Y_train = train_data[1:, 2].astype(int)\n",
    "X_test = test_data[1:, :2]\n",
    "Y_test = test_data[1:, 2].astype(int)\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "num_classes = np.max(Y_train) + 1\n",
    "Y_train = np.eye(num_classes)[Y_train]\n",
    "Y_test = np.eye(num_classes)[Y_test]\n",
    "\n",
    "net = NeuralNet(hidden_layers=3,number_of_neurons_in_layer=30,input_dim=2, number_of_outputs = num_classes, activation_f='relu', loss_f='crossentropy', out_activation_f='softmax')\n",
    "net.train(X_train, Y_train, learning_rate=0.0001, epochs=100, print_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "fce8f742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[270], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Use scatterplot with NumPy arrays\u001b[39;00m\n\u001b[0;32m     10\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(net\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m))\u001b[38;5;241m.\u001b[39margmax() \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m X_test]\n\u001b[1;32m---> 11\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatterplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX1_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX2_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mviridis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Add labels and title\u001b[39;00m\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\seaborn\\relational.py:615\u001b[0m, in \u001b[0;36mscatterplot\u001b[1;34m(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatterplot\u001b[39m(\n\u001b[0;32m    607\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    608\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    613\u001b[0m ):\n\u001b[1;32m--> 615\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_ScatterPlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_hue(palette\u001b[38;5;241m=\u001b[39mpalette, order\u001b[38;5;241m=\u001b[39mhue_order, norm\u001b[38;5;241m=\u001b[39mhue_norm)\n\u001b[0;32m    622\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_size(sizes\u001b[38;5;241m=\u001b[39msizes, order\u001b[38;5;241m=\u001b[39msize_order, norm\u001b[38;5;241m=\u001b[39msize_norm)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\seaborn\\relational.py:396\u001b[0m, in \u001b[0;36m_ScatterPlotter.__init__\u001b[1;34m(self, data, variables, legend)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, variables\u001b[38;5;241m=\u001b[39m{}, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    388\u001b[0m \n\u001b[0;32m    389\u001b[0m     \u001b[38;5;66;03m# TODO this is messy, we want the mapping to be agnostic about\u001b[39;00m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;66;03m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_size_range \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    393\u001b[0m         np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    394\u001b[0m     )\n\u001b[1;32m--> 396\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend \u001b[38;5;241m=\u001b[39m legend\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\seaborn\\_base.py:634\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\seaborn\\_base.py:679\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 679\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m     frame \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mframe\n\u001b[0;32m    681\u001b[0m     names \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mnames\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\seaborn\\_core\\data.py:58\u001b[0m, in \u001b[0;36mPlotData.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     53\u001b[0m     data: DataSource,\n\u001b[0;32m     54\u001b[0m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[0;32m     55\u001b[0m ):\n\u001b[0;32m     57\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data_source(data)\n\u001b[1;32m---> 58\u001b[0m     frame, names, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m=\u001b[39m frame\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m names\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\seaborn\\_core\\data.py:265\u001b[0m, in \u001b[0;36mPlotData._assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    260\u001b[0m             ids[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m(val)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Construct a tidy plot DataFrame. This will convert a number of\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# types automatically, aligning on index in case of pandas objects\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# TODO Note: this fails when variable specs *only* have scalars!\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frame, names, ids\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "X1_test = test_data[1:, 0]\n",
    "X2_test = test_data[1:, 1]\n",
    "C_test = Y_test\n",
    "\n",
    "# Create a scatter plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Use scatterplot with NumPy arrays\n",
    "Y_pred = [np.array(net.predict(input)).argmax() for input in X_test]\n",
    "sns.scatterplot(x=X1_test, y=X2_test, hue=Y_pred, palette='viridis', s=100, edgecolor='k', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend(title='Klasa')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7902e71e",
   "metadata": {},
   "source": [
    "# Printing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1220de7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:\n",
      "  Neuron 1:\n",
      "    Bias (b): 0.6177\n",
      "    Weights (w): 1.3134, -0.6751\n",
      "  Neuron 2:\n",
      "    Bias (b): -0.7529\n",
      "    Weights (w): -0.2574, -0.0172\n",
      "  Neuron 3:\n",
      "    Bias (b): -0.1703\n",
      "    Weights (w): 0.0758, 0.5626\n",
      "  Neuron 4:\n",
      "    Bias (b): -0.1476\n",
      "    Weights (w): 1.4707, 0.6700\n",
      "  Neuron 5:\n",
      "    Bias (b): 0.5845\n",
      "    Weights (w): 0.3136, 0.7803\n",
      "  Neuron 6:\n",
      "    Bias (b): 0.4559\n",
      "    Weights (w): 0.5500, -0.9579\n",
      "  Neuron 7:\n",
      "    Bias (b): -0.5440\n",
      "    Weights (w): -0.8795, 0.8144\n",
      "  Neuron 8:\n",
      "    Bias (b): 0.9770\n",
      "    Weights (w): 0.2283, -0.4247\n",
      "  Neuron 9:\n",
      "    Bias (b): -0.4061\n",
      "    Weights (w): 0.3386, -0.2671\n",
      "  Neuron 10:\n",
      "    Bias (b): 0.3677\n",
      "    Weights (w): 0.4680, -0.0483\n",
      "  Neuron 11:\n",
      "    Bias (b): 0.7027\n",
      "    Weights (w): -0.6643, -0.7824\n",
      "  Neuron 12:\n",
      "    Bias (b): -0.7159\n",
      "    Weights (w): -0.2059, -0.5340\n",
      "  Neuron 13:\n",
      "    Bias (b): -0.2289\n",
      "    Weights (w): -0.7014, -0.5663\n",
      "  Neuron 14:\n",
      "    Bias (b): 0.5174\n",
      "    Weights (w): 0.3774, 0.7974\n",
      "  Neuron 15:\n",
      "    Bias (b): -0.0959\n",
      "    Weights (w): -0.4239, -0.6349\n",
      "  Neuron 16:\n",
      "    Bias (b): -0.2531\n",
      "    Weights (w): -0.3492, -0.2313\n",
      "  Neuron 17:\n",
      "    Bias (b): -0.4595\n",
      "    Weights (w): 0.3349, -1.0556\n",
      "  Neuron 18:\n",
      "    Bias (b): 0.5368\n",
      "    Weights (w): 0.5243, 1.1946\n",
      "  Neuron 19:\n",
      "    Bias (b): -0.0846\n",
      "    Weights (w): 0.6131, 0.2220\n",
      "  Neuron 20:\n",
      "    Bias (b): -0.1511\n",
      "    Weights (w): -0.0409, 0.2840\n",
      "  Neuron 21:\n",
      "    Bias (b): -0.7968\n",
      "    Weights (w): -0.2648, 1.0018\n",
      "  Neuron 22:\n",
      "    Bias (b): -0.1825\n",
      "    Weights (w): 0.2777, 0.7008\n",
      "  Neuron 23:\n",
      "    Bias (b): 0.0257\n",
      "    Weights (w): 0.5576, -0.3835\n",
      "  Neuron 24:\n",
      "    Bias (b): -0.3259\n",
      "    Weights (w): 0.3362, -0.4643\n",
      "  Neuron 25:\n",
      "    Bias (b): 0.2522\n",
      "    Weights (w): -0.8351, -0.0272\n",
      "  Neuron 26:\n",
      "    Bias (b): -0.0810\n",
      "    Weights (w): 0.3486, 0.2808\n",
      "  Neuron 27:\n",
      "    Bias (b): -0.6927\n",
      "    Weights (w): -0.1526, -0.0992\n",
      "  Neuron 28:\n",
      "    Bias (b): -0.8845\n",
      "    Weights (w): -0.3516, 0.0001\n",
      "  Neuron 29:\n",
      "    Bias (b): -0.5990\n",
      "    Weights (w): 0.3662, -0.5115\n",
      "  Neuron 30:\n",
      "    Bias (b): -0.7975\n",
      "    Weights (w): -0.2087, -0.3844\n",
      "\n",
      "Layer 2:\n",
      "  Neuron 1:\n",
      "    Bias (b): 0.2138\n",
      "    Weights (w): 0.4704, 0.5581, -0.9146, 0.2902, -0.5073, -0.1084, 0.5292, 0.0784, -0.9659, -0.9486, -0.2477, 0.7937, -0.5686, -0.1149, -0.7257, 0.6035, -0.2140, -0.2873, -0.4673, 0.6737, -0.1555, -0.8944, 0.6355, -0.4806, -0.6709, -0.5236, 0.0831, 0.9045, -0.4160, -0.0909\n",
      "  Neuron 2:\n",
      "    Bias (b): -0.9663\n",
      "    Weights (w): 0.9907, 0.8945, -0.1947, -0.2210, -0.1361, -0.7498, 0.6345, 0.7945, -0.2400, 0.4647, 0.4706, -0.1159, 0.6942, 0.9659, 0.9128, -0.1352, -0.6184, 0.5847, 0.9103, 0.0407, -0.4513, 0.8320, 0.7555, -0.0107, 0.7247, -0.2084, -0.9469, -0.8213, -0.0398, -0.4547\n",
      "  Neuron 3:\n",
      "    Bias (b): -0.5016\n",
      "    Weights (w): -0.6342, 0.6069, -0.6325, -0.6507, 0.8284, -0.7645, -0.1956, -0.7422, 0.3821, -0.9432, 0.7829, -0.7000, 0.7975, 0.1593, -0.1601, 0.0069, -0.1873, -0.4142, 0.9047, -0.1674, 0.0196, 0.6751, -0.8494, -0.7744, 1.0260, 0.3604, -0.2423, 0.9962, 0.0113, -0.0603\n",
      "  Neuron 4:\n",
      "    Bias (b): -0.4909\n",
      "    Weights (w): 0.2991, -0.4063, 0.4030, -0.3045, 0.4150, -0.0067, -0.3486, -0.6777, -0.6617, -0.1770, 0.8539, 0.1864, -0.8153, 0.1577, -0.3002, -0.7868, -0.2286, -0.4733, 0.7381, 0.4813, -0.4248, 0.6623, -0.0190, 1.0215, 0.5228, -0.1619, -0.0138, -0.0663, 0.2062, -0.7219\n",
      "  Neuron 5:\n",
      "    Bias (b): -0.4703\n",
      "    Weights (w): -0.3173, 0.0419, -0.6115, 0.8738, -0.6127, 0.6647, -0.5940, 0.1201, -0.6797, 0.0896, -0.7970, 0.2565, -0.7166, 0.4471, -0.7323, 0.9205, 0.0101, -0.4000, -0.8934, -0.3061, 0.9270, -0.1494, 0.7972, -0.8547, -0.4596, 0.8952, -0.2588, -0.7986, 0.4920, 0.4991\n",
      "  Neuron 6:\n",
      "    Bias (b): -0.7349\n",
      "    Weights (w): 0.1692, 0.9472, -0.3666, -0.8494, 0.7165, -0.0299, 0.1944, 0.4615, -0.8217, -0.5619, 0.5211, 0.7041, -0.1864, -0.0469, 0.9778, 0.3074, 1.0008, 0.4038, 0.2203, -0.2230, -0.1447, -0.1828, -0.1594, 0.3058, -0.8172, -0.7237, 0.2458, 0.0796, -0.4398, -0.8868\n",
      "  Neuron 7:\n",
      "    Bias (b): 0.8088\n",
      "    Weights (w): 0.9133, -0.1102, -0.6753, -0.8470, 0.5786, -0.7261, -0.0881, -0.1066, 0.0793, 0.9783, 0.0054, -0.8471, 0.4305, 0.4878, 0.6895, -0.9855, 0.8496, 0.3902, 0.3413, 0.3674, 0.1346, -0.0167, -0.8493, 0.0068, 0.4001, 0.2750, -0.3713, -0.0794, 0.6292, 0.2931\n",
      "  Neuron 8:\n",
      "    Bias (b): -0.5238\n",
      "    Weights (w): -0.1315, 0.0621, -0.4994, 0.0802, -0.9595, 0.0326, 0.5921, 0.4102, 0.0627, -0.2024, -0.5683, -0.2754, -0.8596, 0.5232, 0.8935, -0.2596, -0.8497, 0.1943, -0.3171, 0.9152, 0.4334, -0.9926, 0.0592, -0.1870, 0.7121, 0.1728, -0.9539, -0.0317, 0.7608, 0.1232\n",
      "  Neuron 9:\n",
      "    Bias (b): 0.8886\n",
      "    Weights (w): -0.8509, 0.5581, -0.7687, 0.2104, 0.9477, 0.6618, -0.9887, -0.0122, 0.9765, -0.2895, 0.3648, 0.8330, -0.1770, -0.5007, 0.6680, 0.8552, -0.0347, 0.4048, 0.6646, -0.9401, 0.2709, -0.2419, 0.1733, 0.7011, -0.5592, -0.8640, 0.1480, 0.9057, -0.5212, 0.8774\n",
      "  Neuron 10:\n",
      "    Bias (b): -0.9335\n",
      "    Weights (w): 0.7044, 0.1858, 0.2140, 0.3960, 0.6387, -0.7834, 0.4906, -0.6680, 0.2675, -0.0802, 0.2687, -0.4523, -0.7890, 0.3511, -0.2424, 0.3650, -0.4908, 0.1500, 0.7601, -0.7438, 0.1760, 0.8331, 0.8442, -0.8091, -0.2670, 0.5469, -0.0483, 0.6500, -0.7813, -0.4190\n",
      "  Neuron 11:\n",
      "    Bias (b): -0.7965\n",
      "    Weights (w): 0.6542, -0.9564, -0.5237, 0.1053, -1.0044, -0.2162, 0.0024, -0.7953, 0.6224, 0.5403, -0.4483, 0.6610, 0.0619, 0.2042, 1.0309, -0.9657, 0.0566, 0.6246, 0.7404, -0.4214, 0.0052, -0.4288, 0.0331, 0.3957, 0.5566, 0.1850, -0.1900, -0.4679, 0.4785, -0.8175\n",
      "  Neuron 12:\n",
      "    Bias (b): 0.1052\n",
      "    Weights (w): -0.6079, -0.8363, 0.6609, 0.7686, 0.0007, -0.0249, -0.0010, -0.7347, -0.4931, -0.2426, -0.9022, 0.7672, -0.2819, -0.2300, -0.5842, -0.3776, 0.7173, -0.6049, 0.2046, 0.6575, 0.0405, 0.5080, -0.4172, 0.3861, -0.6592, 0.5493, -0.8634, -0.2320, 0.8611, 0.6325\n",
      "  Neuron 13:\n",
      "    Bias (b): 0.8698\n",
      "    Weights (w): -0.2446, -0.1021, 0.1996, -0.3000, 0.1934, 0.6937, -0.5451, 0.2095, 0.8186, -0.2631, 0.5951, -0.9291, -0.3808, 0.7322, 0.2292, 0.9732, 0.0905, -0.5582, -0.9135, 0.9649, -0.8758, 0.6893, 0.3093, 0.8217, 0.9535, -0.1845, -0.5995, 0.6408, -0.1794, -0.9772\n",
      "  Neuron 14:\n",
      "    Bias (b): 0.7703\n",
      "    Weights (w): 0.1138, 0.4683, -0.6980, -0.0301, 0.6477, 0.9637, -0.3830, -0.2395, 0.6540, -0.7277, -0.6300, 0.6525, 0.7421, 0.7256, -0.9708, -0.1248, 0.7909, 0.4183, -0.7900, 0.4441, -0.5500, -0.8031, -0.6400, 0.8498, -0.5305, -0.0738, 0.3306, -0.8392, 0.5032, -0.7371\n",
      "  Neuron 15:\n",
      "    Bias (b): 0.9802\n",
      "    Weights (w): -0.3792, 0.3977, 0.3776, -0.8104, -0.7121, 0.5291, -0.1368, -0.1826, -0.4068, 0.6868, -0.6818, -0.0645, 0.7163, -0.5606, -0.2150, 0.2239, -0.1292, 0.8870, -0.7593, -0.6116, -0.8172, -0.2466, -0.9463, -0.4966, 0.9998, -0.7041, -0.3933, 0.8929, 0.0041, 0.7584\n",
      "  Neuron 16:\n",
      "    Bias (b): -0.7048\n",
      "    Weights (w): -0.0222, 0.5408, 0.2081, -1.0480, 0.5463, -0.6484, -0.6269, -0.3523, 0.4002, -0.8681, -0.0415, 0.7377, 0.4979, 0.1526, 0.8671, -0.2857, -0.7642, -0.3933, -0.5027, 0.4151, 0.5270, -0.0304, -0.7440, -0.0545, 0.8191, -0.0291, 0.8652, 0.9965, -0.6623, 0.8957\n",
      "  Neuron 17:\n",
      "    Bias (b): 0.8887\n",
      "    Weights (w): 0.0390, -0.3231, 0.8152, -0.2440, -0.2132, 0.9782, -0.6874, -0.4109, 0.0226, -0.5887, 0.0679, -0.6673, -0.3106, 0.4319, 0.0198, -0.4075, -0.2325, -0.5593, 0.6938, 0.3893, -0.8740, -0.8815, -0.1127, -0.2558, 0.1909, -0.1004, -0.4062, 0.1145, 0.2178, -0.0119\n",
      "  Neuron 18:\n",
      "    Bias (b): -0.6126\n",
      "    Weights (w): 0.1841, 0.4947, 0.9327, 0.3427, 0.7178, 0.1038, 0.3686, -0.4233, -0.2662, -0.0005, -0.2283, -0.2259, 0.2229, -0.5814, -0.3037, 0.2724, -0.6462, -0.5930, 0.2629, -0.7160, 1.0058, -0.8619, -0.8315, 0.0496, 0.3984, -0.6547, -0.1206, 0.0962, -0.4780, 0.6810\n",
      "  Neuron 19:\n",
      "    Bias (b): 0.1222\n",
      "    Weights (w): 0.7888, -0.9606, 0.7759, 0.2170, 0.1094, -0.3882, -0.9839, 0.0284, 0.8263, 0.2935, 0.6058, -0.2152, 0.5833, 0.9075, -0.2093, 0.2583, -0.3623, -0.3812, -0.9180, -1.0310, 0.5066, -0.9381, 0.5373, 0.3199, 0.5863, -0.4802, 0.6665, 0.3232, 0.0681, 0.0520\n",
      "  Neuron 20:\n",
      "    Bias (b): 0.7052\n",
      "    Weights (w): 0.4483, 0.7545, 0.1545, -0.0091, -0.0373, -0.1810, 1.0220, -0.6951, 0.4358, -1.0358, 0.7791, 0.4151, 0.2638, 0.6141, 0.0788, -0.8628, -0.4093, 0.7326, -0.9731, -1.0248, 0.9794, -0.9379, -1.1313, -1.0702, 0.6936, -1.0827, 0.5044, 0.4222, -0.1464, -0.0864\n",
      "  Neuron 21:\n",
      "    Bias (b): 0.8615\n",
      "    Weights (w): -0.8142, -0.9039, -0.9276, 0.2295, -0.1491, 0.9391, 0.4993, -0.4647, 0.8550, 0.1284, -0.1242, -0.2621, -0.0422, 0.3014, 0.9846, 0.9728, 0.1918, -0.1030, -0.4370, 1.0337, 0.6475, -0.2789, -0.0601, -0.3737, 0.9890, 0.1606, -0.1371, -0.2422, 0.3334, -0.6818\n",
      "  Neuron 22:\n",
      "    Bias (b): 0.6809\n",
      "    Weights (w): 0.1013, 0.0083, 0.4764, 0.8350, -0.6148, -0.7322, 0.8365, 0.8487, 0.5504, -0.0281, -0.2822, 0.9980, -0.7025, 0.9762, -0.8193, 1.0173, 0.7275, -0.5274, 0.8219, 0.6172, 0.7746, 0.9397, 0.8635, -0.1408, 0.0150, -0.3242, -0.7926, -0.0518, -0.5743, -0.3151\n",
      "  Neuron 23:\n",
      "    Bias (b): 0.1998\n",
      "    Weights (w): -0.9542, -0.9401, 0.7170, 0.2534, 0.6742, -0.0775, 0.5315, -0.7381, 0.7809, 0.4197, -0.5798, 0.0085, -0.7612, -0.1671, 0.6570, 0.3561, 0.2398, 0.6341, 0.5829, -0.2131, 0.3769, 0.8795, 0.1931, 0.7442, 0.8560, -0.1693, -0.3702, 0.9723, -0.5143, 0.6489\n",
      "  Neuron 24:\n",
      "    Bias (b): 0.7659\n",
      "    Weights (w): -0.5332, -0.5614, -0.1848, -0.5424, 0.5516, 1.0097, 0.1630, -0.0800, -0.1183, 0.3766, 0.3087, -0.2610, -0.7992, 0.2044, 0.3105, 0.4045, -0.7039, 0.9611, -0.4096, -0.4160, -0.0809, -0.7933, 0.7917, -0.0722, 0.5022, -0.1854, 0.2030, -0.9624, -0.0704, 0.6479\n",
      "  Neuron 25:\n",
      "    Bias (b): -0.9485\n",
      "    Weights (w): -0.3810, 0.7265, 0.6035, 0.3205, 0.4998, -0.3686, 0.6953, -0.8465, 0.6758, 0.6488, -0.2482, -0.1749, 0.1347, -0.3600, -0.0683, 0.5740, -0.6141, -0.9867, -0.5783, 0.5252, 0.7493, 0.2021, 0.7355, -0.2476, -0.3447, 0.5229, -0.5949, 1.0362, -1.0201, 0.6138\n",
      "  Neuron 26:\n",
      "    Bias (b): -0.6532\n",
      "    Weights (w): -0.3999, 0.2205, -0.8417, 0.2126, -0.7558, 0.1781, 0.3390, -0.6487, -0.0359, -0.2521, -0.4436, -0.8341, -0.2282, -0.1207, 0.9731, -0.5786, 0.2356, 0.1074, 0.9645, 0.1418, 0.3489, -0.6533, 0.8567, -0.2783, -0.8890, -0.8120, -0.6242, -0.5278, -0.2055, -0.4273\n",
      "  Neuron 27:\n",
      "    Bias (b): 0.1080\n",
      "    Weights (w): 0.2384, -0.8618, 0.4744, 0.9807, -0.6479, -0.6033, -0.7479, -0.1180, -0.9793, -0.6038, -0.9129, -0.3752, 0.9158, 0.1033, 0.7127, -0.6571, -0.2421, -0.8480, -0.5400, 0.0471, -0.4773, 0.1554, 0.0218, 0.7667, 0.3980, -0.4409, -0.7813, 0.2397, -0.8004, 0.8615\n",
      "  Neuron 28:\n",
      "    Bias (b): -0.6987\n",
      "    Weights (w): -0.2133, -0.9629, -0.8744, 0.3396, 0.5552, -0.8418, -0.0499, -0.5707, 0.7772, -0.0560, -0.8497, -0.5875, -0.8817, -0.0120, 0.8412, 0.3349, 0.1660, -0.6107, 0.6718, -0.2277, 0.7326, 0.6513, -0.4506, -0.4180, 0.2195, -0.1363, 0.8496, 0.9632, -0.4633, 0.5515\n",
      "  Neuron 29:\n",
      "    Bias (b): -0.0836\n",
      "    Weights (w): -0.5583, -0.9442, -0.6247, 0.5886, 0.2422, -0.3093, -0.4290, -0.1748, -0.3964, 0.4756, -0.3836, -0.6468, 0.0109, 0.4937, 0.8249, 0.7983, 0.5298, -0.5973, 0.3127, -0.7097, 0.7920, 0.2953, -0.2357, -0.9543, 0.1968, -0.1471, -0.8908, 0.2062, -0.4082, -0.6403\n",
      "  Neuron 30:\n",
      "    Bias (b): 0.6093\n",
      "    Weights (w): 0.7742, -0.0212, -0.0371, -0.9219, -0.6451, -0.0592, -0.5619, 0.7156, -0.1113, -0.4883, -0.4140, -0.3517, 0.9190, -0.3298, 0.5846, -0.4060, 0.6441, -0.8465, -0.0475, 0.6687, -0.4003, -0.8939, 0.5512, -0.4171, -0.5477, -0.0644, 0.3242, -0.3236, -0.4398, -0.2830\n",
      "\n",
      "Layer 3:\n",
      "  Neuron 1:\n",
      "    Bias (b): 0.0205\n",
      "    Weights (w): -0.0700, 0.6928, 0.4201, -0.9671, 0.8217, -0.7188, -0.1176, 0.5305, 0.5062, -0.5715, 0.0412, -0.4857, 0.9915, -0.3535, 0.6595, -0.1596, -0.7094, 0.0153, 0.6631, -0.0959, 0.4869, 0.8793, 0.8887, 0.6275, -0.2030, -0.8436, -0.4378, -0.6277, -0.9642, 0.5913\n",
      "  Neuron 2:\n",
      "    Bias (b): -0.5924\n",
      "    Weights (w): -0.2531, -0.4088, -0.2065, -0.3634, -0.7343, 0.5103, -0.8470, -0.8947, 0.6437, -0.5928, -0.7337, -0.2656, 0.7925, 0.9385, -0.5777, 0.7444, -0.4006, 0.5100, 0.8283, 0.1922, -0.9948, -0.0551, -0.8681, 0.9241, -0.0424, 0.5375, 0.0230, 0.8147, 0.9317, -0.3644\n",
      "  Neuron 3:\n",
      "    Bias (b): 0.6783\n",
      "    Weights (w): 0.9636, 0.0985, 0.5295, 0.7715, -0.1668, -0.9895, -0.5054, -0.9009, 0.2730, 0.5451, 0.1343, -0.4066, 0.9589, -0.3060, -0.6819, 0.4734, -0.9944, 0.9531, -0.9135, 0.1721, 0.1662, 0.8649, 0.0053, 0.6327, -0.5068, 0.1452, -0.0692, -0.1797, 0.7098, -0.2161\n",
      "  Neuron 4:\n",
      "    Bias (b): -0.4613\n",
      "    Weights (w): 0.1778, 0.8537, 0.8632, 0.1755, 0.7840, -0.8288, 0.6721, -0.8746, 0.7469, -0.2885, 0.9864, 0.9881, 0.3661, 0.4105, 0.0406, -0.5547, -0.2372, -0.4655, -0.4550, -0.2335, -0.9149, 0.8716, -0.4445, -0.0941, -0.9656, -0.9647, -0.3908, -0.0092, -0.7246, 0.9319\n",
      "  Neuron 5:\n",
      "    Bias (b): 0.1505\n",
      "    Weights (w): -0.1101, -0.3700, 0.2904, 0.0244, -0.6780, -0.7243, 0.1140, 0.5479, -0.9905, -0.6442, -0.4710, -0.7896, 0.0634, 0.6415, -0.4066, -0.2672, -0.7279, -0.4610, 0.7833, 0.6235, 0.8077, 0.4711, 1.0144, -0.0047, 0.5199, 0.2254, -0.9299, -0.5099, 0.5316, -0.9324\n",
      "  Neuron 6:\n",
      "    Bias (b): -0.7416\n",
      "    Weights (w): 0.7494, -0.1378, 0.8663, 0.7768, -0.5593, -0.4098, 0.4651, -0.4940, -0.7566, -0.7665, 0.2404, -1.0367, 0.9125, -0.0612, -0.4999, 0.3108, -0.7031, 1.0043, -0.9229, 0.5549, -0.6510, 0.7216, -0.7642, 0.9062, 0.9506, -0.5801, -0.2095, 0.8504, 0.6813, -0.0745\n",
      "  Neuron 7:\n",
      "    Bias (b): 0.9555\n",
      "    Weights (w): -0.8070, 0.3655, -0.1248, -0.0315, -0.3726, 0.4008, -0.4648, -0.9159, 0.0256, -0.8294, -0.3563, -0.8541, -0.1376, 0.1900, 0.5772, 0.8888, -0.5882, -0.3063, 0.8546, 0.4144, 0.8198, -0.2375, -0.1897, -0.1514, -0.8809, 0.4751, 0.9312, 0.8510, 0.4648, -0.5795\n",
      "  Neuron 8:\n",
      "    Bias (b): -1.0545\n",
      "    Weights (w): 0.1125, -0.3728, -0.3858, -0.9184, -0.3048, 0.0251, 0.7893, 0.4231, -0.1676, -0.1915, 0.6956, 0.8050, -0.6554, 0.8711, 0.5428, -0.2907, -0.6657, -0.9806, -0.4733, 0.2443, 0.1562, 0.7716, -0.9578, -0.4729, -0.3909, 0.2435, 0.6955, 0.4079, -0.6853, -0.3008\n",
      "  Neuron 9:\n",
      "    Bias (b): -0.8502\n",
      "    Weights (w): -0.7040, -0.5936, -0.1875, 0.7912, 0.5070, -0.9791, -0.2002, -0.5433, -0.5928, -0.9710, 0.2706, 1.0174, -0.0109, -0.8489, -0.6680, 0.7570, 0.2926, 0.4499, 0.2759, 0.3344, 0.7238, -0.7759, 0.7828, -0.2822, -0.2392, -0.9091, 0.9160, -0.3739, 0.6346, 0.6589\n",
      "  Neuron 10:\n",
      "    Bias (b): -0.2617\n",
      "    Weights (w): -0.1530, -0.0582, 0.7027, 0.5272, -0.3899, -0.1035, 0.4323, 0.3641, 0.5318, 0.6293, 0.3708, -0.2982, -0.5491, 0.9255, -0.0231, -0.3292, 0.5818, 0.0318, -0.1747, 0.6235, -0.8857, -0.2935, -0.2258, -0.4153, 0.2477, -0.2497, 0.5866, 0.9819, 0.8118, -0.5785\n",
      "  Neuron 11:\n",
      "    Bias (b): 0.9739\n",
      "    Weights (w): 0.7932, 0.6717, -0.2705, -0.3643, 0.9744, 0.1989, -0.7109, 0.9136, -0.5410, 0.0303, 0.0548, 0.6092, -0.5645, 0.5722, 0.7269, -0.2833, 0.2569, -0.1903, -0.1248, 0.1445, -0.7735, -0.6327, 0.8632, -0.3197, 0.5810, 0.7078, 0.5680, -0.6287, 0.7783, 0.6542\n",
      "  Neuron 12:\n",
      "    Bias (b): -0.7590\n",
      "    Weights (w): 0.6774, 0.1824, 0.4252, 0.9615, -0.2205, 0.2276, 0.3701, 0.5204, -0.4150, 0.4535, -0.8528, -0.0183, 0.8477, 0.3425, 0.9427, -0.8871, -0.1112, -0.7371, -0.9414, -0.9098, 0.8381, 0.9878, -0.1167, 0.6773, 1.0125, -0.7437, -0.0139, 0.8853, -0.7696, -0.0906\n",
      "  Neuron 13:\n",
      "    Bias (b): -0.2250\n",
      "    Weights (w): 0.3908, 0.7822, 0.9443, 0.8907, 0.4976, 0.0800, 0.0157, -0.7294, 0.0002, 0.6536, 0.0998, 0.6030, -0.1358, 0.3479, -0.6560, -0.9793, -0.5444, -0.4992, -0.3568, 0.7554, 0.3138, 0.0017, -0.3174, 0.0083, -0.5828, -0.1538, -0.1407, 0.9498, 0.2201, -0.0862\n",
      "  Neuron 14:\n",
      "    Bias (b): 0.7325\n",
      "    Weights (w): 0.0976, -0.2892, -0.9164, 0.7717, -0.6318, -0.9010, 0.0884, -0.5719, 0.9208, 0.4940, -0.5809, -0.5855, -0.8263, 0.3969, -0.2821, -0.3189, -0.2628, 0.4523, -0.1927, 0.4261, 0.3800, -0.7045, 0.4521, -0.8113, -0.0394, 0.1960, -0.6223, 0.5388, 0.3285, -0.1852\n",
      "  Neuron 15:\n",
      "    Bias (b): -0.4934\n",
      "    Weights (w): 0.6522, 0.8819, 0.8062, 0.3056, -0.5874, -0.9405, -0.8761, -0.2322, 0.7091, 0.6541, 0.5834, 0.1190, 0.0837, 0.3309, -0.7951, -0.1256, -0.8848, 0.7082, 0.6611, -0.5367, 0.0783, 0.9060, -0.1806, 0.5147, 0.0516, 0.9980, -0.1277, 0.9818, 0.6495, 0.6609\n",
      "  Neuron 16:\n",
      "    Bias (b): -0.9541\n",
      "    Weights (w): 0.8751, -0.3393, 0.7944, 0.0564, -0.2018, 0.0456, -0.4089, -0.2326, -0.1040, -0.6764, 0.2596, -0.7725, -0.0535, -0.6723, -0.1631, 0.4201, -0.0685, 0.5544, 0.3852, -0.3793, -0.9631, -0.2189, 0.2113, -0.0345, -0.6150, -0.1852, 0.8205, -0.0229, -0.8075, 0.4930\n",
      "  Neuron 17:\n",
      "    Bias (b): 0.3486\n",
      "    Weights (w): 0.8228, -0.3436, 0.5346, -0.3804, -0.8117, 0.8891, 0.2700, 0.6887, 0.5555, -0.6422, 0.0719, 0.0460, -0.1748, 0.1363, 0.2113, -0.3530, 0.2995, 0.7763, -0.3661, -0.4324, -0.8224, -0.4036, -0.7900, 0.9762, 0.5532, -0.9576, 0.5710, 0.1587, -0.9275, 0.5840\n",
      "  Neuron 18:\n",
      "    Bias (b): 0.8746\n",
      "    Weights (w): 0.0876, 0.0523, -0.8576, -0.9571, -0.8143, -0.5544, -0.9333, 0.0974, -0.9222, -0.1892, 0.0063, -0.9839, 0.7560, 0.3730, -0.3964, 0.8154, 0.3574, -0.6352, 0.7071, -0.3598, 0.5253, -0.1802, -0.1065, -0.9971, -0.4027, 0.8582, 0.1743, 0.5518, -0.5769, 0.0248\n",
      "  Neuron 19:\n",
      "    Bias (b): 0.1572\n",
      "    Weights (w): 0.7259, 0.1747, -0.4364, 0.4712, -0.7042, -0.7266, 0.3066, -0.6577, -0.7880, 0.2966, -0.3116, 0.2387, 0.7872, 0.5375, 0.8735, -0.7057, 0.2872, 0.9053, -0.0910, -0.0204, -0.8483, -0.4193, -0.6851, -0.9818, 0.7608, -0.4171, 0.2575, 0.7140, -0.0598, 0.8616\n",
      "  Neuron 20:\n",
      "    Bias (b): 0.4259\n",
      "    Weights (w): 0.9153, 0.1207, 0.3151, -0.9087, -0.6745, 0.1246, -0.7330, 0.6110, -0.0273, 0.3686, 0.6052, -1.0001, 0.3310, -0.3784, -0.3701, 0.8869, 0.1692, -0.5858, -0.9216, -0.8149, -0.3342, 0.3736, -0.5480, -0.4707, -0.6417, 0.1853, 0.6021, -0.6392, -0.1525, -0.6468\n",
      "  Neuron 21:\n",
      "    Bias (b): -0.4825\n",
      "    Weights (w): -0.0735, 0.4380, 0.5088, 0.0202, 0.7154, 0.0322, -0.1376, 0.6643, 0.8078, 0.0231, 0.7210, -0.5813, -0.1781, -0.9431, -0.5041, -0.6891, 0.1549, 0.7396, 0.1525, 0.9037, -0.1524, -0.6681, 0.6849, 0.0336, 0.8352, 0.2903, 0.0191, 0.6543, -0.5075, 0.7429\n",
      "  Neuron 22:\n",
      "    Bias (b): -0.4816\n",
      "    Weights (w): -0.9820, 0.6374, -0.2637, -0.4334, -0.4062, 0.1182, 0.2677, -0.3361, 0.0991, 0.3008, 0.8749, 0.8011, 0.9028, -0.8991, 0.2414, 0.8450, 0.0187, -0.2448, 0.8202, -0.2101, -0.7759, -0.5466, 0.7249, 0.4976, 0.8152, -0.5701, 0.6203, -0.6945, 0.6566, -0.2419\n",
      "  Neuron 23:\n",
      "    Bias (b): 0.5383\n",
      "    Weights (w): 0.9568, -0.7827, -0.8988, -0.5314, -0.5359, -0.1998, -0.2858, -0.5298, 0.5525, -0.4998, -0.7226, -0.4018, -0.1128, 0.9254, 0.1645, -0.1840, -0.7414, -0.3402, -0.9255, -0.4628, 0.5308, -0.8475, -0.6314, -0.2252, -0.9724, 0.0740, -0.7769, -0.1529, 0.5993, -0.5639\n",
      "  Neuron 24:\n",
      "    Bias (b): 0.8891\n",
      "    Weights (w): 0.8701, -0.6709, -0.4828, 0.5712, 0.4918, -0.4728, -0.5364, -0.1310, 0.6244, 0.6002, 0.2655, -0.6368, -0.6139, 0.0594, 0.6617, -0.0938, 0.6374, -0.0744, -0.8851, -0.9203, -0.0195, 0.3826, 0.6995, -0.4887, -0.9821, 0.1230, -0.4464, 0.5205, -0.5394, 0.7588\n",
      "  Neuron 25:\n",
      "    Bias (b): -0.0040\n",
      "    Weights (w): -0.4559, 0.4484, 0.6145, -0.7023, 0.5206, -0.1560, -0.2073, 0.5454, 0.8446, -0.1124, -0.2280, 0.6839, -0.9735, -0.3651, -0.5223, 0.5808, 0.1360, -0.1616, 0.0612, 0.4243, 0.7866, -0.4118, 0.4720, 0.2067, 0.5159, 0.9581, -0.6719, -0.1370, 0.8344, -0.9125\n",
      "  Neuron 26:\n",
      "    Bias (b): -0.7206\n",
      "    Weights (w): 0.4209, 0.3884, -0.2517, 0.4602, -0.3883, 0.2788, 0.6673, -0.8618, -1.0043, -0.6934, 0.2223, 0.7962, 0.6940, -1.0388, 0.5642, -0.7153, -0.5977, 0.9599, -0.0692, 0.3872, -0.7708, 0.9810, 0.4187, -0.0156, 0.7385, 0.3426, -0.4469, -0.4753, 0.5203, -0.0061\n",
      "  Neuron 27:\n",
      "    Bias (b): 0.3505\n",
      "    Weights (w): -0.5381, 0.9313, -0.0219, -0.3265, 0.1017, 0.3196, -0.3675, 0.1238, 0.5913, -0.6171, 0.1278, 0.6503, 0.2321, 0.7037, -0.4406, -0.8891, 0.5340, -0.8395, 0.2801, 0.3562, -0.7602, 0.0058, -0.5164, -0.1151, 1.0349, -0.2308, -0.5485, -0.8176, -0.1354, 0.0952\n",
      "  Neuron 28:\n",
      "    Bias (b): -0.1384\n",
      "    Weights (w): 0.6998, -0.3289, -0.8945, -0.0357, 0.0431, -0.5395, 0.9188, 0.9200, 0.9245, -0.9624, -0.9650, -0.6240, -0.1195, -0.5062, 0.5357, 0.4646, -0.7086, -0.9015, -0.4313, -0.2494, 0.4113, -0.0419, -0.0367, -0.5113, 0.9554, -0.3948, -0.0907, -0.6861, -0.8906, 0.9682\n",
      "  Neuron 29:\n",
      "    Bias (b): -0.2755\n",
      "    Weights (w): -0.0410, -0.5088, 0.6307, -0.9809, -0.2226, -0.8827, -0.1412, 0.1146, 0.3023, 0.8767, 0.5738, -0.2360, 0.4520, -0.1878, 0.1811, 0.3622, 0.8611, -0.7056, 0.1848, -0.6207, 0.3091, 0.5683, -0.7559, -0.2197, 0.7234, -0.3202, -0.7305, -0.9861, 0.7024, 0.3418\n",
      "  Neuron 30:\n",
      "    Bias (b): 0.7237\n",
      "    Weights (w): 0.2426, 0.1659, 0.1280, 0.4917, -0.9662, -0.8602, -0.6031, 0.5182, 0.3268, -0.6242, -0.1533, -0.7575, -0.6311, -0.0905, 0.6396, 0.1520, 0.2505, -0.7174, -0.8424, 0.1016, 0.5618, 0.8475, -0.1920, -0.3114, 0.2669, 0.2737, 0.8083, -0.7357, 0.2361, -0.9320\n",
      "\n",
      "Layer 4:\n",
      "  Neuron 1:\n",
      "    Bias (b): 0.7137\n",
      "    Weights (w): 0.7308, 0.4780, -0.2953, 0.9628, 0.9801, -0.4140, -0.8294, -0.9518, 0.3008, 0.6925, -0.0805, -0.6238, 0.4291, 0.5944, 0.2469, -0.4731, -1.0337, 0.7425, -0.9510, -0.4527, 0.5034, -0.6280, -0.0992, -0.0679, -0.3726, -0.1164, 0.1508, -0.9856, -0.6201, -0.0046\n",
      "  Neuron 2:\n",
      "    Bias (b): 0.2068\n",
      "    Weights (w): -0.3744, -0.7652, 0.7170, -1.1048, -0.0182, 0.9858, -0.2632, 0.6139, -0.8018, -0.5229, 0.1667, 0.8538, 0.2992, 0.0426, 0.3761, 0.7366, -0.7264, 0.3844, -0.2727, 0.9208, -0.5175, 0.4908, 0.3214, -1.1325, 0.7878, -0.4000, 0.2366, -0.5414, -0.9029, 0.8661\n",
      "  Neuron 3:\n",
      "    Bias (b): 0.9600\n",
      "    Weights (w): 1.0610, 0.5702, 0.0657, 0.9541, -0.1764, 0.4075, -0.7527, -0.7186, -0.0630, -0.4101, 0.9552, -0.2518, 0.5226, 0.0313, 0.0107, 0.8265, 0.1085, -0.0297, 0.2566, -0.7696, -0.6081, -0.3910, 0.1428, -0.5692, -0.6207, -0.9251, -0.6321, 0.2834, -0.4239, 0.4070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from print_nn import print_neural_network_parameters\n",
    "print_neural_network_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91087390",
   "metadata": {},
   "source": [
    "# Wpływ liczby warstw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5d3d191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, mean loss: 13795.081603768924\n",
      "Epoch: 1, mean loss: 18790.044587445864\n",
      "Epoch: 2, mean loss: 13603.02958050908\n",
      "Epoch: 3, mean loss: 15219.860738315094\n",
      "Epoch: 4, mean loss: 14045.948479637595\n",
      "Epoch: 5, mean loss: 12034.55258836963\n",
      "Epoch: 6, mean loss: 21067.201432845693\n",
      "Epoch: 7, mean loss: 11636.447668467887\n",
      "Epoch: 8, mean loss: 14534.168468932894\n",
      "Epoch: 9, mean loss: 22099.34926747066\n",
      "Epoch: 10, mean loss: 12343.602036656031\n",
      "Epoch: 11, mean loss: 14865.54056740736\n",
      "Epoch: 12, mean loss: 13666.718280042864\n",
      "Epoch: 13, mean loss: 15683.492422534295\n",
      "Epoch: 14, mean loss: 14716.145941850305\n",
      "Epoch: 15, mean loss: 12636.960763155306\n",
      "Epoch: 16, mean loss: 17540.571806138392\n",
      "Epoch: 17, mean loss: 17283.44960744119\n",
      "Epoch: 18, mean loss: 15641.744699632876\n",
      "Epoch: 19, mean loss: 19069.652362566423\n",
      "Epoch: 20, mean loss: 17315.35343442699\n",
      "Epoch: 21, mean loss: 13428.204635225295\n",
      "Epoch: 22, mean loss: 12566.489675743689\n",
      "Epoch: 23, mean loss: 17048.998626896348\n",
      "Epoch: 24, mean loss: 24065.636383703557\n",
      "Epoch: 25, mean loss: 18430.406348688164\n",
      "Epoch: 26, mean loss: 14659.739051386647\n",
      "Epoch: 27, mean loss: 21663.032949409233\n",
      "Epoch: 28, mean loss: 11182.774961426516\n",
      "Epoch: 29, mean loss: 18174.3098057044\n",
      "Epoch: 30, mean loss: 15193.21454789526\n",
      "Epoch: 31, mean loss: 17820.653094581467\n",
      "Epoch: 32, mean loss: 17705.850997144946\n",
      "Epoch: 33, mean loss: 19534.45947102183\n",
      "Epoch: 34, mean loss: 14696.819717186429\n",
      "Epoch: 35, mean loss: 12628.052450368989\n",
      "Epoch: 36, mean loss: 14765.841601411397\n",
      "Epoch: 37, mean loss: 15610.29668491104\n",
      "Epoch: 38, mean loss: 18116.78532488984\n",
      "Epoch: 39, mean loss: 16102.041505527957\n",
      "Epoch: 40, mean loss: 16013.068457288493\n",
      "Epoch: 41, mean loss: 15520.5206200427\n",
      "Epoch: 42, mean loss: 17677.820574394766\n",
      "Epoch: 43, mean loss: 13828.7772991285\n",
      "Epoch: 44, mean loss: 13116.825701148698\n",
      "Epoch: 45, mean loss: 12405.94548638216\n",
      "Epoch: 46, mean loss: 18493.634687769223\n",
      "Epoch: 47, mean loss: 16013.458978684126\n",
      "Epoch: 48, mean loss: 14153.08371766883\n",
      "Epoch: 49, mean loss: 10551.367941130735\n",
      "Epoch: 50, mean loss: 16689.905644110862\n",
      "Epoch: 51, mean loss: 12456.198256857462\n",
      "Epoch: 52, mean loss: 14379.203829918783\n",
      "Epoch: 53, mean loss: 16658.8342522501\n",
      "Epoch: 54, mean loss: 14946.676093613045\n",
      "Epoch: 55, mean loss: 14624.760351312585\n",
      "Epoch: 56, mean loss: 21034.119158548463\n",
      "Epoch: 57, mean loss: 15220.079763410715\n",
      "Epoch: 58, mean loss: 23990.82796458319\n",
      "Epoch: 59, mean loss: 21003.98593240293\n",
      "Epoch: 60, mean loss: 15874.40343063961\n",
      "Epoch: 61, mean loss: 13928.914701799957\n",
      "Epoch: 62, mean loss: 10422.941072256806\n",
      "Epoch: 63, mean loss: 13964.479422646742\n",
      "Epoch: 64, mean loss: 13087.788872638543\n",
      "Epoch: 65, mean loss: 22672.339468206163\n",
      "Epoch: 66, mean loss: 18518.684822529438\n",
      "Epoch: 67, mean loss: 19005.39706688791\n",
      "Epoch: 68, mean loss: 11179.104787426535\n",
      "Epoch: 69, mean loss: 16424.696461641117\n",
      "Epoch: 70, mean loss: 17414.65630452777\n",
      "Epoch: 71, mean loss: 13114.109215995952\n",
      "Epoch: 72, mean loss: 12070.279365177896\n",
      "Epoch: 73, mean loss: 14052.371538158604\n",
      "Epoch: 74, mean loss: 11356.130273520732\n",
      "Epoch: 75, mean loss: 13317.730374949504\n",
      "Epoch: 76, mean loss: 18023.89077115331\n",
      "Epoch: 77, mean loss: 16829.352791660735\n",
      "Epoch: 78, mean loss: 13191.041086750894\n",
      "Epoch: 79, mean loss: 12473.728903822981\n",
      "Epoch: 80, mean loss: 15641.476665964323\n",
      "Epoch: 81, mean loss: 17665.266275890444\n",
      "Epoch: 82, mean loss: 19651.68260311142\n",
      "Epoch: 83, mean loss: 17307.84922075944\n",
      "Epoch: 84, mean loss: 21281.479498921028\n",
      "Epoch: 85, mean loss: 18508.76723851107\n",
      "Epoch: 86, mean loss: 11895.251768112756\n",
      "Epoch: 87, mean loss: 20697.75263624262\n",
      "Epoch: 88, mean loss: 16909.374189778988\n",
      "Epoch: 89, mean loss: 13266.787865463284\n",
      "Epoch: 90, mean loss: 12360.278350218541\n",
      "Epoch: 91, mean loss: 15486.41535772079\n",
      "Epoch: 92, mean loss: 17295.01063011025\n",
      "Epoch: 93, mean loss: 15403.216599089159\n",
      "Epoch: 94, mean loss: 17619.373967857176\n",
      "Epoch: 95, mean loss: 15485.807829697414\n",
      "Epoch: 96, mean loss: 11499.607930825277\n",
      "Epoch: 97, mean loss: 15344.572402284266\n",
      "Epoch: 98, mean loss: 9030.873296088233\n",
      "Epoch: 99, mean loss: 15908.253694650357\n",
      "Epoch: 100, mean loss: 16779.47666931995\n",
      "Epoch: 101, mean loss: 14801.681814407499\n",
      "Epoch: 102, mean loss: 16073.18293368908\n",
      "Epoch: 103, mean loss: 15498.302663168031\n",
      "Epoch: 104, mean loss: 16052.52051394354\n",
      "Epoch: 105, mean loss: 15343.880183138428\n",
      "Epoch: 106, mean loss: 15037.851804903368\n",
      "Epoch: 107, mean loss: 26377.658322934072\n",
      "Epoch: 108, mean loss: 13192.65502808046\n",
      "Epoch: 109, mean loss: 13907.261016738024\n",
      "Epoch: 110, mean loss: 13236.440216309846\n",
      "Epoch: 111, mean loss: 17110.92312206146\n",
      "Epoch: 112, mean loss: 16030.831568767819\n",
      "Epoch: 113, mean loss: 21044.92263275913\n",
      "Epoch: 114, mean loss: 14487.030587589079\n",
      "Epoch: 115, mean loss: 15328.148024636612\n",
      "Epoch: 116, mean loss: 19017.680229376245\n",
      "Epoch: 117, mean loss: 16115.362789310846\n",
      "Epoch: 118, mean loss: 17005.72564313554\n",
      "Epoch: 119, mean loss: 17922.002894490106\n",
      "Epoch: 120, mean loss: 16049.896176765795\n",
      "Epoch: 121, mean loss: 12383.004746783625\n",
      "Epoch: 122, mean loss: 12598.544118976912\n",
      "Epoch: 123, mean loss: 21384.376087568\n",
      "Epoch: 124, mean loss: 17914.174593312793\n",
      "Epoch: 125, mean loss: 11458.125760607347\n",
      "Epoch: 126, mean loss: 16557.73610217083\n",
      "Epoch: 127, mean loss: 12603.88291334128\n",
      "Epoch: 128, mean loss: 18564.368668324503\n",
      "Epoch: 129, mean loss: 15522.1887999565\n",
      "Epoch: 130, mean loss: 16988.840331485913\n",
      "Epoch: 131, mean loss: 16291.275430922664\n",
      "Epoch: 132, mean loss: 14727.622499557738\n",
      "Epoch: 133, mean loss: 19193.63522029694\n",
      "Epoch: 134, mean loss: 17210.62364983499\n",
      "Epoch: 135, mean loss: 13519.713182712969\n",
      "Epoch: 136, mean loss: 16133.65646593888\n",
      "Epoch: 137, mean loss: 18844.436280296977\n",
      "Epoch: 138, mean loss: 16937.60267071991\n",
      "Epoch: 139, mean loss: 15150.918086840547\n",
      "Epoch: 140, mean loss: 13746.839638582223\n",
      "Epoch: 141, mean loss: 21281.09133007602\n",
      "Epoch: 142, mean loss: 15358.436368690369\n",
      "Epoch: 143, mean loss: 13674.127034469942\n",
      "Epoch: 144, mean loss: 16379.539571791986\n",
      "Epoch: 145, mean loss: 17890.448331661773\n",
      "Epoch: 146, mean loss: 13185.151744273577\n",
      "Epoch: 147, mean loss: 12909.714965047348\n",
      "Epoch: 148, mean loss: 13253.769039911682\n",
      "Epoch: 149, mean loss: 13739.367191891837\n",
      "Epoch: 150, mean loss: 13768.954560217438\n",
      "Epoch: 151, mean loss: 20972.37983527587\n",
      "Epoch: 152, mean loss: 15984.153795524053\n",
      "Epoch: 153, mean loss: 16630.308738615884\n",
      "Epoch: 154, mean loss: 15471.135484803166\n",
      "Epoch: 155, mean loss: 17304.018618532627\n",
      "Epoch: 156, mean loss: 13258.732398104714\n",
      "Epoch: 157, mean loss: 10146.075576690217\n",
      "Epoch: 158, mean loss: 17236.207705345143\n",
      "Epoch: 159, mean loss: 13492.688435901362\n",
      "Epoch: 160, mean loss: 15751.67824071451\n",
      "Epoch: 161, mean loss: 13900.362400690125\n",
      "Epoch: 162, mean loss: 17999.19210396173\n",
      "Epoch: 163, mean loss: 17546.115064443093\n",
      "Epoch: 164, mean loss: 18552.22941108746\n",
      "Epoch: 165, mean loss: 15471.856581652695\n",
      "Epoch: 166, mean loss: 8700.790673824702\n",
      "Epoch: 167, mean loss: 13330.156592407371\n",
      "Epoch: 168, mean loss: 18632.576588094627\n",
      "Epoch: 169, mean loss: 13962.07422134476\n",
      "Epoch: 170, mean loss: 14870.965855177807\n",
      "Epoch: 171, mean loss: 12058.178980111543\n",
      "Epoch: 172, mean loss: 13014.311385444664\n",
      "Epoch: 173, mean loss: 13023.225084184722\n",
      "Epoch: 174, mean loss: 13981.430751328817\n",
      "Epoch: 175, mean loss: 15562.900114456617\n",
      "Epoch: 176, mean loss: 13519.762905384494\n",
      "Epoch: 177, mean loss: 19024.26733375335\n",
      "Epoch: 178, mean loss: 16561.846234454675\n",
      "Epoch: 179, mean loss: 13992.241965962126\n",
      "Epoch: 180, mean loss: 15444.331664730435\n",
      "Epoch: 181, mean loss: 13563.531597690513\n",
      "Epoch: 182, mean loss: 15128.303215655857\n",
      "Epoch: 183, mean loss: 16742.28930220945\n",
      "Epoch: 184, mean loss: 12065.980346370083\n",
      "Epoch: 185, mean loss: 12367.514839381693\n",
      "Epoch: 186, mean loss: 16177.799386226332\n",
      "Epoch: 187, mean loss: 19771.66568691377\n",
      "Epoch: 188, mean loss: 14509.768603244207\n",
      "Epoch: 189, mean loss: 13510.350503273947\n",
      "Epoch: 190, mean loss: 14917.82152822145\n",
      "Epoch: 191, mean loss: 13216.805543660774\n",
      "Epoch: 192, mean loss: 11021.936887531298\n",
      "Epoch: 193, mean loss: 16653.848243204247\n",
      "Epoch: 194, mean loss: 14552.150622451685\n",
      "Epoch: 195, mean loss: 15532.579686616557\n",
      "Epoch: 196, mean loss: 13496.75180499607\n",
      "Epoch: 197, mean loss: 16443.78732549407\n",
      "Epoch: 198, mean loss: 18252.375211593906\n",
      "Epoch: 199, mean loss: 17658.855029241633\n",
      "Epoch: 200, mean loss: 18427.961100899352\n",
      "Epoch: 201, mean loss: 14586.644723318619\n",
      "Epoch: 202, mean loss: 16251.344525333467\n",
      "Epoch: 203, mean loss: 17497.518861936594\n",
      "Epoch: 204, mean loss: 15062.611773195616\n",
      "Epoch: 205, mean loss: 17360.619082876965\n",
      "Epoch: 206, mean loss: 14100.760900465068\n",
      "Epoch: 207, mean loss: 13081.126124087332\n",
      "Epoch: 208, mean loss: 19482.712640687576\n",
      "Epoch: 209, mean loss: 19062.65069820584\n",
      "Epoch: 210, mean loss: 12672.592547555605\n",
      "Epoch: 211, mean loss: 16977.657912053503\n",
      "Epoch: 212, mean loss: 11702.662003112877\n",
      "Epoch: 213, mean loss: 13730.605126907616\n",
      "Epoch: 214, mean loss: 18901.818503564136\n",
      "Epoch: 215, mean loss: 17041.521534050495\n",
      "Epoch: 216, mean loss: 14018.60193634591\n",
      "Epoch: 217, mean loss: 10963.276623271631\n",
      "Epoch: 218, mean loss: 20189.80789535826\n",
      "Epoch: 219, mean loss: 15541.959877275713\n",
      "Epoch: 220, mean loss: 14330.923828685853\n",
      "Epoch: 221, mean loss: 13572.577784700969\n",
      "Epoch: 222, mean loss: 13711.442554144789\n",
      "Epoch: 223, mean loss: 22159.67503721563\n",
      "Epoch: 224, mean loss: 18549.25873103888\n",
      "Epoch: 225, mean loss: 12437.67232040428\n",
      "Epoch: 226, mean loss: 15123.840915754494\n",
      "Epoch: 227, mean loss: 13512.658651169093\n",
      "Epoch: 228, mean loss: 21187.00396671505\n",
      "Epoch: 229, mean loss: 20820.789903575333\n",
      "Epoch: 230, mean loss: 14154.09942867814\n",
      "Epoch: 231, mean loss: 13491.672306722872\n",
      "Epoch: 232, mean loss: 14465.641726384372\n",
      "Epoch: 233, mean loss: 11360.78150799593\n",
      "Epoch: 234, mean loss: 14736.511823515746\n",
      "Epoch: 235, mean loss: 20934.954479844826\n",
      "Epoch: 236, mean loss: 20682.03073440189\n",
      "Epoch: 237, mean loss: 18146.140210749047\n",
      "Epoch: 238, mean loss: 16920.7741679744\n",
      "Epoch: 239, mean loss: 14255.082791419467\n",
      "Epoch: 240, mean loss: 16629.421831578296\n",
      "Epoch: 241, mean loss: 21727.33079006283\n",
      "Epoch: 242, mean loss: 12965.736956691791\n",
      "Epoch: 243, mean loss: 12046.757567211842\n",
      "Epoch: 244, mean loss: 13449.41261475135\n",
      "Epoch: 245, mean loss: 19523.757151453723\n",
      "Epoch: 246, mean loss: 15937.030212725578\n",
      "Epoch: 247, mean loss: 20133.792418677993\n",
      "Epoch: 248, mean loss: 16040.215965441677\n",
      "Epoch: 249, mean loss: 15696.832238195446\n",
      "Epoch: 250, mean loss: 12416.088173023869\n",
      "Epoch: 251, mean loss: 20191.22562463735\n",
      "Epoch: 252, mean loss: 19450.59037875663\n",
      "Epoch: 253, mean loss: 17922.84022146304\n",
      "Epoch: 254, mean loss: 12693.444118682586\n",
      "Epoch: 255, mean loss: 17067.590480709456\n",
      "Epoch: 256, mean loss: 15170.919072099416\n",
      "Epoch: 257, mean loss: 15362.305461270089\n",
      "Epoch: 258, mean loss: 19066.905276556812\n",
      "Epoch: 259, mean loss: 21216.482611507457\n",
      "Epoch: 260, mean loss: 13608.588341345867\n",
      "Epoch: 261, mean loss: 20113.405691418873\n",
      "Epoch: 262, mean loss: 12995.194918315774\n",
      "Epoch: 263, mean loss: 15028.668456520445\n",
      "Epoch: 264, mean loss: 13975.026901282446\n",
      "Epoch: 265, mean loss: 13020.961375425273\n",
      "Epoch: 266, mean loss: 18817.15149771247\n",
      "Epoch: 267, mean loss: 18283.081503278998\n",
      "Epoch: 268, mean loss: 13983.93102667204\n",
      "Epoch: 269, mean loss: 13147.770054324505\n",
      "Epoch: 270, mean loss: 15240.721990944585\n",
      "Epoch: 271, mean loss: 12852.478702213972\n",
      "Epoch: 272, mean loss: 18463.136040876117\n",
      "Epoch: 273, mean loss: 24025.010988336475\n",
      "Epoch: 274, mean loss: 10743.02805161053\n",
      "Epoch: 275, mean loss: 9392.665246001887\n",
      "Epoch: 276, mean loss: 19888.063447841585\n",
      "Epoch: 277, mean loss: 14565.726320154807\n",
      "Epoch: 278, mean loss: 13501.396498657075\n",
      "Epoch: 279, mean loss: 20129.100030736387\n",
      "Epoch: 280, mean loss: 12532.494662808735\n",
      "Epoch: 281, mean loss: 20863.332243207158\n",
      "Epoch: 282, mean loss: 15148.305870174956\n",
      "Epoch: 283, mean loss: 14470.059589102555\n",
      "Epoch: 284, mean loss: 15487.181715062718\n",
      "Epoch: 285, mean loss: 10020.196355411339\n",
      "Epoch: 286, mean loss: 18306.510686322013\n",
      "Epoch: 287, mean loss: 16778.256126019252\n",
      "Epoch: 288, mean loss: 12047.751319605157\n",
      "Epoch: 289, mean loss: 16827.550838302715\n",
      "Epoch: 290, mean loss: 17879.70853080328\n",
      "Epoch: 291, mean loss: 16224.789942304165\n",
      "Epoch: 292, mean loss: 16520.45511073219\n",
      "Epoch: 293, mean loss: 15863.802996840055\n",
      "Epoch: 294, mean loss: 15677.501856567465\n",
      "Epoch: 295, mean loss: 12360.662467909095\n",
      "Epoch: 296, mean loss: 11930.57081432458\n",
      "Epoch: 297, mean loss: 15824.621915192793\n",
      "Epoch: 298, mean loss: 14451.89771580031\n",
      "Epoch: 299, mean loss: 13501.607756036117\n",
      "Epoch: 300, mean loss: 19753.23483342714\n",
      "Epoch: 301, mean loss: 18530.52207565473\n",
      "Epoch: 302, mean loss: 16679.03084528904\n",
      "Epoch: 303, mean loss: 16970.648432858452\n",
      "Epoch: 304, mean loss: 18244.00101210302\n",
      "Epoch: 305, mean loss: 15991.86882775722\n",
      "Epoch: 306, mean loss: 18976.384873387375\n",
      "Epoch: 307, mean loss: 23921.67772197481\n",
      "Epoch: 308, mean loss: 14060.020426766201\n",
      "Epoch: 309, mean loss: 18306.401481357287\n",
      "Epoch: 310, mean loss: 12457.857015278832\n",
      "Epoch: 311, mean loss: 10280.785931249231\n",
      "Epoch: 312, mean loss: 16498.89690421639\n",
      "Epoch: 313, mean loss: 11707.516341549883\n",
      "Epoch: 314, mean loss: 17726.018276667302\n",
      "Epoch: 315, mean loss: 23348.646864086168\n",
      "Epoch: 316, mean loss: 15551.280455061633\n",
      "Epoch: 317, mean loss: 15737.137904649599\n",
      "Epoch: 318, mean loss: 13845.115283344385\n",
      "Epoch: 319, mean loss: 13871.38811779876\n",
      "Epoch: 320, mean loss: 20288.735674748525\n",
      "Epoch: 321, mean loss: 12457.646045492867\n",
      "Epoch: 322, mean loss: 18737.03644532351\n",
      "Epoch: 323, mean loss: 12792.957812499437\n",
      "Epoch: 324, mean loss: 11141.295902537147\n",
      "Epoch: 325, mean loss: 11510.054097765897\n",
      "Epoch: 326, mean loss: 16875.584731338018\n",
      "Epoch: 327, mean loss: 14135.883710409773\n",
      "Epoch: 328, mean loss: 11168.945877227048\n",
      "Epoch: 329, mean loss: 21730.202902242938\n",
      "Epoch: 330, mean loss: 17169.847560164857\n",
      "Epoch: 331, mean loss: 12934.937110986213\n",
      "Epoch: 332, mean loss: 10991.560012348013\n",
      "Epoch: 333, mean loss: 15790.667219930529\n",
      "Epoch: 334, mean loss: 15566.235438003665\n",
      "Epoch: 335, mean loss: 13553.044860656883\n",
      "Epoch: 336, mean loss: 19398.179591916476\n",
      "Epoch: 337, mean loss: 14322.566073547336\n",
      "Epoch: 338, mean loss: 15347.375714267355\n",
      "Epoch: 339, mean loss: 18547.187565254575\n",
      "Epoch: 340, mean loss: 12661.697998894853\n",
      "Epoch: 341, mean loss: 15796.636559220406\n",
      "Epoch: 342, mean loss: 20775.217804171192\n",
      "Epoch: 343, mean loss: 17798.510769593162\n",
      "Epoch: 344, mean loss: 19633.09889979926\n",
      "Epoch: 345, mean loss: 15401.787673220964\n",
      "Epoch: 346, mean loss: 14697.132966249344\n",
      "Epoch: 347, mean loss: 17693.963617564175\n",
      "Epoch: 348, mean loss: 16827.941146370067\n",
      "Epoch: 349, mean loss: 15435.01162379673\n",
      "Epoch: 350, mean loss: 14785.64453500095\n",
      "Epoch: 351, mean loss: 17199.957970906464\n",
      "Epoch: 352, mean loss: 14742.724553728229\n",
      "Epoch: 353, mean loss: 19577.791502361637\n",
      "Epoch: 354, mean loss: 18048.79546842035\n",
      "Epoch: 355, mean loss: 16417.249346695953\n",
      "Epoch: 356, mean loss: 15339.87590612372\n",
      "Epoch: 357, mean loss: 17650.298331254155\n",
      "Epoch: 358, mean loss: 13678.31491984043\n",
      "Epoch: 359, mean loss: 13907.805424053055\n",
      "Epoch: 360, mean loss: 16560.82774003873\n",
      "Epoch: 361, mean loss: 19190.0635792188\n",
      "Epoch: 362, mean loss: 15589.221316886516\n",
      "Epoch: 363, mean loss: 19113.798478382614\n",
      "Epoch: 364, mean loss: 10715.912525352709\n",
      "Epoch: 365, mean loss: 17179.423078922617\n",
      "Epoch: 366, mean loss: 15216.192510793288\n",
      "Epoch: 367, mean loss: 15813.457076318224\n",
      "Epoch: 368, mean loss: 16492.05396380492\n",
      "Epoch: 369, mean loss: 13266.929339956301\n",
      "Epoch: 370, mean loss: 20488.441398278977\n",
      "Epoch: 371, mean loss: 13910.923894083444\n",
      "Epoch: 372, mean loss: 14518.280571399597\n",
      "Epoch: 373, mean loss: 14758.73145730648\n",
      "Epoch: 374, mean loss: 15511.324766968515\n",
      "Epoch: 375, mean loss: 14638.362068099392\n",
      "Epoch: 376, mean loss: 12707.047792550415\n",
      "Epoch: 377, mean loss: 15255.580629155897\n",
      "Epoch: 378, mean loss: 17659.335740992\n",
      "Epoch: 379, mean loss: 21478.768061742867\n",
      "Epoch: 380, mean loss: 12757.174371594598\n",
      "Epoch: 381, mean loss: 12424.944491182032\n",
      "Epoch: 382, mean loss: 24913.854195459586\n",
      "Epoch: 383, mean loss: 19152.00383582463\n",
      "Epoch: 384, mean loss: 16400.351638442142\n",
      "Epoch: 385, mean loss: 10234.940242288634\n",
      "Epoch: 386, mean loss: 15059.272669937991\n",
      "Epoch: 387, mean loss: 16605.487523343876\n",
      "Epoch: 388, mean loss: 14955.195992699744\n",
      "Epoch: 389, mean loss: 15627.06575140842\n",
      "Epoch: 390, mean loss: 16953.735574393497\n",
      "Epoch: 391, mean loss: 17983.725867333593\n",
      "Epoch: 392, mean loss: 17697.612939711034\n",
      "Epoch: 393, mean loss: 15619.541379176506\n",
      "Epoch: 394, mean loss: 12376.6485889032\n",
      "Epoch: 395, mean loss: 12821.655924635937\n",
      "Epoch: 396, mean loss: 17325.507054414215\n",
      "Epoch: 397, mean loss: 16681.225765987907\n",
      "Epoch: 398, mean loss: 14738.799629931402\n",
      "Epoch: 399, mean loss: 17495.70215978156\n",
      "Epoch: 400, mean loss: 16127.22775665643\n",
      "Epoch: 401, mean loss: 12790.284839168471\n",
      "Epoch: 402, mean loss: 17731.018472760174\n",
      "Epoch: 403, mean loss: 15530.702361189054\n",
      "Epoch: 404, mean loss: 21602.361367051708\n",
      "Epoch: 405, mean loss: 19688.901723651314\n",
      "Epoch: 406, mean loss: 15102.41011679719\n",
      "Epoch: 407, mean loss: 15114.619658240374\n",
      "Epoch: 408, mean loss: 21270.656756685836\n",
      "Epoch: 409, mean loss: 13886.189158554065\n",
      "Epoch: 410, mean loss: 13188.835895973747\n",
      "Epoch: 411, mean loss: 17175.294401955794\n",
      "Epoch: 412, mean loss: 16781.3319993394\n",
      "Epoch: 413, mean loss: 16313.98356940758\n",
      "Epoch: 414, mean loss: 15736.109376349446\n",
      "Epoch: 415, mean loss: 15014.369496082982\n",
      "Epoch: 416, mean loss: 14136.067670887687\n",
      "Epoch: 417, mean loss: 10778.474251598416\n",
      "Epoch: 418, mean loss: 13543.25958204906\n",
      "Epoch: 419, mean loss: 12233.68328723839\n",
      "Epoch: 420, mean loss: 11987.737165231983\n",
      "Epoch: 421, mean loss: 14395.226923115773\n",
      "Epoch: 422, mean loss: 14561.081818953426\n",
      "Epoch: 423, mean loss: 14744.911031878706\n",
      "Epoch: 424, mean loss: 18548.73776338539\n",
      "Epoch: 425, mean loss: 17906.872237505133\n",
      "Epoch: 426, mean loss: 10075.206091054235\n",
      "Epoch: 427, mean loss: 7203.569081222243\n",
      "Epoch: 428, mean loss: 13805.75153137869\n",
      "Epoch: 429, mean loss: 13964.620501737843\n",
      "Epoch: 430, mean loss: 16597.08759042958\n",
      "Epoch: 431, mean loss: 17956.198240418522\n",
      "Epoch: 432, mean loss: 16859.558440408095\n",
      "Epoch: 433, mean loss: 18802.904238654915\n",
      "Epoch: 434, mean loss: 14831.650035414781\n",
      "Epoch: 435, mean loss: 17251.743925490042\n",
      "Epoch: 436, mean loss: 15105.313892572867\n",
      "Epoch: 437, mean loss: 18785.91084859602\n",
      "Epoch: 438, mean loss: 14565.58375141729\n",
      "Epoch: 439, mean loss: 14500.344248432988\n",
      "Epoch: 440, mean loss: 15668.470874694336\n",
      "Epoch: 441, mean loss: 10041.875883173534\n",
      "Epoch: 442, mean loss: 13700.133324791037\n",
      "Epoch: 443, mean loss: 14223.99741945043\n",
      "Epoch: 444, mean loss: 17802.2018363969\n",
      "Epoch: 445, mean loss: 17335.240034060356\n",
      "Epoch: 446, mean loss: 15822.060430719263\n",
      "Epoch: 447, mean loss: 14490.728747956988\n",
      "Epoch: 448, mean loss: 14586.097342503463\n",
      "Epoch: 449, mean loss: 14476.642342752924\n",
      "Epoch: 450, mean loss: 12900.723118613041\n",
      "Epoch: 451, mean loss: 12506.626498043559\n",
      "Epoch: 452, mean loss: 15071.152522543463\n",
      "Epoch: 453, mean loss: 14332.485684993178\n",
      "Epoch: 454, mean loss: 16196.532994538504\n",
      "Epoch: 455, mean loss: 11657.831944458576\n",
      "Epoch: 456, mean loss: 16517.184324253663\n",
      "Epoch: 457, mean loss: 19146.46414856648\n",
      "Epoch: 458, mean loss: 19772.142180113424\n",
      "Epoch: 459, mean loss: 18816.583289210874\n",
      "Epoch: 460, mean loss: 19310.79921632779\n",
      "Epoch: 461, mean loss: 19735.663881209475\n",
      "Epoch: 462, mean loss: 15761.15591299586\n",
      "Epoch: 463, mean loss: 16757.513378771677\n",
      "Epoch: 464, mean loss: 13726.747410353182\n",
      "Epoch: 465, mean loss: 19333.910506573968\n",
      "Epoch: 466, mean loss: 14651.830100501915\n",
      "Epoch: 467, mean loss: 16890.747836152703\n",
      "Epoch: 468, mean loss: 11598.064038258713\n",
      "Epoch: 469, mean loss: 17030.0697539467\n",
      "Epoch: 470, mean loss: 16425.611086049186\n",
      "Epoch: 471, mean loss: 17470.208567555175\n",
      "Epoch: 472, mean loss: 16226.159107564818\n",
      "Epoch: 473, mean loss: 10872.429886830845\n",
      "Epoch: 474, mean loss: 16033.205517130873\n",
      "Epoch: 475, mean loss: 15327.02980654275\n",
      "Epoch: 476, mean loss: 17139.438940529748\n",
      "Epoch: 477, mean loss: 11282.011379617124\n",
      "Epoch: 478, mean loss: 22067.59498746769\n",
      "Epoch: 479, mean loss: 15962.072767069843\n",
      "Epoch: 480, mean loss: 18683.933661427636\n",
      "Epoch: 481, mean loss: 16620.83108563148\n",
      "Epoch: 482, mean loss: 11227.639875729139\n",
      "Epoch: 483, mean loss: 19872.499932370396\n",
      "Epoch: 484, mean loss: 18231.81901695686\n",
      "Epoch: 485, mean loss: 15614.144749528337\n",
      "Epoch: 486, mean loss: 13984.647889803213\n",
      "Epoch: 487, mean loss: 14406.34292772698\n",
      "Epoch: 488, mean loss: 15092.648586119434\n",
      "Epoch: 489, mean loss: 19086.615271690694\n",
      "Epoch: 490, mean loss: 16769.31025622865\n",
      "Epoch: 491, mean loss: 13313.342939609132\n",
      "Epoch: 492, mean loss: 15515.130485434336\n",
      "Epoch: 493, mean loss: 13641.954224942412\n",
      "Epoch: 494, mean loss: 16826.027085336726\n",
      "Epoch: 495, mean loss: 14703.192436723286\n",
      "Epoch: 496, mean loss: 18158.405554762405\n",
      "Epoch: 497, mean loss: 18579.874527966072\n",
      "Epoch: 498, mean loss: 15649.511398975013\n",
      "Epoch: 499, mean loss: 11239.340060034807\n",
      "Done\n",
      "Epoch: 0, mean loss: 16818.193341061917\n",
      "Epoch: 1, mean loss: 18482.364723517163\n",
      "Epoch: 2, mean loss: 14308.34298301453\n",
      "Epoch: 3, mean loss: 15584.626383643605\n",
      "Epoch: 4, mean loss: 9457.235308689864\n",
      "Epoch: 5, mean loss: 18069.1637924356\n",
      "Epoch: 6, mean loss: 17791.08387424252\n",
      "Epoch: 7, mean loss: 11448.018930862176\n",
      "Epoch: 8, mean loss: 19196.031432556018\n",
      "Epoch: 9, mean loss: 18372.99262105522\n",
      "Epoch: 10, mean loss: 12482.197621955685\n",
      "Epoch: 11, mean loss: 15920.047785513518\n",
      "Epoch: 12, mean loss: 16551.94382183954\n",
      "Epoch: 13, mean loss: 14411.15415692478\n",
      "Epoch: 14, mean loss: 14008.76135128535\n",
      "Epoch: 15, mean loss: 16554.610884729\n",
      "Epoch: 16, mean loss: 20709.93398516592\n",
      "Epoch: 17, mean loss: 14273.402737951716\n",
      "Epoch: 18, mean loss: 16336.926612035379\n",
      "Epoch: 19, mean loss: 20252.637857875703\n",
      "Epoch: 20, mean loss: 16641.201569604116\n",
      "Epoch: 21, mean loss: 12099.783183608484\n",
      "Epoch: 22, mean loss: 14687.828414432402\n",
      "Epoch: 23, mean loss: 17806.02894155713\n",
      "Epoch: 24, mean loss: 26229.959228511503\n",
      "Epoch: 25, mean loss: 13837.428971658353\n",
      "Epoch: 26, mean loss: 21001.252417556687\n",
      "Epoch: 27, mean loss: 18376.30956848811\n",
      "Epoch: 28, mean loss: 14387.396400952073\n",
      "Epoch: 29, mean loss: 17121.6793593424\n",
      "Epoch: 30, mean loss: 14421.840864077609\n",
      "Epoch: 31, mean loss: 20645.008231246025\n",
      "Epoch: 32, mean loss: 19635.540600385408\n",
      "Epoch: 33, mean loss: 15654.73268467932\n",
      "Epoch: 34, mean loss: 13606.199044348887\n",
      "Epoch: 35, mean loss: 14105.527604451385\n",
      "Epoch: 36, mean loss: 14954.617091423572\n",
      "Epoch: 37, mean loss: 18048.606623755993\n",
      "Epoch: 38, mean loss: 19441.392160903193\n",
      "Epoch: 39, mean loss: 13325.38526293274\n",
      "Epoch: 40, mean loss: 14335.902475871477\n",
      "Epoch: 41, mean loss: 19134.85657244679\n",
      "Epoch: 42, mean loss: 17032.786747669867\n",
      "Epoch: 43, mean loss: 10307.404677608534\n",
      "Epoch: 44, mean loss: 17675.566899216756\n",
      "Epoch: 45, mean loss: 15245.390425514403\n",
      "Epoch: 46, mean loss: 15000.172572186222\n",
      "Epoch: 47, mean loss: 17259.590814158517\n",
      "Epoch: 48, mean loss: 11119.159394796827\n",
      "Epoch: 49, mean loss: 16133.239202263589\n",
      "Epoch: 50, mean loss: 13403.359264247894\n",
      "Epoch: 51, mean loss: 10967.174327868619\n",
      "Epoch: 52, mean loss: 19784.549320679904\n",
      "Epoch: 53, mean loss: 13645.993184795294\n",
      "Epoch: 54, mean loss: 15998.128036444257\n",
      "Epoch: 55, mean loss: 19102.326782302735\n",
      "Epoch: 56, mean loss: 16807.677850404183\n",
      "Epoch: 57, mean loss: 19728.00150670031\n",
      "Epoch: 58, mean loss: 22922.5522721243\n",
      "Epoch: 59, mean loss: 20632.65587649805\n",
      "Epoch: 60, mean loss: 13165.804662042528\n",
      "Epoch: 61, mean loss: 11297.098305981932\n",
      "Epoch: 62, mean loss: 15740.36910796746\n",
      "Epoch: 63, mean loss: 12611.76407026101\n",
      "Epoch: 64, mean loss: 17081.350266294852\n",
      "Epoch: 65, mean loss: 22022.948696477073\n",
      "Epoch: 66, mean loss: 17663.246641654903\n",
      "Epoch: 67, mean loss: 16391.873072489\n",
      "Epoch: 68, mean loss: 14425.653368848236\n",
      "Epoch: 69, mean loss: 16766.443155202636\n",
      "Epoch: 70, mean loss: 14147.175551471875\n",
      "Epoch: 71, mean loss: 13270.287322341197\n",
      "Epoch: 72, mean loss: 10540.059180566786\n",
      "Epoch: 73, mean loss: 14117.129834938289\n",
      "Epoch: 74, mean loss: 13132.147260927137\n",
      "Epoch: 75, mean loss: 17282.892617973423\n",
      "Epoch: 76, mean loss: 20501.30688359844\n",
      "Epoch: 77, mean loss: 12670.774068069068\n",
      "Epoch: 78, mean loss: 14174.168623207343\n",
      "Epoch: 79, mean loss: 11038.294586760683\n",
      "Epoch: 80, mean loss: 17468.964009159456\n",
      "Epoch: 81, mean loss: 20007.78829233672\n",
      "Epoch: 82, mean loss: 18092.90738260391\n",
      "Epoch: 83, mean loss: 22211.362743742953\n",
      "Epoch: 84, mean loss: 18067.766517084452\n",
      "Epoch: 85, mean loss: 13585.38612041137\n",
      "Epoch: 86, mean loss: 18263.60584226612\n",
      "Epoch: 87, mean loss: 20644.356836319523\n",
      "Epoch: 88, mean loss: 16024.960580393137\n",
      "Epoch: 89, mean loss: 9647.317301496687\n",
      "Epoch: 90, mean loss: 15797.12700210351\n",
      "Epoch: 91, mean loss: 18766.84025456355\n",
      "Epoch: 92, mean loss: 15039.627401925332\n",
      "Epoch: 93, mean loss: 12747.679564677921\n",
      "Epoch: 94, mean loss: 20114.586041932504\n",
      "Epoch: 95, mean loss: 13240.78110345428\n",
      "Epoch: 96, mean loss: 10604.746431647662\n",
      "Epoch: 97, mean loss: 14221.130216615416\n",
      "Epoch: 98, mean loss: 10735.352967143954\n",
      "Epoch: 99, mean loss: 19547.90464102389\n",
      "Epoch: 100, mean loss: 14822.905435938095\n",
      "Epoch: 101, mean loss: 15166.553939790074\n",
      "Epoch: 102, mean loss: 17804.159311590534\n",
      "Epoch: 103, mean loss: 16412.768612110045\n",
      "Epoch: 104, mean loss: 14204.355951910456\n",
      "Epoch: 105, mean loss: 16686.424076306797\n",
      "Epoch: 106, mean loss: 18110.545727486744\n",
      "Epoch: 107, mean loss: 20482.13495096787\n",
      "Epoch: 108, mean loss: 13443.972742972714\n",
      "Epoch: 109, mean loss: 10772.388905635326\n",
      "Epoch: 110, mean loss: 19219.83037635705\n",
      "Epoch: 111, mean loss: 14884.064254052932\n",
      "Epoch: 112, mean loss: 14654.090061317833\n",
      "Epoch: 113, mean loss: 20397.943247491257\n",
      "Epoch: 114, mean loss: 14919.260137178644\n",
      "Epoch: 115, mean loss: 15599.59047932334\n",
      "Epoch: 116, mean loss: 17635.151521497304\n",
      "Epoch: 117, mean loss: 20137.233334510525\n",
      "Epoch: 118, mean loss: 17867.755891080564\n",
      "Epoch: 119, mean loss: 15025.460970618191\n",
      "Epoch: 120, mean loss: 17182.54120847779\n",
      "Epoch: 121, mean loss: 9960.744887277393\n",
      "Epoch: 122, mean loss: 15306.948068572352\n",
      "Epoch: 123, mean loss: 18420.096213953755\n",
      "Epoch: 124, mean loss: 16679.54000519112\n",
      "Epoch: 125, mean loss: 15903.17257595967\n",
      "Epoch: 126, mean loss: 10584.91085891057\n",
      "Epoch: 127, mean loss: 19295.891842604764\n",
      "Epoch: 128, mean loss: 15069.89771983922\n",
      "Epoch: 129, mean loss: 16376.638986655318\n",
      "Epoch: 130, mean loss: 15796.868039625006\n",
      "Epoch: 131, mean loss: 15736.80293978341\n",
      "Epoch: 132, mean loss: 14995.229039600226\n",
      "Epoch: 133, mean loss: 20996.432214516095\n",
      "Epoch: 134, mean loss: 15165.514030983382\n",
      "Epoch: 135, mean loss: 12386.165566437052\n",
      "Epoch: 136, mean loss: 18717.876920398834\n",
      "Epoch: 137, mean loss: 15778.71439495606\n",
      "Epoch: 138, mean loss: 15172.295540657178\n",
      "Epoch: 139, mean loss: 12442.193699877553\n",
      "Epoch: 140, mean loss: 15934.642211190449\n",
      "Epoch: 141, mean loss: 20529.34754959841\n",
      "Epoch: 142, mean loss: 14334.492896308264\n",
      "Epoch: 143, mean loss: 11962.029052895185\n",
      "Epoch: 144, mean loss: 18950.151892979186\n",
      "Epoch: 145, mean loss: 13917.147682102239\n",
      "Epoch: 146, mean loss: 12501.112281027708\n",
      "Epoch: 147, mean loss: 13238.384948792867\n",
      "Epoch: 148, mean loss: 13723.338210153952\n",
      "Epoch: 149, mean loss: 11654.433460057628\n",
      "Epoch: 150, mean loss: 14917.694995641856\n",
      "Epoch: 151, mean loss: 18822.220326937382\n",
      "Epoch: 152, mean loss: 14726.479120681608\n",
      "Epoch: 153, mean loss: 18465.753699031105\n",
      "Epoch: 154, mean loss: 14830.881156981097\n",
      "Epoch: 155, mean loss: 12464.957949843729\n",
      "Epoch: 156, mean loss: 10948.55399074254\n",
      "Epoch: 157, mean loss: 13906.656319394448\n",
      "Epoch: 158, mean loss: 14048.85050691667\n",
      "Epoch: 159, mean loss: 11190.044701398116\n",
      "Epoch: 160, mean loss: 14788.470988179939\n",
      "Epoch: 161, mean loss: 12042.339799817104\n",
      "Epoch: 162, mean loss: 17720.714873710524\n",
      "Epoch: 163, mean loss: 18813.053635841872\n",
      "Epoch: 164, mean loss: 17082.279300548547\n",
      "Epoch: 165, mean loss: 7634.001553301759\n",
      "Epoch: 166, mean loss: 10081.103811616806\n",
      "Epoch: 167, mean loss: 17751.06448465959\n",
      "Epoch: 168, mean loss: 11303.40902346234\n",
      "Epoch: 169, mean loss: 12543.275184445201\n",
      "Epoch: 170, mean loss: 13068.09097318329\n",
      "Epoch: 171, mean loss: 9536.770630869518\n",
      "Epoch: 172, mean loss: 13831.231893712216\n",
      "Epoch: 173, mean loss: 11350.719186406575\n",
      "Epoch: 174, mean loss: 12487.443091192648\n",
      "Epoch: 175, mean loss: 11956.672577899773\n",
      "Epoch: 176, mean loss: 14198.938867073217\n",
      "Epoch: 177, mean loss: 16660.52732797241\n",
      "Epoch: 178, mean loss: 15301.56197095116\n",
      "Epoch: 179, mean loss: 9369.205749725581\n",
      "Epoch: 180, mean loss: 12145.4549135606\n",
      "Epoch: 181, mean loss: 12194.303321646743\n",
      "Epoch: 182, mean loss: 12480.077795620942\n",
      "Epoch: 183, mean loss: 15140.559840848082\n",
      "Epoch: 184, mean loss: 8559.702037890514\n",
      "Epoch: 185, mean loss: 9326.134378375547\n",
      "Epoch: 186, mean loss: 17998.143494178603\n",
      "Epoch: 187, mean loss: 15739.622604931295\n",
      "Epoch: 188, mean loss: 8248.392533916065\n",
      "Epoch: 189, mean loss: 12706.418335979879\n",
      "Epoch: 190, mean loss: 9094.692282664239\n",
      "Epoch: 191, mean loss: 9638.823931738607\n",
      "Epoch: 192, mean loss: 12781.932282876014\n",
      "Epoch: 193, mean loss: 12249.323895020474\n",
      "Epoch: 194, mean loss: 9877.850373590822\n",
      "Epoch: 195, mean loss: 12757.107595344598\n",
      "Epoch: 196, mean loss: 10612.971550184455\n",
      "Epoch: 197, mean loss: 13280.660751916716\n",
      "Epoch: 198, mean loss: 17063.625201974053\n",
      "Epoch: 199, mean loss: 11282.31242212517\n",
      "Epoch: 200, mean loss: 13891.087543378617\n",
      "Epoch: 201, mean loss: 14008.990341333787\n",
      "Epoch: 202, mean loss: 10561.616238665452\n",
      "Epoch: 203, mean loss: 12787.269891255093\n",
      "Epoch: 204, mean loss: 14919.28062596191\n",
      "Epoch: 205, mean loss: 8832.86563029948\n",
      "Epoch: 206, mean loss: 10315.981458396895\n",
      "Epoch: 207, mean loss: 11660.319275952083\n",
      "Epoch: 208, mean loss: 13297.396757396697\n",
      "Epoch: 209, mean loss: 10117.495183691448\n",
      "Epoch: 210, mean loss: 9359.450488236434\n",
      "Epoch: 211, mean loss: 11389.768763996912\n",
      "Epoch: 212, mean loss: 8914.295504089821\n",
      "Epoch: 213, mean loss: 10271.92992169911\n",
      "Epoch: 214, mean loss: 14162.557717860322\n",
      "Epoch: 215, mean loss: 7743.421247323434\n",
      "Epoch: 216, mean loss: 10081.450679875252\n",
      "Epoch: 217, mean loss: 13432.16788930518\n",
      "Epoch: 218, mean loss: 11119.097684408602\n",
      "Epoch: 219, mean loss: 6987.264703937881\n",
      "Epoch: 220, mean loss: 11901.821562588715\n",
      "Epoch: 221, mean loss: 5412.386803366173\n",
      "Epoch: 222, mean loss: 13924.03110768383\n",
      "Epoch: 223, mean loss: 12640.79754117892\n",
      "Epoch: 224, mean loss: 10490.331847321393\n",
      "Epoch: 225, mean loss: 8176.824378832431\n",
      "Epoch: 226, mean loss: 10693.96374091594\n",
      "Epoch: 227, mean loss: 9117.858243544923\n",
      "Epoch: 228, mean loss: 14219.249369811292\n",
      "Epoch: 229, mean loss: 9737.591972717046\n",
      "Epoch: 230, mean loss: 8680.979366699818\n",
      "Epoch: 231, mean loss: 9727.314033575076\n",
      "Epoch: 232, mean loss: 7955.2743967317\n",
      "Epoch: 233, mean loss: 8469.546865244254\n",
      "Epoch: 234, mean loss: 11081.315736864857\n",
      "Epoch: 235, mean loss: 11696.574107301323\n",
      "Epoch: 236, mean loss: 11422.590579145546\n",
      "Epoch: 237, mean loss: 10081.07785152138\n",
      "Epoch: 238, mean loss: 7391.723301046231\n",
      "Epoch: 239, mean loss: 10092.738301289353\n",
      "Epoch: 240, mean loss: 11421.726402498492\n",
      "Epoch: 241, mean loss: 8896.726453479821\n",
      "Epoch: 242, mean loss: 7756.276184797812\n",
      "Epoch: 243, mean loss: 7300.1719670364955\n",
      "Epoch: 244, mean loss: 8235.332437206198\n",
      "Epoch: 245, mean loss: 12366.192841361632\n",
      "Epoch: 246, mean loss: 8733.916182039075\n",
      "Epoch: 247, mean loss: 9900.241545778412\n",
      "Epoch: 248, mean loss: 9500.655994347873\n",
      "Epoch: 249, mean loss: 8817.98751746864\n",
      "Epoch: 250, mean loss: 7829.74522105495\n",
      "Epoch: 251, mean loss: 11857.320665244572\n",
      "Epoch: 252, mean loss: 10252.018948744735\n",
      "Epoch: 253, mean loss: 9018.683618605754\n",
      "Epoch: 254, mean loss: 6335.111530504098\n",
      "Epoch: 255, mean loss: 10257.283156295218\n",
      "Epoch: 256, mean loss: 9433.761608309276\n",
      "Epoch: 257, mean loss: 9649.431333404136\n",
      "Epoch: 258, mean loss: 10299.252194614483\n",
      "Epoch: 259, mean loss: 9198.033501310807\n",
      "Epoch: 260, mean loss: 9006.480621333902\n",
      "Epoch: 261, mean loss: 8224.16523718223\n",
      "Epoch: 262, mean loss: 8733.86784203845\n",
      "Epoch: 263, mean loss: 6965.419496322407\n",
      "Epoch: 264, mean loss: 9770.474832338921\n",
      "Epoch: 265, mean loss: 5104.911827246014\n",
      "Epoch: 266, mean loss: 11048.883033196884\n",
      "Epoch: 267, mean loss: 8260.262321798573\n",
      "Epoch: 268, mean loss: 7106.172021154492\n",
      "Epoch: 269, mean loss: 7915.266074332021\n",
      "Epoch: 270, mean loss: 8112.137935574508\n",
      "Epoch: 271, mean loss: 7902.20208636863\n",
      "Epoch: 272, mean loss: 14252.102021994478\n",
      "Epoch: 273, mean loss: 7275.758301055378\n",
      "Epoch: 274, mean loss: 6721.935080408556\n",
      "Epoch: 275, mean loss: 7168.861973992298\n",
      "Epoch: 276, mean loss: 10504.854078903927\n",
      "Epoch: 277, mean loss: 6123.748796334945\n",
      "Epoch: 278, mean loss: 9875.582318983354\n",
      "Epoch: 279, mean loss: 9181.93239173422\n",
      "Epoch: 280, mean loss: 7382.336611758222\n",
      "Epoch: 281, mean loss: 9401.628366800958\n",
      "Epoch: 282, mean loss: 8557.180346964542\n",
      "Epoch: 283, mean loss: 6714.516647027207\n",
      "Epoch: 284, mean loss: 7502.7653989306\n",
      "Epoch: 285, mean loss: 6299.495403656253\n",
      "Epoch: 286, mean loss: 9670.670631921334\n",
      "Epoch: 287, mean loss: 6992.496407888095\n",
      "Epoch: 288, mean loss: 6833.603178402782\n",
      "Epoch: 289, mean loss: 8203.244672348686\n",
      "Epoch: 290, mean loss: 7911.448804518004\n",
      "Epoch: 291, mean loss: 7984.7633066593735\n",
      "Epoch: 292, mean loss: 8189.524229339094\n",
      "Epoch: 293, mean loss: 6701.254952011655\n",
      "Epoch: 294, mean loss: 7217.700377037765\n",
      "Epoch: 295, mean loss: 6010.82439581439\n",
      "Epoch: 296, mean loss: 6685.379727272735\n",
      "Epoch: 297, mean loss: 7910.202528354769\n",
      "Epoch: 298, mean loss: 7170.214879666269\n",
      "Epoch: 299, mean loss: 7909.564984724197\n",
      "Epoch: 300, mean loss: 9270.858815918707\n",
      "Epoch: 301, mean loss: 7677.440299305687\n",
      "Epoch: 302, mean loss: 7996.985281455357\n",
      "Epoch: 303, mean loss: 8318.607633010804\n",
      "Epoch: 304, mean loss: 7660.60403091456\n",
      "Epoch: 305, mean loss: 9697.833548423432\n",
      "Epoch: 306, mean loss: 7663.8863211493535\n",
      "Epoch: 307, mean loss: 8040.961739079856\n",
      "Epoch: 308, mean loss: 8130.272828291886\n",
      "Epoch: 309, mean loss: 6789.396264105254\n",
      "Epoch: 310, mean loss: 6015.553861300609\n",
      "Epoch: 311, mean loss: 7020.802553519554\n",
      "Epoch: 312, mean loss: 7168.1811491609\n",
      "Epoch: 313, mean loss: 6327.858436808474\n",
      "Epoch: 314, mean loss: 8403.969451780233\n",
      "Epoch: 315, mean loss: 9564.832925963505\n",
      "Epoch: 316, mean loss: 6978.660509677017\n",
      "Epoch: 317, mean loss: 6690.155930450216\n",
      "Epoch: 318, mean loss: 5975.883660603307\n",
      "Epoch: 319, mean loss: 7612.1029049478075\n",
      "Epoch: 320, mean loss: 7272.124865684009\n",
      "Epoch: 321, mean loss: 7348.847094145126\n",
      "Epoch: 322, mean loss: 6949.727317850708\n",
      "Epoch: 323, mean loss: 5635.915813492965\n",
      "Epoch: 324, mean loss: 6637.7776688910435\n",
      "Epoch: 325, mean loss: 7274.482701849738\n",
      "Epoch: 326, mean loss: 6235.912911050944\n",
      "Epoch: 327, mean loss: 7182.36488214462\n",
      "Epoch: 328, mean loss: 7099.701565212253\n",
      "Epoch: 329, mean loss: 8128.7872683558435\n",
      "Epoch: 330, mean loss: 7196.453223476406\n",
      "Epoch: 331, mean loss: 6845.500386966014\n",
      "Epoch: 332, mean loss: 5576.98292549201\n",
      "Epoch: 333, mean loss: 7427.724334552032\n",
      "Epoch: 334, mean loss: 6257.836566784054\n",
      "Epoch: 335, mean loss: 6385.207983595768\n",
      "Epoch: 336, mean loss: 7980.7638292036145\n",
      "Epoch: 337, mean loss: 5687.290431591873\n",
      "Epoch: 338, mean loss: 6832.8630225101\n",
      "Epoch: 339, mean loss: 6858.133278310518\n",
      "Epoch: 340, mean loss: 6560.913656509439\n",
      "Epoch: 341, mean loss: 7145.979200806235\n",
      "Epoch: 342, mean loss: 8847.263572582213\n",
      "Epoch: 343, mean loss: 7058.541111358952\n",
      "Epoch: 344, mean loss: 6963.796361823875\n",
      "Epoch: 345, mean loss: 6060.820334722638\n",
      "Epoch: 346, mean loss: 7034.807736472253\n",
      "Epoch: 347, mean loss: 6829.089380206276\n",
      "Epoch: 348, mean loss: 5858.691359178602\n",
      "Epoch: 349, mean loss: 7042.2874089961\n",
      "Epoch: 350, mean loss: 6822.8072332289685\n",
      "Epoch: 351, mean loss: 6380.43983753458\n",
      "Epoch: 352, mean loss: 7082.056616782698\n",
      "Epoch: 353, mean loss: 7315.147211248009\n",
      "Epoch: 354, mean loss: 8033.960008964711\n",
      "Epoch: 355, mean loss: 6608.0250283588\n",
      "Epoch: 356, mean loss: 6725.435722820159\n",
      "Epoch: 357, mean loss: 5341.24674026698\n",
      "Epoch: 358, mean loss: 5657.086612701321\n",
      "Epoch: 359, mean loss: 6803.878362865222\n",
      "Epoch: 360, mean loss: 6564.276715741839\n",
      "Epoch: 361, mean loss: 7973.58987200938\n",
      "Epoch: 362, mean loss: 5657.426437489414\n",
      "Epoch: 363, mean loss: 6644.4699038858535\n",
      "Epoch: 364, mean loss: 5834.025148653489\n",
      "Epoch: 365, mean loss: 6449.478253621878\n",
      "Epoch: 366, mean loss: 5402.539156765116\n",
      "Epoch: 367, mean loss: 6059.660170236776\n",
      "Epoch: 368, mean loss: 6003.782746697252\n",
      "Epoch: 369, mean loss: 6591.299768548408\n",
      "Epoch: 370, mean loss: 6331.131360385962\n",
      "Epoch: 371, mean loss: 6427.467083390591\n",
      "Epoch: 372, mean loss: 6282.918021502722\n",
      "Epoch: 373, mean loss: 6045.697485351677\n",
      "Epoch: 374, mean loss: 6392.1773295354915\n",
      "Epoch: 375, mean loss: 5768.26721270349\n",
      "Epoch: 376, mean loss: 5520.592618799472\n",
      "Epoch: 377, mean loss: 5855.246969371784\n",
      "Epoch: 378, mean loss: 6775.406628924909\n",
      "Epoch: 379, mean loss: 6375.600035401831\n",
      "Epoch: 380, mean loss: 6045.976277769657\n",
      "Epoch: 381, mean loss: 7165.110606982659\n",
      "Epoch: 382, mean loss: 6581.146397572784\n",
      "Epoch: 383, mean loss: 7586.830183392173\n",
      "Epoch: 384, mean loss: 4891.053744007118\n",
      "Epoch: 385, mean loss: 5361.370303928961\n",
      "Epoch: 386, mean loss: 5538.423755804142\n",
      "Epoch: 387, mean loss: 5633.385828116817\n",
      "Epoch: 388, mean loss: 5493.687486178948\n",
      "Epoch: 389, mean loss: 5464.089651204506\n",
      "Epoch: 390, mean loss: 7073.713929643619\n",
      "Epoch: 391, mean loss: 5424.925354577888\n",
      "Epoch: 392, mean loss: 5507.740913769503\n",
      "Epoch: 393, mean loss: 5009.905647878863\n",
      "Epoch: 394, mean loss: 5096.54175526709\n",
      "Epoch: 395, mean loss: 4977.420793839261\n",
      "Epoch: 396, mean loss: 5026.960796583674\n",
      "Epoch: 397, mean loss: 7078.812402801472\n",
      "Epoch: 398, mean loss: 4550.2938730544065\n",
      "Epoch: 399, mean loss: 5294.2787939062855\n",
      "Epoch: 400, mean loss: 4964.709307159178\n",
      "Epoch: 401, mean loss: 5052.4269899342935\n",
      "Epoch: 402, mean loss: 5538.761564238493\n",
      "Epoch: 403, mean loss: 6494.684190260532\n",
      "Epoch: 404, mean loss: 5934.5643104470555\n",
      "Epoch: 405, mean loss: 5229.337236674776\n",
      "Epoch: 406, mean loss: 5162.434996815443\n",
      "Epoch: 407, mean loss: 4717.10316731009\n",
      "Epoch: 408, mean loss: 6935.951390046565\n",
      "Epoch: 409, mean loss: 4255.717398889931\n",
      "Epoch: 410, mean loss: 5276.258515377489\n",
      "Epoch: 411, mean loss: 4816.531608835485\n",
      "Epoch: 412, mean loss: 4547.72413013089\n",
      "Epoch: 413, mean loss: 4495.580640698894\n",
      "Epoch: 414, mean loss: 5176.540411105406\n",
      "Epoch: 415, mean loss: 4577.779303605622\n",
      "Epoch: 416, mean loss: 4808.467105543823\n",
      "Epoch: 417, mean loss: 3609.7055390067044\n",
      "Epoch: 418, mean loss: 5717.910941533409\n",
      "Epoch: 419, mean loss: 4209.02036724553\n",
      "Epoch: 420, mean loss: 4940.161557732752\n",
      "Epoch: 421, mean loss: 4503.0583391084\n",
      "Epoch: 422, mean loss: 4720.572535219409\n",
      "Epoch: 423, mean loss: 4583.883334518877\n",
      "Epoch: 424, mean loss: 6107.8840324388075\n",
      "Epoch: 425, mean loss: 5124.368165368377\n",
      "Epoch: 426, mean loss: 3861.603137015772\n",
      "Epoch: 427, mean loss: 3626.497469406509\n",
      "Epoch: 428, mean loss: 5442.068615035151\n",
      "Epoch: 429, mean loss: 3834.317852375133\n",
      "Epoch: 430, mean loss: 4971.885186095926\n",
      "Epoch: 431, mean loss: 5009.7498064130405\n",
      "Epoch: 432, mean loss: 5339.772582190801\n",
      "Epoch: 433, mean loss: 4370.727817325569\n",
      "Epoch: 434, mean loss: 5323.4714036510695\n",
      "Epoch: 435, mean loss: 4566.5073588126925\n",
      "Epoch: 436, mean loss: 4861.771569610429\n",
      "Epoch: 437, mean loss: 4715.445846518076\n",
      "Epoch: 438, mean loss: 4116.982122339806\n",
      "Epoch: 439, mean loss: 4169.003814414787\n",
      "Epoch: 440, mean loss: 3913.9691085227955\n",
      "Epoch: 441, mean loss: 3147.9379752965124\n",
      "Epoch: 442, mean loss: 4417.702872704069\n",
      "Epoch: 443, mean loss: 4377.225184503528\n",
      "Epoch: 444, mean loss: 5028.0442032333285\n",
      "Epoch: 445, mean loss: 4892.690918600717\n",
      "Epoch: 446, mean loss: 4647.750872788641\n",
      "Epoch: 447, mean loss: 4816.613434688724\n",
      "Epoch: 448, mean loss: 3760.4069907694375\n",
      "Epoch: 449, mean loss: 3336.6102357620966\n",
      "Epoch: 450, mean loss: 3756.9209642597934\n",
      "Epoch: 451, mean loss: 4230.7442683467025\n",
      "Epoch: 452, mean loss: 3736.909170330841\n",
      "Epoch: 453, mean loss: 4194.698411884028\n",
      "Epoch: 454, mean loss: 3930.9170025858753\n",
      "Epoch: 455, mean loss: 3530.186908721431\n",
      "Epoch: 456, mean loss: 5033.929537745491\n",
      "Epoch: 457, mean loss: 5124.120257932513\n",
      "Epoch: 458, mean loss: 4224.0578007864915\n",
      "Epoch: 459, mean loss: 4859.069388343101\n",
      "Epoch: 460, mean loss: 5024.267047604708\n",
      "Epoch: 461, mean loss: 3979.143438833231\n",
      "Epoch: 462, mean loss: 4579.71828386137\n",
      "Epoch: 463, mean loss: 3404.9900499606842\n",
      "Epoch: 464, mean loss: 4051.315807405953\n",
      "Epoch: 465, mean loss: 3753.309752371989\n",
      "Epoch: 466, mean loss: 3042.3789481182757\n",
      "Epoch: 467, mean loss: 4389.597515602697\n",
      "Epoch: 468, mean loss: 3399.514739354388\n",
      "Epoch: 469, mean loss: 4528.430810157626\n",
      "Epoch: 470, mean loss: 3879.9351916752244\n",
      "Epoch: 471, mean loss: 3841.8624259561752\n",
      "Epoch: 472, mean loss: 3455.2724847738236\n",
      "Epoch: 473, mean loss: 3252.360887401529\n",
      "Epoch: 474, mean loss: 3893.380521658559\n",
      "Epoch: 475, mean loss: 3238.096843475532\n",
      "Epoch: 476, mean loss: 3703.5898525170596\n",
      "Epoch: 477, mean loss: 4256.497556385115\n",
      "Epoch: 478, mean loss: 3919.1708247208376\n",
      "Epoch: 479, mean loss: 3411.607173293613\n",
      "Epoch: 480, mean loss: 3447.471381445879\n",
      "Epoch: 481, mean loss: 3146.730786180551\n",
      "Epoch: 482, mean loss: 3230.785416672041\n",
      "Epoch: 483, mean loss: 3593.753293280696\n",
      "Epoch: 484, mean loss: 3374.989410905536\n",
      "Epoch: 485, mean loss: 3388.1285448243366\n",
      "Epoch: 486, mean loss: 2760.7404257746525\n",
      "Epoch: 487, mean loss: 3580.139475859392\n",
      "Epoch: 488, mean loss: 3427.2570397319137\n",
      "Epoch: 489, mean loss: 3670.7830116650425\n",
      "Epoch: 490, mean loss: 2651.65647843151\n",
      "Epoch: 491, mean loss: 3530.51233364402\n",
      "Epoch: 492, mean loss: 2479.3085737245383\n",
      "Epoch: 493, mean loss: 2984.5443833673057\n",
      "Epoch: 494, mean loss: 3232.538750813678\n",
      "Epoch: 495, mean loss: 3550.2771557725046\n",
      "Epoch: 496, mean loss: 3180.8221396600334\n",
      "Epoch: 497, mean loss: 3059.953910936633\n",
      "Epoch: 498, mean loss: 2763.224709682829\n",
      "Epoch: 499, mean loss: 3182.6385684281554\n",
      "Done\n",
      "Epoch: 0, mean loss: 12411.047068354814\n",
      "Epoch: 1, mean loss: 16938.292785721263\n",
      "Epoch: 2, mean loss: 9916.184585831408\n",
      "Epoch: 3, mean loss: 15230.682388367806\n",
      "Epoch: 4, mean loss: 18129.12832470531\n",
      "Epoch: 5, mean loss: 11415.855044426287\n",
      "Epoch: 6, mean loss: 17975.162982931928\n",
      "Epoch: 7, mean loss: 18023.604690269087\n",
      "Epoch: 8, mean loss: 11477.905371484003\n",
      "Epoch: 9, mean loss: 15547.911336694144\n",
      "Epoch: 10, mean loss: 14992.88433594921\n",
      "Epoch: 11, mean loss: 14620.432389289585\n",
      "Epoch: 12, mean loss: 13296.760413776263\n",
      "Epoch: 13, mean loss: 15824.093980262976\n",
      "Epoch: 14, mean loss: 19303.798178791047\n",
      "Epoch: 15, mean loss: 13864.617815840966\n",
      "Epoch: 16, mean loss: 15129.164019357124\n",
      "Epoch: 17, mean loss: 15231.686399424845\n",
      "Epoch: 18, mean loss: 18926.89494442882\n",
      "Epoch: 19, mean loss: 12211.726729837887\n",
      "Epoch: 20, mean loss: 11607.65808055474\n",
      "Epoch: 21, mean loss: 17547.912884146277\n",
      "Epoch: 22, mean loss: 23734.81523065741\n",
      "Epoch: 23, mean loss: 15216.480015498348\n",
      "Epoch: 24, mean loss: 15636.029121676715\n",
      "Epoch: 25, mean loss: 21276.78313573526\n",
      "Epoch: 26, mean loss: 10712.377989146536\n",
      "Epoch: 27, mean loss: 18026.40518401969\n",
      "Epoch: 28, mean loss: 13878.137338655033\n",
      "Epoch: 29, mean loss: 17224.487834826028\n",
      "Epoch: 30, mean loss: 18072.579754181526\n",
      "Epoch: 31, mean loss: 15917.326112417173\n",
      "Epoch: 32, mean loss: 12323.240586647817\n",
      "Epoch: 33, mean loss: 12643.325579413939\n",
      "Epoch: 34, mean loss: 12756.832483517588\n",
      "Epoch: 35, mean loss: 17161.855537519717\n",
      "Epoch: 36, mean loss: 17727.818797484735\n",
      "Epoch: 37, mean loss: 10676.389503263359\n",
      "Epoch: 38, mean loss: 13661.421797056513\n",
      "Epoch: 39, mean loss: 16255.754135826279\n",
      "Epoch: 40, mean loss: 18225.17795958138\n",
      "Epoch: 41, mean loss: 8486.734910852974\n",
      "Epoch: 42, mean loss: 15455.390330108987\n",
      "Epoch: 43, mean loss: 11075.623142639277\n",
      "Epoch: 44, mean loss: 14263.26363477482\n",
      "Epoch: 45, mean loss: 16446.3082057229\n",
      "Epoch: 46, mean loss: 10298.828663872664\n",
      "Epoch: 47, mean loss: 14008.598788743015\n",
      "Epoch: 48, mean loss: 11280.853460421327\n",
      "Epoch: 49, mean loss: 8058.926920480882\n",
      "Epoch: 50, mean loss: 17101.73233381989\n",
      "Epoch: 51, mean loss: 10955.215451159129\n",
      "Epoch: 52, mean loss: 15064.89323199914\n",
      "Epoch: 53, mean loss: 14462.178568940171\n",
      "Epoch: 54, mean loss: 16660.156365755935\n",
      "Epoch: 55, mean loss: 14117.232565241244\n",
      "Epoch: 56, mean loss: 19767.85262815268\n",
      "Epoch: 57, mean loss: 16594.512179918293\n",
      "Epoch: 58, mean loss: 14258.246408575103\n",
      "Epoch: 59, mean loss: 9217.617826690612\n",
      "Epoch: 60, mean loss: 12117.147237177855\n",
      "Epoch: 61, mean loss: 11355.894651880471\n",
      "Epoch: 62, mean loss: 12374.6084568537\n",
      "Epoch: 63, mean loss: 16910.13598491642\n",
      "Epoch: 64, mean loss: 15563.952801223331\n",
      "Epoch: 65, mean loss: 14646.303068235518\n",
      "Epoch: 66, mean loss: 9090.43518159835\n",
      "Epoch: 67, mean loss: 13251.753386492528\n",
      "Epoch: 68, mean loss: 12304.244938253054\n",
      "Epoch: 69, mean loss: 11246.447571559027\n",
      "Epoch: 70, mean loss: 8810.706043976976\n",
      "Epoch: 71, mean loss: 10288.513347475802\n",
      "Epoch: 72, mean loss: 9723.281563705494\n",
      "Epoch: 73, mean loss: 10597.169083688133\n",
      "Epoch: 74, mean loss: 16513.575071038184\n",
      "Epoch: 75, mean loss: 8478.722608013113\n",
      "Epoch: 76, mean loss: 11872.593151874718\n",
      "Epoch: 77, mean loss: 9792.824673666855\n",
      "Epoch: 78, mean loss: 11723.402532079077\n",
      "Epoch: 79, mean loss: 12194.064703006661\n",
      "Epoch: 80, mean loss: 16191.506926069618\n",
      "Epoch: 81, mean loss: 15653.08334760122\n",
      "Epoch: 82, mean loss: 12950.247075653682\n",
      "Epoch: 83, mean loss: 10729.140222937718\n",
      "Epoch: 84, mean loss: 14108.842406397618\n",
      "Epoch: 85, mean loss: 12900.17101618188\n",
      "Epoch: 86, mean loss: 12231.588702378942\n",
      "Epoch: 87, mean loss: 8105.382888230652\n",
      "Epoch: 88, mean loss: 8229.046837175556\n",
      "Epoch: 89, mean loss: 13981.712880254134\n",
      "Epoch: 90, mean loss: 9714.491112766478\n",
      "Epoch: 91, mean loss: 8403.347798948753\n",
      "Epoch: 92, mean loss: 14165.50364876666\n",
      "Epoch: 93, mean loss: 9346.147014429172\n",
      "Epoch: 94, mean loss: 7977.902259224091\n",
      "Epoch: 95, mean loss: 9143.794578141536\n",
      "Epoch: 96, mean loss: 6073.45207336736\n",
      "Epoch: 97, mean loss: 11171.414386333152\n",
      "Epoch: 98, mean loss: 11141.03309730072\n",
      "Epoch: 99, mean loss: 9333.081412140216\n",
      "Epoch: 100, mean loss: 11961.364438020455\n",
      "Epoch: 101, mean loss: 10321.421371296317\n",
      "Epoch: 102, mean loss: 8013.746974292994\n",
      "Epoch: 103, mean loss: 11067.513111703613\n",
      "Epoch: 104, mean loss: 11948.016838342806\n",
      "Epoch: 105, mean loss: 13413.925750188473\n",
      "Epoch: 106, mean loss: 8892.423396536029\n",
      "Epoch: 107, mean loss: 7124.746430804425\n",
      "Epoch: 108, mean loss: 9605.920701435005\n",
      "Epoch: 109, mean loss: 8539.45738088025\n",
      "Epoch: 110, mean loss: 11616.765580292327\n",
      "Epoch: 111, mean loss: 11676.914942719251\n",
      "Epoch: 112, mean loss: 8719.3474470345\n",
      "Epoch: 113, mean loss: 8188.957428521635\n",
      "Epoch: 114, mean loss: 10091.684713641516\n",
      "Epoch: 115, mean loss: 12070.057119959547\n",
      "Epoch: 116, mean loss: 9288.628951721616\n",
      "Epoch: 117, mean loss: 9825.45918493125\n",
      "Epoch: 118, mean loss: 9684.192511911204\n",
      "Epoch: 119, mean loss: 5711.002919670524\n",
      "Epoch: 120, mean loss: 7338.956907298769\n",
      "Epoch: 121, mean loss: 11963.227708602795\n",
      "Epoch: 122, mean loss: 10152.26536631667\n",
      "Epoch: 123, mean loss: 8835.547336660391\n",
      "Epoch: 124, mean loss: 7508.539288721364\n",
      "Epoch: 125, mean loss: 9406.38001244742\n",
      "Epoch: 126, mean loss: 9203.633646253908\n",
      "Epoch: 127, mean loss: 8061.885026103038\n",
      "Epoch: 128, mean loss: 8906.737448878739\n",
      "Epoch: 129, mean loss: 7826.3090591368755\n",
      "Epoch: 130, mean loss: 8618.820105923573\n",
      "Epoch: 131, mean loss: 10653.410031073992\n",
      "Epoch: 132, mean loss: 8069.718712093054\n",
      "Epoch: 133, mean loss: 6961.629547328559\n",
      "Epoch: 134, mean loss: 9029.718141154335\n",
      "Epoch: 135, mean loss: 9274.26382410906\n",
      "Epoch: 136, mean loss: 9108.093994779025\n",
      "Epoch: 137, mean loss: 7313.4728371679885\n",
      "Epoch: 138, mean loss: 8263.203461143054\n",
      "Epoch: 139, mean loss: 11478.0096917041\n",
      "Epoch: 140, mean loss: 7366.59175097583\n",
      "Epoch: 141, mean loss: 6776.310361184813\n",
      "Epoch: 142, mean loss: 9828.691890346872\n",
      "Epoch: 143, mean loss: 8746.299882462954\n",
      "Epoch: 144, mean loss: 7076.683918498515\n",
      "Epoch: 145, mean loss: 7911.490931851162\n",
      "Epoch: 146, mean loss: 6580.665351067687\n",
      "Epoch: 147, mean loss: 5579.45709747507\n",
      "Epoch: 148, mean loss: 7868.249928417228\n",
      "Epoch: 149, mean loss: 10417.281320040676\n",
      "Epoch: 150, mean loss: 6777.795550132593\n",
      "Epoch: 151, mean loss: 8859.21684896271\n",
      "Epoch: 152, mean loss: 8065.588642945433\n",
      "Epoch: 153, mean loss: 8481.554305876667\n",
      "Epoch: 154, mean loss: 5683.264764300259\n",
      "Epoch: 155, mean loss: 6708.425768104601\n",
      "Epoch: 156, mean loss: 8027.106118726593\n",
      "Epoch: 157, mean loss: 7171.960724294501\n",
      "Epoch: 158, mean loss: 8515.834305673761\n",
      "Epoch: 159, mean loss: 7067.832942226081\n",
      "Epoch: 160, mean loss: 9336.91569340104\n",
      "Epoch: 161, mean loss: 8859.49980238459\n",
      "Epoch: 162, mean loss: 8826.119760768597\n",
      "Epoch: 163, mean loss: 5558.782306269881\n",
      "Epoch: 164, mean loss: 6004.959248191207\n",
      "Epoch: 165, mean loss: 7268.0599654540265\n",
      "Epoch: 166, mean loss: 7483.182375853277\n",
      "Epoch: 167, mean loss: 7385.338790583646\n",
      "Epoch: 168, mean loss: 7769.413212608966\n",
      "Epoch: 169, mean loss: 6059.311960504523\n",
      "Epoch: 170, mean loss: 7244.971601240712\n",
      "Epoch: 171, mean loss: 7070.266178865036\n",
      "Epoch: 172, mean loss: 7206.881177957919\n",
      "Epoch: 173, mean loss: 7536.316160727287\n",
      "Epoch: 174, mean loss: 7498.635931879609\n",
      "Epoch: 175, mean loss: 9332.423816945939\n",
      "Epoch: 176, mean loss: 6013.774286833079\n",
      "Epoch: 177, mean loss: 6753.78629321008\n",
      "Epoch: 178, mean loss: 6781.985499570543\n",
      "Epoch: 179, mean loss: 7004.768478378371\n",
      "Epoch: 180, mean loss: 5945.64674212714\n",
      "Epoch: 181, mean loss: 8019.051416996259\n",
      "Epoch: 182, mean loss: 5502.31808462131\n",
      "Epoch: 183, mean loss: 5366.075973579395\n",
      "Epoch: 184, mean loss: 7955.35611176838\n",
      "Epoch: 185, mean loss: 8785.649646403306\n",
      "Epoch: 186, mean loss: 4865.130427197276\n",
      "Epoch: 187, mean loss: 6878.374427927457\n",
      "Epoch: 188, mean loss: 5977.584148488721\n",
      "Epoch: 189, mean loss: 5706.525243429597\n",
      "Epoch: 190, mean loss: 6642.145316383222\n",
      "Epoch: 191, mean loss: 7932.012199775603\n",
      "Epoch: 192, mean loss: 6199.34200498125\n",
      "Epoch: 193, mean loss: 6924.830851606608\n",
      "Epoch: 194, mean loss: 6532.658214575819\n",
      "Epoch: 195, mean loss: 7921.08585185854\n",
      "Epoch: 196, mean loss: 8839.452608274183\n",
      "Epoch: 197, mean loss: 7030.086005596468\n",
      "Epoch: 198, mean loss: 8734.114554603922\n",
      "Epoch: 199, mean loss: 5159.115896936405\n",
      "Epoch: 200, mean loss: 8656.275084332672\n",
      "Epoch: 201, mean loss: 7854.399339700917\n",
      "Epoch: 202, mean loss: 7038.692881783865\n",
      "Epoch: 203, mean loss: 7272.253399516345\n",
      "Epoch: 204, mean loss: 6899.902438676866\n",
      "Epoch: 205, mean loss: 6432.138818180412\n",
      "Epoch: 206, mean loss: 8099.679439099941\n",
      "Epoch: 207, mean loss: 6263.983904741021\n",
      "Epoch: 208, mean loss: 6256.118463640933\n",
      "Epoch: 209, mean loss: 6899.033542821603\n",
      "Epoch: 210, mean loss: 5606.500097596864\n",
      "Epoch: 211, mean loss: 6289.77048953816\n",
      "Epoch: 212, mean loss: 7538.949319595456\n",
      "Epoch: 213, mean loss: 6065.3896538309855\n",
      "Epoch: 214, mean loss: 5928.767195931869\n",
      "Epoch: 215, mean loss: 7831.1711926793705\n",
      "Epoch: 216, mean loss: 7472.896892585965\n",
      "Epoch: 217, mean loss: 4996.143769675894\n",
      "Epoch: 218, mean loss: 6420.082496863375\n",
      "Epoch: 219, mean loss: 5725.789434025401\n",
      "Epoch: 220, mean loss: 6874.415826676047\n",
      "Epoch: 221, mean loss: 7441.236132594513\n",
      "Epoch: 222, mean loss: 7587.061737892813\n",
      "Epoch: 223, mean loss: 5451.593022366392\n",
      "Epoch: 224, mean loss: 6677.0651177879745\n",
      "Epoch: 225, mean loss: 6483.436684513968\n",
      "Epoch: 226, mean loss: 7553.589225332303\n",
      "Epoch: 227, mean loss: 6761.048008224456\n",
      "Epoch: 228, mean loss: 6118.290454413438\n",
      "Epoch: 229, mean loss: 6650.595813843513\n",
      "Epoch: 230, mean loss: 6210.816369179159\n",
      "Epoch: 231, mean loss: 4577.3014163960415\n",
      "Epoch: 232, mean loss: 6851.893697261319\n",
      "Epoch: 233, mean loss: 7654.813140342819\n",
      "Epoch: 234, mean loss: 7212.185785618445\n",
      "Epoch: 235, mean loss: 5801.5750423435975\n",
      "Epoch: 236, mean loss: 5856.910473777409\n",
      "Epoch: 237, mean loss: 6731.007093010583\n",
      "Epoch: 238, mean loss: 5864.709810793704\n",
      "Epoch: 239, mean loss: 7415.4782004007875\n",
      "Epoch: 240, mean loss: 5561.250197939554\n",
      "Epoch: 241, mean loss: 4950.497491174666\n",
      "Epoch: 242, mean loss: 5374.822622323456\n",
      "Epoch: 243, mean loss: 7605.395219826989\n",
      "Epoch: 244, mean loss: 5496.235899229908\n",
      "Epoch: 245, mean loss: 6654.593577525517\n",
      "Epoch: 246, mean loss: 6653.530659500144\n",
      "Epoch: 247, mean loss: 5467.302861039528\n",
      "Epoch: 248, mean loss: 5137.25356067027\n",
      "Epoch: 249, mean loss: 7456.9115123786005\n",
      "Epoch: 250, mean loss: 7250.167943706781\n",
      "Epoch: 251, mean loss: 5771.966467320066\n",
      "Epoch: 252, mean loss: 5356.793235433586\n",
      "Epoch: 253, mean loss: 7272.577852492802\n",
      "Epoch: 254, mean loss: 6001.571788526888\n",
      "Epoch: 255, mean loss: 6269.7873739153365\n",
      "Epoch: 256, mean loss: 6803.499638864538\n",
      "Epoch: 257, mean loss: 6445.046736381863\n",
      "Epoch: 258, mean loss: 6070.799248559625\n",
      "Epoch: 259, mean loss: 5905.780237004415\n",
      "Epoch: 260, mean loss: 5464.982396038566\n",
      "Epoch: 261, mean loss: 4962.2460129950105\n",
      "Epoch: 262, mean loss: 5275.377430413596\n",
      "Epoch: 263, mean loss: 4872.246997681277\n",
      "Epoch: 264, mean loss: 6750.84293224958\n",
      "Epoch: 265, mean loss: 6095.976154895879\n",
      "Epoch: 266, mean loss: 4637.371901224218\n",
      "Epoch: 267, mean loss: 5370.899415832871\n",
      "Epoch: 268, mean loss: 5422.134062246438\n",
      "Epoch: 269, mean loss: 5291.55835183963\n",
      "Epoch: 270, mean loss: 7254.565678017598\n",
      "Epoch: 271, mean loss: 6914.673042127833\n",
      "Epoch: 272, mean loss: 4482.919481887179\n",
      "Epoch: 273, mean loss: 4143.704210457121\n",
      "Epoch: 274, mean loss: 6699.6517412773355\n",
      "Epoch: 275, mean loss: 4883.930246230579\n",
      "Epoch: 276, mean loss: 5693.4939772714815\n",
      "Epoch: 277, mean loss: 6684.634765693353\n",
      "Epoch: 278, mean loss: 3780.2408043854807\n",
      "Epoch: 279, mean loss: 6571.500046560668\n",
      "Epoch: 280, mean loss: 5748.30147505778\n",
      "Epoch: 281, mean loss: 4441.722127821632\n",
      "Epoch: 282, mean loss: 4906.367581682678\n",
      "Epoch: 283, mean loss: 4252.631633369226\n",
      "Epoch: 284, mean loss: 5297.801606337669\n",
      "Epoch: 285, mean loss: 5434.856691320465\n",
      "Epoch: 286, mean loss: 4453.723468182467\n",
      "Epoch: 287, mean loss: 5294.110530730482\n",
      "Epoch: 288, mean loss: 4339.41868924676\n",
      "Epoch: 289, mean loss: 5245.745459614097\n",
      "Epoch: 290, mean loss: 4716.0260628904025\n",
      "Epoch: 291, mean loss: 4912.803357515962\n",
      "Epoch: 292, mean loss: 3876.9004563718568\n",
      "Epoch: 293, mean loss: 4715.756574788892\n",
      "Epoch: 294, mean loss: 3925.790730390956\n",
      "Epoch: 295, mean loss: 4742.108327158055\n",
      "Epoch: 296, mean loss: 4181.8969828184245\n",
      "Epoch: 297, mean loss: 5239.682383224059\n",
      "Epoch: 298, mean loss: 5530.624860794077\n",
      "Epoch: 299, mean loss: 5412.621004767405\n",
      "Epoch: 300, mean loss: 4415.322932549916\n",
      "Epoch: 301, mean loss: 5442.54534569515\n",
      "Epoch: 302, mean loss: 4334.232922151359\n",
      "Epoch: 303, mean loss: 6022.668917517955\n",
      "Epoch: 304, mean loss: 4358.403944507221\n",
      "Epoch: 305, mean loss: 5782.541100473978\n",
      "Epoch: 306, mean loss: 4332.050162464834\n",
      "Epoch: 307, mean loss: 4543.919183623391\n",
      "Epoch: 308, mean loss: 3388.592489874136\n",
      "Epoch: 309, mean loss: 4768.870748379672\n",
      "Epoch: 310, mean loss: 3877.4527804726495\n",
      "Epoch: 311, mean loss: 3527.9153407613076\n",
      "Epoch: 312, mean loss: 5155.57374809022\n",
      "Epoch: 313, mean loss: 5728.690432619211\n",
      "Epoch: 314, mean loss: 3825.7358085600886\n",
      "Epoch: 315, mean loss: 3660.640336831743\n",
      "Epoch: 316, mean loss: 4458.18867019138\n",
      "Epoch: 317, mean loss: 3924.5018944289895\n",
      "Epoch: 318, mean loss: 4902.308479637868\n",
      "Epoch: 319, mean loss: 4046.650471254936\n",
      "Epoch: 320, mean loss: 4630.797904186152\n",
      "Epoch: 321, mean loss: 3245.521483926286\n",
      "Epoch: 322, mean loss: 3673.1912944136684\n",
      "Epoch: 323, mean loss: 4146.990691259692\n",
      "Epoch: 324, mean loss: 3878.440788809283\n",
      "Epoch: 325, mean loss: 3919.609855374232\n",
      "Epoch: 326, mean loss: 4364.734325900512\n",
      "Epoch: 327, mean loss: 4247.942051042134\n",
      "Epoch: 328, mean loss: 4323.121423569103\n",
      "Epoch: 329, mean loss: 4182.147900186003\n",
      "Epoch: 330, mean loss: 3140.1797906579754\n",
      "Epoch: 331, mean loss: 4120.047509572539\n",
      "Epoch: 332, mean loss: 3336.3319724593375\n",
      "Epoch: 333, mean loss: 3825.0558135325177\n",
      "Epoch: 334, mean loss: 4522.428216558766\n",
      "Epoch: 335, mean loss: 2889.787396421727\n",
      "Epoch: 336, mean loss: 3982.068732601806\n",
      "Epoch: 337, mean loss: 4049.134742511647\n",
      "Epoch: 338, mean loss: 3466.0830501227283\n",
      "Epoch: 339, mean loss: 4065.186052989317\n",
      "Epoch: 340, mean loss: 4351.920943111074\n",
      "Epoch: 341, mean loss: 3556.851214630422\n",
      "Epoch: 342, mean loss: 4134.636117606059\n",
      "Epoch: 343, mean loss: 3558.6784196787207\n",
      "Epoch: 344, mean loss: 3830.874264688206\n",
      "Epoch: 345, mean loss: 3140.196347981983\n",
      "Epoch: 346, mean loss: 3545.1246276309807\n",
      "Epoch: 347, mean loss: 3732.9838985815277\n",
      "Epoch: 348, mean loss: 3609.1037629210505\n",
      "Epoch: 349, mean loss: 3388.7773320404044\n",
      "Epoch: 350, mean loss: 3310.6419386290495\n",
      "Epoch: 351, mean loss: 3849.109154030277\n",
      "Epoch: 352, mean loss: 4788.851472720511\n",
      "Epoch: 353, mean loss: 3786.461648587237\n",
      "Epoch: 354, mean loss: 3021.9369894774654\n",
      "Epoch: 355, mean loss: 3087.330527023821\n",
      "Epoch: 356, mean loss: 2599.162083067373\n",
      "Epoch: 357, mean loss: 3566.891157476121\n",
      "Epoch: 358, mean loss: 3705.123651082546\n",
      "Epoch: 359, mean loss: 3973.473529078599\n",
      "Epoch: 360, mean loss: 3086.0933879852046\n",
      "Epoch: 361, mean loss: 3673.671906822271\n",
      "Epoch: 362, mean loss: 2804.28646057808\n",
      "Epoch: 363, mean loss: 3247.9405854857864\n",
      "Epoch: 364, mean loss: 2671.4498106408932\n",
      "Epoch: 365, mean loss: 2872.984108417256\n",
      "Epoch: 366, mean loss: 3146.7408784471295\n",
      "Epoch: 367, mean loss: 3180.6959893313783\n",
      "Epoch: 368, mean loss: 3482.9977761554446\n",
      "Epoch: 369, mean loss: 2991.4272274186515\n",
      "Epoch: 370, mean loss: 2752.06105687992\n",
      "Epoch: 371, mean loss: 2742.840997592831\n",
      "Epoch: 372, mean loss: 3127.7704748945134\n",
      "Epoch: 373, mean loss: 3019.8620143791486\n",
      "Epoch: 374, mean loss: 2625.4836133873496\n",
      "Epoch: 375, mean loss: 2957.812034086046\n",
      "Epoch: 376, mean loss: 3465.9123868513784\n",
      "Epoch: 377, mean loss: 2881.743588512238\n",
      "Epoch: 378, mean loss: 3359.1641409551016\n",
      "Epoch: 379, mean loss: 2545.790309707683\n",
      "Epoch: 380, mean loss: 3784.1386444363534\n",
      "Epoch: 381, mean loss: 3714.66405671786\n",
      "Epoch: 382, mean loss: 2710.7095164441284\n",
      "Epoch: 383, mean loss: 2281.102168769556\n",
      "Epoch: 384, mean loss: 2857.363028138552\n",
      "Epoch: 385, mean loss: 2548.978479703123\n",
      "Epoch: 386, mean loss: 2718.3721199656093\n",
      "Epoch: 387, mean loss: 2838.784810369821\n",
      "Epoch: 388, mean loss: 2825.3133376213104\n",
      "Epoch: 389, mean loss: 2868.3933132869465\n",
      "Epoch: 390, mean loss: 2307.85148337349\n",
      "Epoch: 391, mean loss: 2363.282057939955\n",
      "Epoch: 392, mean loss: 2401.1114192684445\n",
      "Epoch: 393, mean loss: 2209.526802203758\n",
      "Epoch: 394, mean loss: 2611.7416627492807\n",
      "Epoch: 395, mean loss: 2623.442358946502\n",
      "Epoch: 396, mean loss: 2475.0907603963806\n",
      "Epoch: 397, mean loss: 2490.3094196331526\n",
      "Epoch: 398, mean loss: 2653.3186589557167\n",
      "Epoch: 399, mean loss: 1991.667560227646\n",
      "Epoch: 400, mean loss: 3013.6607576970537\n",
      "Epoch: 401, mean loss: 2657.2965320026774\n",
      "Epoch: 402, mean loss: 2625.9061803656764\n",
      "Epoch: 403, mean loss: 2471.7506710662474\n",
      "Epoch: 404, mean loss: 2663.826168405765\n",
      "Epoch: 405, mean loss: 2096.196678935017\n",
      "Epoch: 406, mean loss: 3073.5449519030494\n",
      "Epoch: 407, mean loss: 1731.5464515373963\n",
      "Epoch: 408, mean loss: 2603.1441447694024\n",
      "Epoch: 409, mean loss: 2117.015147622824\n",
      "Epoch: 410, mean loss: 2156.8990456549877\n",
      "Epoch: 411, mean loss: 1880.8316146494553\n",
      "Epoch: 412, mean loss: 2291.4153631415033\n",
      "Epoch: 413, mean loss: 2136.7634341771827\n",
      "Epoch: 414, mean loss: 2074.206842644641\n",
      "Epoch: 415, mean loss: 1361.581875633199\n",
      "Epoch: 416, mean loss: 2262.39164223917\n",
      "Epoch: 417, mean loss: 1487.2446841796782\n",
      "Epoch: 418, mean loss: 2142.619568930271\n",
      "Epoch: 419, mean loss: 2057.1620935102924\n",
      "Epoch: 420, mean loss: 1993.3351488991016\n",
      "Epoch: 421, mean loss: 1986.2507706915994\n",
      "Epoch: 422, mean loss: 2776.1930929879654\n",
      "Epoch: 423, mean loss: 2262.538916129525\n",
      "Epoch: 424, mean loss: 1566.3336509032215\n",
      "Epoch: 425, mean loss: 1352.0534239453557\n",
      "Epoch: 426, mean loss: 1888.7671971492987\n",
      "Epoch: 427, mean loss: 1607.4258085164358\n",
      "Epoch: 428, mean loss: 2303.91703304468\n",
      "Epoch: 429, mean loss: 2141.8319523677333\n",
      "Epoch: 430, mean loss: 2222.2155693469153\n",
      "Epoch: 431, mean loss: 2034.5359809869392\n",
      "Epoch: 432, mean loss: 2133.6478522190646\n",
      "Epoch: 433, mean loss: 1964.7295481976337\n",
      "Epoch: 434, mean loss: 1662.2793485433544\n",
      "Epoch: 435, mean loss: 2503.726694552334\n",
      "Epoch: 436, mean loss: 1608.9872123396824\n",
      "Epoch: 437, mean loss: 1767.598044848686\n",
      "Epoch: 438, mean loss: 1722.3482181581776\n",
      "Epoch: 439, mean loss: 1308.8067178623317\n",
      "Epoch: 440, mean loss: 1572.441441664254\n",
      "Epoch: 441, mean loss: 1783.1253466897024\n",
      "Epoch: 442, mean loss: 2116.2911147578598\n",
      "Epoch: 443, mean loss: 2035.8490191172432\n",
      "Epoch: 444, mean loss: 1743.4480289246885\n",
      "Epoch: 445, mean loss: 2010.1537491589165\n",
      "Epoch: 446, mean loss: 1403.5676054205\n",
      "Epoch: 447, mean loss: 1371.6368079848803\n",
      "Epoch: 448, mean loss: 1467.97909410666\n",
      "Epoch: 449, mean loss: 1795.9854004670901\n",
      "Epoch: 450, mean loss: 1560.242535020338\n",
      "Epoch: 451, mean loss: 1586.2019509715353\n",
      "Epoch: 452, mean loss: 1504.5929526505977\n",
      "Epoch: 453, mean loss: 1434.3060889130543\n",
      "Epoch: 454, mean loss: 1935.4501939750385\n",
      "Epoch: 455, mean loss: 1847.465810589277\n",
      "Epoch: 456, mean loss: 1764.4769149722126\n",
      "Epoch: 457, mean loss: 2100.6185066867556\n",
      "Epoch: 458, mean loss: 1929.2965133300397\n",
      "Epoch: 459, mean loss: 1698.1143537266357\n",
      "Epoch: 460, mean loss: 1653.3775795848603\n",
      "Epoch: 461, mean loss: 1409.8419748610077\n",
      "Epoch: 462, mean loss: 1496.5613093523702\n",
      "Epoch: 463, mean loss: 1382.667817673989\n",
      "Epoch: 464, mean loss: 1283.5392365559592\n",
      "Epoch: 465, mean loss: 1666.4294493052057\n",
      "Epoch: 466, mean loss: 1284.6845496632063\n",
      "Epoch: 467, mean loss: 1865.7178471039483\n",
      "Epoch: 468, mean loss: 1616.7691148428892\n",
      "Epoch: 469, mean loss: 1490.2104585800248\n",
      "Epoch: 470, mean loss: 1503.1138507596777\n",
      "Epoch: 471, mean loss: 1288.4274929503235\n",
      "Epoch: 472, mean loss: 1576.4036704571417\n",
      "Epoch: 473, mean loss: 1173.1286742100485\n",
      "Epoch: 474, mean loss: 1384.081387996124\n",
      "Epoch: 475, mean loss: 1706.186051503865\n",
      "Epoch: 476, mean loss: 1466.7737600042828\n",
      "Epoch: 477, mean loss: 1318.201309682137\n",
      "Epoch: 478, mean loss: 1483.8961161558557\n",
      "Epoch: 479, mean loss: 1183.3802756778691\n",
      "Epoch: 480, mean loss: 1183.6205619118982\n",
      "Epoch: 481, mean loss: 1336.1788903433094\n",
      "Epoch: 482, mean loss: 1225.434456259717\n",
      "Epoch: 483, mean loss: 1266.022798839589\n",
      "Epoch: 484, mean loss: 1095.370406470679\n",
      "Epoch: 485, mean loss: 1222.074313724699\n",
      "Epoch: 486, mean loss: 1582.1207004912512\n",
      "Epoch: 487, mean loss: 1237.2050917033548\n",
      "Epoch: 488, mean loss: 1214.6231562143112\n",
      "Epoch: 489, mean loss: 1169.9247159161532\n",
      "Epoch: 490, mean loss: 1179.1792257338939\n",
      "Epoch: 491, mean loss: 1121.0456949186034\n",
      "Epoch: 492, mean loss: 1070.2905846305262\n",
      "Epoch: 493, mean loss: 1435.1642880058585\n",
      "Epoch: 494, mean loss: 1214.371140922841\n",
      "Epoch: 495, mean loss: 1247.7037347407174\n",
      "Epoch: 496, mean loss: 1193.814271935401\n",
      "Epoch: 497, mean loss: 1094.5243776738223\n",
      "Epoch: 498, mean loss: 1378.1023014295088\n",
      "Epoch: 499, mean loss: 1266.6352044581947\n",
      "Done\n",
      "Epoch: 0, mean loss: 20932.233220750903\n",
      "Epoch: 1, mean loss: 10436.897380945573\n",
      "Epoch: 2, mean loss: 15810.24251740774\n",
      "Epoch: 3, mean loss: 22279.033402711837\n",
      "Epoch: 4, mean loss: 11820.751064230744\n",
      "Epoch: 5, mean loss: 15097.247616435645\n",
      "Epoch: 6, mean loss: 14193.789497610775\n",
      "Epoch: 7, mean loss: 14233.464374245701\n",
      "Epoch: 8, mean loss: 12643.738200816973\n",
      "Epoch: 9, mean loss: 14998.448168479954\n",
      "Epoch: 10, mean loss: 15016.543545597455\n",
      "Epoch: 11, mean loss: 17914.019741524447\n",
      "Epoch: 12, mean loss: 12713.728653892695\n",
      "Epoch: 13, mean loss: 20535.949024034984\n",
      "Epoch: 14, mean loss: 15772.187591825297\n",
      "Epoch: 15, mean loss: 14234.848389418788\n",
      "Epoch: 16, mean loss: 9448.966999785494\n",
      "Epoch: 17, mean loss: 16889.370902526585\n",
      "Epoch: 18, mean loss: 23081.332187494834\n",
      "Epoch: 19, mean loss: 15093.570954020162\n",
      "Epoch: 20, mean loss: 15357.825780894438\n",
      "Epoch: 21, mean loss: 20125.052152505777\n",
      "Epoch: 22, mean loss: 11470.290330679445\n",
      "Epoch: 23, mean loss: 14689.05283362834\n",
      "Epoch: 24, mean loss: 14109.39735390252\n",
      "Epoch: 25, mean loss: 14969.963163289685\n",
      "Epoch: 26, mean loss: 17745.288502494917\n",
      "Epoch: 27, mean loss: 17099.363507744638\n",
      "Epoch: 28, mean loss: 14842.669960850602\n",
      "Epoch: 29, mean loss: 10553.74899403416\n",
      "Epoch: 30, mean loss: 13194.10347196989\n",
      "Epoch: 31, mean loss: 12054.022211149453\n",
      "Epoch: 32, mean loss: 17746.1154252667\n",
      "Epoch: 33, mean loss: 14009.047208722432\n",
      "Epoch: 34, mean loss: 13457.296379866937\n",
      "Epoch: 35, mean loss: 13714.662896427526\n",
      "Epoch: 36, mean loss: 16021.32268689004\n",
      "Epoch: 37, mean loss: 11435.468903767487\n",
      "Epoch: 38, mean loss: 10910.574539849016\n",
      "Epoch: 39, mean loss: 9612.375014420251\n",
      "Epoch: 40, mean loss: 15860.322559977609\n",
      "Epoch: 41, mean loss: 12898.592363787464\n",
      "Epoch: 42, mean loss: 11506.56243872276\n",
      "Epoch: 43, mean loss: 9493.355334284823\n",
      "Epoch: 44, mean loss: 14098.409571370074\n",
      "Epoch: 45, mean loss: 9887.088907960968\n",
      "Epoch: 46, mean loss: 9935.06358851674\n",
      "Epoch: 47, mean loss: 15130.597059359217\n",
      "Epoch: 48, mean loss: 10807.37457554571\n",
      "Epoch: 49, mean loss: 12849.494953736032\n",
      "Epoch: 50, mean loss: 16033.031562326376\n",
      "Epoch: 51, mean loss: 12492.257302366945\n",
      "Epoch: 52, mean loss: 17353.32040477187\n",
      "Epoch: 53, mean loss: 16462.040025023398\n",
      "Epoch: 54, mean loss: 14320.954077821232\n",
      "Epoch: 55, mean loss: 10548.430801361734\n",
      "Epoch: 56, mean loss: 8879.744252087065\n",
      "Epoch: 57, mean loss: 8801.204633475982\n",
      "Epoch: 58, mean loss: 11665.025358812441\n",
      "Epoch: 59, mean loss: 15552.139620560058\n",
      "Epoch: 60, mean loss: 14115.019291187842\n",
      "Epoch: 61, mean loss: 13621.926349887199\n",
      "Epoch: 62, mean loss: 8575.35002778534\n",
      "Epoch: 63, mean loss: 12526.307209122444\n",
      "Epoch: 64, mean loss: 11941.856870217533\n",
      "Epoch: 65, mean loss: 10052.908724829284\n",
      "Epoch: 66, mean loss: 7936.0614613345115\n",
      "Epoch: 67, mean loss: 8283.533007450978\n",
      "Epoch: 68, mean loss: 9796.996671843022\n",
      "Epoch: 69, mean loss: 7340.127109784596\n",
      "Epoch: 70, mean loss: 12132.238428402212\n",
      "Epoch: 71, mean loss: 12799.064489580558\n",
      "Epoch: 72, mean loss: 7619.373909264671\n",
      "Epoch: 73, mean loss: 10015.388906106838\n",
      "Epoch: 74, mean loss: 9877.731886375153\n",
      "Epoch: 75, mean loss: 11910.698703248652\n",
      "Epoch: 76, mean loss: 12558.348090277317\n",
      "Epoch: 77, mean loss: 12322.110180401365\n",
      "Epoch: 78, mean loss: 14057.960692061775\n",
      "Epoch: 79, mean loss: 11561.65521841966\n",
      "Epoch: 80, mean loss: 8998.669181712867\n",
      "Epoch: 81, mean loss: 13559.191923566337\n",
      "Epoch: 82, mean loss: 11298.921570123728\n",
      "Epoch: 83, mean loss: 9125.90506342575\n",
      "Epoch: 84, mean loss: 6779.138215224462\n",
      "Epoch: 85, mean loss: 10645.084180151212\n",
      "Epoch: 86, mean loss: 9562.97634967497\n",
      "Epoch: 87, mean loss: 9830.86993713035\n",
      "Epoch: 88, mean loss: 10194.696937634635\n",
      "Epoch: 89, mean loss: 10508.96666686888\n",
      "Epoch: 90, mean loss: 6533.804864320542\n",
      "Epoch: 91, mean loss: 9993.549645858098\n",
      "Epoch: 92, mean loss: 4429.236272323934\n",
      "Epoch: 93, mean loss: 10154.740597912756\n",
      "Epoch: 94, mean loss: 9217.080472701116\n",
      "Epoch: 95, mean loss: 9109.491759582463\n",
      "Epoch: 96, mean loss: 10411.364186254026\n",
      "Epoch: 97, mean loss: 8638.305099474035\n",
      "Epoch: 98, mean loss: 8984.35428261293\n",
      "Epoch: 99, mean loss: 8451.993898626208\n",
      "Epoch: 100, mean loss: 8035.6413588888445\n",
      "Epoch: 101, mean loss: 15586.669123960926\n",
      "Epoch: 102, mean loss: 8692.77158816621\n",
      "Epoch: 103, mean loss: 6866.439398182822\n",
      "Epoch: 104, mean loss: 8308.426763844862\n",
      "Epoch: 105, mean loss: 8934.375055487868\n",
      "Epoch: 106, mean loss: 10377.426264034057\n",
      "Epoch: 107, mean loss: 9974.390776694598\n",
      "Epoch: 108, mean loss: 8940.207660634967\n",
      "Epoch: 109, mean loss: 7031.579103673127\n",
      "Epoch: 110, mean loss: 10242.265544216985\n",
      "Epoch: 111, mean loss: 8923.881323366271\n",
      "Epoch: 112, mean loss: 8806.460907007384\n",
      "Epoch: 113, mean loss: 9434.882893112339\n",
      "Epoch: 114, mean loss: 8603.044051361867\n",
      "Epoch: 115, mean loss: 8350.766179538618\n",
      "Epoch: 116, mean loss: 6415.264109298561\n",
      "Epoch: 117, mean loss: 9785.764380113567\n",
      "Epoch: 118, mean loss: 10902.413201994086\n",
      "Epoch: 119, mean loss: 6294.325786111208\n",
      "Epoch: 120, mean loss: 9127.068167273106\n",
      "Epoch: 121, mean loss: 7323.721118720291\n",
      "Epoch: 122, mean loss: 9516.186221061289\n",
      "Epoch: 123, mean loss: 7500.282266132692\n",
      "Epoch: 124, mean loss: 7416.514402850761\n",
      "Epoch: 125, mean loss: 8395.173419531688\n",
      "Epoch: 126, mean loss: 7919.699901904689\n",
      "Epoch: 127, mean loss: 10010.971672136673\n",
      "Epoch: 128, mean loss: 9141.863258335035\n",
      "Epoch: 129, mean loss: 6638.68099527954\n",
      "Epoch: 130, mean loss: 7375.024917198432\n",
      "Epoch: 131, mean loss: 9683.540030717535\n",
      "Epoch: 132, mean loss: 8412.576985592297\n",
      "Epoch: 133, mean loss: 7826.651035267755\n",
      "Epoch: 134, mean loss: 6718.303375416056\n",
      "Epoch: 135, mean loss: 10792.832125580862\n",
      "Epoch: 136, mean loss: 7739.0125705294695\n",
      "Epoch: 137, mean loss: 6267.573541642229\n",
      "Epoch: 138, mean loss: 9489.013937631658\n",
      "Epoch: 139, mean loss: 8006.974277386585\n",
      "Epoch: 140, mean loss: 6956.852068832609\n",
      "Epoch: 141, mean loss: 7484.57550144194\n",
      "Epoch: 142, mean loss: 6353.977882984647\n",
      "Epoch: 143, mean loss: 6776.691013061148\n",
      "Epoch: 144, mean loss: 5974.7907150724\n",
      "Epoch: 145, mean loss: 9526.1629930223\n",
      "Epoch: 146, mean loss: 8194.320650910373\n",
      "Epoch: 147, mean loss: 7072.579788258084\n",
      "Epoch: 148, mean loss: 7202.506852774864\n",
      "Epoch: 149, mean loss: 9091.715875687638\n",
      "Epoch: 150, mean loss: 6192.944058561522\n",
      "Epoch: 151, mean loss: 5935.227021069349\n",
      "Epoch: 152, mean loss: 7949.050015338708\n",
      "Epoch: 153, mean loss: 7901.23986424366\n",
      "Epoch: 154, mean loss: 7103.739807365118\n",
      "Epoch: 155, mean loss: 7660.645088574863\n",
      "Epoch: 156, mean loss: 7737.040204292289\n",
      "Epoch: 157, mean loss: 9135.681005212666\n",
      "Epoch: 158, mean loss: 8185.92057168858\n",
      "Epoch: 159, mean loss: 6881.925790704874\n",
      "Epoch: 160, mean loss: 5088.939889204068\n",
      "Epoch: 161, mean loss: 6531.914426670947\n",
      "Epoch: 162, mean loss: 8308.360366194745\n",
      "Epoch: 163, mean loss: 6640.591277480631\n",
      "Epoch: 164, mean loss: 6627.701022249608\n",
      "Epoch: 165, mean loss: 5742.221503313618\n",
      "Epoch: 166, mean loss: 7274.676618724977\n",
      "Epoch: 167, mean loss: 6501.62412207209\n",
      "Epoch: 168, mean loss: 7011.222349817382\n",
      "Epoch: 169, mean loss: 7757.826367924435\n",
      "Epoch: 170, mean loss: 6673.4292618927\n",
      "Epoch: 171, mean loss: 7346.255047252901\n",
      "Epoch: 172, mean loss: 8034.356491219243\n",
      "Epoch: 173, mean loss: 6059.3282040621925\n",
      "Epoch: 174, mean loss: 6685.297954406044\n",
      "Epoch: 175, mean loss: 6493.260342496277\n",
      "Epoch: 176, mean loss: 6002.689449255546\n",
      "Epoch: 177, mean loss: 7630.516271550412\n",
      "Epoch: 178, mean loss: 5601.9468995692405\n",
      "Epoch: 179, mean loss: 5059.252766124633\n",
      "Epoch: 180, mean loss: 5904.209482082978\n",
      "Epoch: 181, mean loss: 8426.654429791268\n",
      "Epoch: 182, mean loss: 6462.314988834955\n",
      "Epoch: 183, mean loss: 6221.863409918387\n",
      "Epoch: 184, mean loss: 6289.584775169803\n",
      "Epoch: 185, mean loss: 5766.842498473735\n",
      "Epoch: 186, mean loss: 4811.001402982832\n",
      "Epoch: 187, mean loss: 7536.338572805507\n",
      "Epoch: 188, mean loss: 6794.213074939758\n",
      "Epoch: 189, mean loss: 6766.45518038718\n",
      "Epoch: 190, mean loss: 6132.814196213319\n",
      "Epoch: 191, mean loss: 6738.851046563539\n",
      "Epoch: 192, mean loss: 8029.929731293634\n",
      "Epoch: 193, mean loss: 7687.878385527662\n",
      "Epoch: 194, mean loss: 7544.5809388009075\n",
      "Epoch: 195, mean loss: 5960.248483442535\n",
      "Epoch: 196, mean loss: 7794.778534782663\n",
      "Epoch: 197, mean loss: 6797.297756940903\n",
      "Epoch: 198, mean loss: 6659.2180340508485\n",
      "Epoch: 199, mean loss: 7952.957506968571\n",
      "Epoch: 200, mean loss: 5928.215623737389\n",
      "Epoch: 201, mean loss: 6427.868604226477\n",
      "Epoch: 202, mean loss: 6810.839377936048\n",
      "Epoch: 203, mean loss: 7207.546115193454\n",
      "Epoch: 204, mean loss: 5105.38638943239\n",
      "Epoch: 205, mean loss: 6946.116497327754\n",
      "Epoch: 206, mean loss: 5233.353856611659\n",
      "Epoch: 207, mean loss: 6140.445900799878\n",
      "Epoch: 208, mean loss: 6246.345008570008\n",
      "Epoch: 209, mean loss: 7297.753891083981\n",
      "Epoch: 210, mean loss: 5062.946221717085\n",
      "Epoch: 211, mean loss: 5247.320160510852\n",
      "Epoch: 212, mean loss: 8178.865785079796\n",
      "Epoch: 213, mean loss: 6147.668988717596\n",
      "Epoch: 214, mean loss: 5638.135151467033\n",
      "Epoch: 215, mean loss: 6082.49189732936\n",
      "Epoch: 216, mean loss: 5284.272595322027\n",
      "Epoch: 217, mean loss: 7875.427537473933\n",
      "Epoch: 218, mean loss: 7130.212193110693\n",
      "Epoch: 219, mean loss: 4821.767856082341\n",
      "Epoch: 220, mean loss: 5839.789257308002\n",
      "Epoch: 221, mean loss: 6247.758887045917\n",
      "Epoch: 222, mean loss: 7238.056111708709\n",
      "Epoch: 223, mean loss: 6814.5199054759905\n",
      "Epoch: 224, mean loss: 5684.386781378922\n",
      "Epoch: 225, mean loss: 5860.198506305912\n",
      "Epoch: 226, mean loss: 5326.824374459271\n",
      "Epoch: 227, mean loss: 5863.136567380455\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scale(X_test, minx, maxx)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m net \u001b[38;5;129;01min\u001b[39;00m nets:\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[92], line 215\u001b[0m, in \u001b[0;36mNeuralNet.train\u001b[1;34m(self, X, Y, learning_rate, epochs, print_logs, batch_size)\u001b[0m\n\u001b[0;32m    213\u001b[0m batch_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng\u001b[38;5;241m.\u001b[39mintegers(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X), size\u001b[38;5;241m=\u001b[39mn)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X[batch_ind], Y[batch_ind]):\n\u001b[1;32m--> 215\u001b[0m     tot_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop_single_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     tot_grad \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    217\u001b[0m         (grad[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m neuron\u001b[38;5;241m.\u001b[39mw_grad, grad[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m neuron\u001b[38;5;241m.\u001b[39mb_grad)\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m grad, neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tot_grad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons)\n\u001b[0;32m    219\u001b[0m     ]\n\u001b[0;32m    220\u001b[0m tot_error \u001b[38;5;241m=\u001b[39m tot_error \u001b[38;5;241m/\u001b[39m n\n",
      "Cell \u001b[1;32mIn[92], line 166\u001b[0m, in \u001b[0;36mNeuralNet.backprop_single_input\u001b[1;34m(self, input, y_true)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop_single_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, y_true):\n\u001b[1;32m--> 166\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_f(y_true, y_pred)\n\u001b[0;32m    168\u001b[0m     error_d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_f_d(y_pred\u001b[38;5;241m=\u001b[39my_pred, y_true\u001b[38;5;241m=\u001b[39my_true)\n",
      "Cell \u001b[1;32mIn[92], line 162\u001b[0m, in \u001b[0;36mNeuralNet.predict\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    160\u001b[0m outputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 162\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m layer]\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_activation_f(outputs)\n",
      "Cell \u001b[1;32mIn[92], line 69\u001b[0m, in \u001b[0;36mNeuron.work\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwork\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(inputs)\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_f(\u001b[38;5;28msum\u001b[39m)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:2349\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2345\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[1;32m-> 2349\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[0;32m   2350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2351\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2353\u001b[0m \u001b[38;5;124;03m    Sum of array elements over a given axis.\u001b[39;00m\n\u001b[0;32m   2354\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2468\u001b[0m \u001b[38;5;124;03m    15\u001b[39;00m\n\u001b[0;32m   2469\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, _gentype):\n\u001b[0;32m   2471\u001b[0m         \u001b[38;5;66;03m# 2018-02-25, 1.15.0\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = np.genfromtxt(\n",
    "    \"./projekt1/regression/data.activation.train.1000.csv\", delimiter=\",\"\n",
    ")\n",
    "test_data = np.genfromtxt(\n",
    "    \"./projekt1/regression/data.activation.test.1000.csv\", delimiter=\",\"\n",
    ")\n",
    "X_train = train_data[1:, 0]\n",
    "Y_train = train_data[1:, 1]\n",
    "X_test = test_data[1:, 0]\n",
    "Y_test = test_data[1:, 1]\n",
    "minx = min(X_train)\n",
    "maxx = max(X_train)\n",
    "\n",
    "numbers_of_layer = [0,1,2,3,4]\n",
    "numbers_of_neurons = [1,5,20,50,100]\n",
    "\n",
    "nets = []\n",
    "for l in numbers_of_layer:\n",
    "    for n in numbers_of_neurons: \n",
    "        nets.append(NeuralNet(hidden_layers=l, number_of_neurons_in_layer=n, input_dim=1, activation_f=\"relu\"))\n",
    "    \n",
    "\n",
    "X_scaled = scale(X_train, minx, maxx)\n",
    "X_test_scaled = scale(X_test, minx, maxx)\n",
    "for net in nets:\n",
    "    net.train(X_scaled, Y_train, learning_rate=0.0001, epochs=500, print_logs=True)\n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for net in nets:\n",
    "    plt.plot(X_test_scaled, Y_test, label = 'test')\n",
    "    Y_pred = [net.predict_regre(x) for x in X_test_scaled]\n",
    "    plt.plot(X_test_scaled, Y_pred, label = 'pred')\n",
    "    plt.show()\n",
    "             \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt(\n",
    "    \"./projekt1/regression/data.activation.train.1000.csv\", delimiter=\",\"\n",
    ")\n",
    "test_data = np.genfromtxt(\n",
    "    \"./projekt1/regression/data.activation.test.1000.csv\", delimiter=\",\"\n",
    ")\n",
    "X_train = train_data[1:, 0]\n",
    "Y_train = train_data[1:, 1]\n",
    "X_test = test_data[1:, 0]\n",
    "Y_test = test_data[1:, 1]\n",
    "minx = min(X_train)\n",
    "maxx = max(X_train)\n",
    "\n",
    "numbers_of_layer = [0,1,2,3,4]\n",
    "numbers_of_neurons = [1,5,20,50,100]\n",
    "\n",
    "net = NeuralNet(hidden_layers=4, number_of_neurons_in_layer=100, input_dim=1, activation_f=\"relu\")\n",
    "    \n",
    "\n",
    "X_scaled = scale(X_train, minx, maxx)\n",
    "X_test_scaled = scale(X_test, minx, maxx)\n",
    "net.train_regre(X_scaled, Y_train, learning_rate=0.0001, epochs=500, print_logs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d011bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X_test_scaled, Y_test, label = 'test')\n",
    "Y_pred = [net.predict_regre(x) for x in X_test_scaled]\n",
    "plt.plot(X_test_scaled, Y_pred, label = 'pred')\n",
    "plt.show()             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a1d91",
   "metadata": {},
   "source": [
    "# Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a77476b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mnist_reader import read_mnist_data, plot_img\n",
    "images, labels = read_mnist_data(img_path=\"./mnist/train-images.idx3-ubyte\", labels_path=\"./mnist/train-labels.idx1-ubyte\")\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "print(labels[1])\n",
    "plot_img(images[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911968a",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "73fe1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from mnist_reader import read_mnist_data, plot_img\n",
    "\n",
    "images_traing, labels_train = read_mnist_data(img_path=\"./mnist/train-images.idx3-ubyte\", labels_path=\"./mnist/train-labels.idx1-ubyte\")\n",
    "images_test, labels_test = read_mnist_data(img_path=\"./mnist/t10k-images.idx3-ubyte\", labels_path=\"./mnist/t10k-labels.idx1-ubyte\")\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(labels_train.reshape(-1,1))\n",
    "\n",
    "Y_train = enc.transform(labels_train.reshape(-1,1)).toarray()\n",
    "X_train = images_traing.reshape(60000,28*28)/255.0 - 0.5\n",
    "Y_test = enc.transform(labels_test.reshape(-1,1)).toarray()\n",
    "X_test = images_test.reshape(10000,28*28)/255.0 - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "995551be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, mean loss: 5.768263729929753\n",
      "Epoch: 1, mean loss: 5.065732687969692\n",
      "Epoch: 2, mean loss: 5.078281752609003\n",
      "Epoch: 3, mean loss: 4.81277648893807\n",
      "Epoch: 4, mean loss: 5.836615903590104\n",
      "Epoch: 5, mean loss: 4.613334766491152\n",
      "Epoch: 6, mean loss: 3.499151793738627\n",
      "Epoch: 7, mean loss: 4.9012411901340425\n",
      "Epoch: 8, mean loss: 3.792851820487632\n",
      "Epoch: 9, mean loss: 3.8129417398647405\n",
      "Epoch: 10, mean loss: 4.178926467307597\n",
      "Epoch: 11, mean loss: 3.271906165834336\n",
      "Epoch: 12, mean loss: 4.1683857629538945\n",
      "Epoch: 13, mean loss: 3.133783502173677\n",
      "Epoch: 14, mean loss: 2.6937492022004874\n",
      "Epoch: 15, mean loss: 3.4007833914551777\n",
      "Epoch: 16, mean loss: 3.1778598699541205\n",
      "Epoch: 17, mean loss: 3.1763229296727475\n",
      "Epoch: 18, mean loss: 2.330833338897622\n",
      "Epoch: 19, mean loss: 2.8245285079099807\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[236], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m NeuralNet(\n\u001b[0;32m      2\u001b[0m     number_of_neurons_in_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m      3\u001b[0m     hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[235], line 222\u001b[0m, in \u001b[0;36mNeuralNet.train\u001b[1;34m(self, X, Y, learning_rate, epochs, print_logs, batch_size, momentum)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X[batch_ind], Y[batch_ind]):\n\u001b[0;32m    220\u001b[0m     tot_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackprop_single_input(x, y)\n\u001b[0;32m    221\u001b[0m     tot_grad \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 222\u001b[0m         (grad[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m neuron\u001b[38;5;241m.\u001b[39mw_grad, grad[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m neuron\u001b[38;5;241m.\u001b[39mb_grad)\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m grad, neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tot_grad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons)\n\u001b[0;32m    224\u001b[0m     ]\n\u001b[0;32m    225\u001b[0m tot_error \u001b[38;5;241m=\u001b[39m tot_error \u001b[38;5;241m/\u001b[39m n\n\u001b[0;32m    226\u001b[0m prev_grad \u001b[38;5;241m=\u001b[39m [(tot[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mn \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m prev[\u001b[38;5;241m0\u001b[39m],tot[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39mn \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m prev[\u001b[38;5;241m1\u001b[39m] ) \u001b[38;5;28;01mfor\u001b[39;00m tot, prev \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tot_grad, prev_grad)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "classifier = NeuralNet(\n",
    "    number_of_neurons_in_layer=50,\n",
    "    hidden_layers=3,\n",
    "    input_dim=28*28,\n",
    "    number_of_outputs=10,\n",
    "    activation_f='tanh',\n",
    "    loss_f='crossentropy',\n",
    "    out_activation_f='softmax',\n",
    "    seed=10\n",
    ")\n",
    "\n",
    "classifier.train(X = X_train, Y=Y_train, learning_rate=0.01, epochs=10000, batch_size=32, print_logs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc393f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy 91%\n",
    "\n",
    "# classifier = NeuralNet(\n",
    "#     number_of_neurons_in_layer=100,\n",
    "#     hidden_layers=3,\n",
    "#     input_dim=28*28,\n",
    "#     number_of_outputs=10,\n",
    "#     activation_f='tanh',\n",
    "#     loss_f='crossentropy',\n",
    "#     out_activation_f='softmax',\n",
    "#     seed=10\n",
    "# )\n",
    "\n",
    "# importNeuralNetWeights(classifier, '89 (1).txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0601be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, mean loss: 2.4259348206287865\n",
      "Epoch: 1, mean loss: 2.413090018488392\n",
      "Epoch: 2, mean loss: 2.4610063480811784\n",
      "Epoch: 3, mean loss: 2.480098287886002\n",
      "Epoch: 4, mean loss: 2.582691723024386\n",
      "Epoch: 5, mean loss: 2.3765323572635006\n",
      "Epoch: 6, mean loss: 2.4311515986451404\n",
      "Epoch: 7, mean loss: 2.350174469349835\n",
      "Epoch: 8, mean loss: 2.319451723638116\n",
      "Epoch: 9, mean loss: 2.276252557453513\n",
      "Epoch: 10, mean loss: 2.2809078142071595\n",
      "Epoch: 11, mean loss: 2.3360956731593236\n",
      "Epoch: 12, mean loss: 2.271418074014015\n",
      "Epoch: 13, mean loss: 2.2440367303268474\n",
      "Epoch: 14, mean loss: 2.262419959447306\n",
      "Epoch: 15, mean loss: 2.3029246737520332\n",
      "Epoch: 16, mean loss: 2.3201657238322797\n",
      "Epoch: 17, mean loss: 2.2533144661186832\n",
      "Epoch: 18, mean loss: 2.2624896306022526\n",
      "Epoch: 19, mean loss: 2.373501266782351\n",
      "Epoch: 20, mean loss: 2.409390612950209\n",
      "Epoch: 21, mean loss: 2.253207220329645\n",
      "Epoch: 22, mean loss: 2.2980246274573872\n",
      "Epoch: 23, mean loss: 2.310778289496706\n",
      "Epoch: 24, mean loss: 2.317357671252343\n",
      "Epoch: 25, mean loss: 2.228799775635789\n",
      "Epoch: 26, mean loss: 2.208206551173281\n",
      "Epoch: 27, mean loss: 2.177815532193817\n",
      "Epoch: 28, mean loss: 2.2044286742043417\n",
      "Epoch: 29, mean loss: 2.234907633184828\n",
      "Epoch: 30, mean loss: 2.1890333441921115\n",
      "Epoch: 31, mean loss: 2.2200981437676557\n",
      "Epoch: 32, mean loss: 2.1790338494512542\n",
      "Epoch: 33, mean loss: 2.2213439791098355\n",
      "Epoch: 34, mean loss: 2.21101774043284\n",
      "Epoch: 35, mean loss: 2.2120789886051653\n",
      "Epoch: 36, mean loss: 2.2391366695205805\n",
      "Epoch: 37, mean loss: 2.2560374741036906\n",
      "Epoch: 38, mean loss: 2.0687053394142785\n",
      "Epoch: 39, mean loss: 2.198765532127058\n",
      "Epoch: 40, mean loss: 2.2205414810874813\n",
      "Epoch: 41, mean loss: 2.1559298608740343\n",
      "Epoch: 42, mean loss: 2.1790600587074427\n",
      "Epoch: 43, mean loss: 2.206401534652253\n",
      "Epoch: 44, mean loss: 2.0966993698013536\n",
      "Epoch: 45, mean loss: 2.216763410098087\n",
      "Epoch: 46, mean loss: 2.262991076499181\n",
      "Epoch: 47, mean loss: 2.198056574050405\n",
      "Epoch: 48, mean loss: 2.1804174726809844\n",
      "Epoch: 49, mean loss: 2.136606949301816\n",
      "Epoch: 50, mean loss: 2.1342045673461527\n",
      "Epoch: 51, mean loss: 2.1294073842961625\n",
      "Epoch: 52, mean loss: 2.174703643150269\n",
      "Epoch: 53, mean loss: 2.1072586055912725\n",
      "Epoch: 54, mean loss: 2.1084702124655763\n",
      "Epoch: 55, mean loss: 2.1621255846175185\n",
      "Epoch: 56, mean loss: 2.1213859634152246\n",
      "Epoch: 57, mean loss: 2.1126268557995673\n",
      "Epoch: 58, mean loss: 2.0765014246686957\n",
      "Epoch: 59, mean loss: 2.1147532918560623\n",
      "Epoch: 60, mean loss: 2.0303676299008426\n",
      "Epoch: 61, mean loss: 2.0659524166212266\n",
      "Epoch: 62, mean loss: 2.0347471029502007\n",
      "Epoch: 63, mean loss: 2.0850604829490282\n",
      "Epoch: 64, mean loss: 2.0213240231876872\n",
      "Epoch: 65, mean loss: 2.0234821297170003\n",
      "Epoch: 66, mean loss: 1.962232351960905\n",
      "Epoch: 67, mean loss: 2.0183464125710096\n",
      "Epoch: 68, mean loss: 1.9172489113696747\n",
      "Epoch: 69, mean loss: 2.031216812904143\n",
      "Epoch: 70, mean loss: 2.08918148676383\n",
      "Epoch: 71, mean loss: 1.948351939058859\n",
      "Epoch: 72, mean loss: 2.076225783402148\n",
      "Epoch: 73, mean loss: 1.9430790004607141\n",
      "Epoch: 74, mean loss: 1.923316305514912\n",
      "Epoch: 75, mean loss: 1.8867376628281807\n",
      "Epoch: 76, mean loss: 1.9462190081679072\n",
      "Epoch: 77, mean loss: 1.805983027185501\n",
      "Epoch: 78, mean loss: 1.8996468211348398\n",
      "Epoch: 79, mean loss: 1.8113571917442894\n",
      "Epoch: 80, mean loss: 1.783476490486122\n",
      "Epoch: 81, mean loss: 1.8762283691541848\n",
      "Epoch: 82, mean loss: 1.9866354070823533\n",
      "Epoch: 83, mean loss: 1.8338069756847803\n",
      "Epoch: 84, mean loss: 1.8559858844219947\n",
      "Epoch: 85, mean loss: 1.7665794422254444\n",
      "Epoch: 86, mean loss: 1.9288447811941625\n",
      "Epoch: 87, mean loss: 1.6528691044954187\n",
      "Epoch: 88, mean loss: 1.8261025480272985\n",
      "Epoch: 89, mean loss: 1.6843299323620946\n",
      "Epoch: 90, mean loss: 1.7254864729803105\n",
      "Epoch: 91, mean loss: 1.71258792433872\n",
      "Epoch: 92, mean loss: 1.724945236434317\n",
      "Epoch: 93, mean loss: 1.6744447144356993\n",
      "Epoch: 94, mean loss: 1.6497894033582814\n",
      "Epoch: 95, mean loss: 1.7283581916725856\n",
      "Epoch: 96, mean loss: 1.7569353261629783\n",
      "Epoch: 97, mean loss: 1.7458341213232778\n",
      "Epoch: 98, mean loss: 1.5824945820149647\n",
      "Epoch: 99, mean loss: 1.6030108932158567\n",
      "Epoch: 100, mean loss: 1.6042063172346006\n",
      "Epoch: 101, mean loss: 1.3576592748885317\n",
      "Epoch: 102, mean loss: 1.5869569083480812\n",
      "Epoch: 103, mean loss: 1.3916424721240324\n",
      "Epoch: 104, mean loss: 1.6037315261644094\n",
      "Epoch: 105, mean loss: 1.3751871532707844\n",
      "Epoch: 106, mean loss: 1.4914075491807526\n",
      "Epoch: 107, mean loss: 1.6253637140259432\n",
      "Epoch: 108, mean loss: 1.3604119164654416\n",
      "Epoch: 109, mean loss: 1.4471115285180292\n",
      "Epoch: 110, mean loss: 1.497446033403187\n",
      "Epoch: 111, mean loss: 1.313164461547828\n",
      "Epoch: 112, mean loss: 1.3427462297150643\n",
      "Epoch: 113, mean loss: 1.4603516292162797\n",
      "Epoch: 114, mean loss: 1.5042799992441054\n",
      "Epoch: 115, mean loss: 1.2609666702461888\n",
      "Epoch: 116, mean loss: 1.644178971039906\n",
      "Epoch: 117, mean loss: 1.4068074578381569\n",
      "Epoch: 118, mean loss: 1.4102413758525432\n",
      "Epoch: 119, mean loss: 1.3480312853854028\n",
      "Epoch: 120, mean loss: 1.1917640846248743\n",
      "Epoch: 121, mean loss: 1.2945061257127048\n",
      "Epoch: 122, mean loss: 1.0713576310763162\n",
      "Epoch: 123, mean loss: 1.283330757801\n",
      "Epoch: 124, mean loss: 1.3065674393589721\n",
      "Epoch: 125, mean loss: 1.2332718289593616\n",
      "Epoch: 126, mean loss: 1.3700224318411782\n",
      "Epoch: 127, mean loss: 1.3573462083217103\n",
      "Epoch: 128, mean loss: 1.0364619474201675\n",
      "Epoch: 129, mean loss: 1.1956217401170721\n",
      "Epoch: 130, mean loss: 1.1546115968659074\n",
      "Epoch: 131, mean loss: 1.1258876144830618\n",
      "Epoch: 132, mean loss: 1.3520694442385643\n",
      "Epoch: 133, mean loss: 1.40353689733759\n",
      "Epoch: 134, mean loss: 1.2157136392275336\n",
      "Epoch: 135, mean loss: 1.0157074230295633\n",
      "Epoch: 136, mean loss: 0.8729628260519187\n",
      "Epoch: 137, mean loss: 1.3943887480165262\n",
      "Epoch: 138, mean loss: 1.2586751074163176\n",
      "Epoch: 139, mean loss: 1.0341524370468915\n",
      "Epoch: 140, mean loss: 0.9144612709128259\n",
      "Epoch: 141, mean loss: 1.2121655063218695\n",
      "Epoch: 142, mean loss: 1.050313666520622\n",
      "Epoch: 143, mean loss: 0.9962126216393891\n",
      "Epoch: 144, mean loss: 1.0793063083000856\n",
      "Epoch: 145, mean loss: 0.9687975303824755\n",
      "Epoch: 146, mean loss: 0.8457580185967875\n",
      "Epoch: 147, mean loss: 0.8483636176481274\n",
      "Epoch: 148, mean loss: 1.0252017699187816\n",
      "Epoch: 149, mean loss: 0.9720155668113767\n",
      "Epoch: 150, mean loss: 0.9879102355642522\n",
      "Epoch: 151, mean loss: 0.9901789670434009\n",
      "Epoch: 152, mean loss: 0.9220140368261002\n",
      "Epoch: 153, mean loss: 0.8435268572961478\n",
      "Epoch: 154, mean loss: 0.9121638803884652\n",
      "Epoch: 155, mean loss: 1.0211575912667648\n",
      "Epoch: 156, mean loss: 0.8113094323784271\n",
      "Epoch: 157, mean loss: 0.878560550417075\n",
      "Epoch: 158, mean loss: 0.8917550341108536\n",
      "Epoch: 159, mean loss: 0.8661137905442716\n",
      "Epoch: 160, mean loss: 0.6512048363710903\n",
      "Epoch: 161, mean loss: 0.8620902124357966\n",
      "Epoch: 162, mean loss: 0.7189923628831929\n",
      "Epoch: 163, mean loss: 0.7443602808867738\n",
      "Epoch: 164, mean loss: 0.6676236336879408\n",
      "Epoch: 165, mean loss: 0.6214153760597216\n",
      "Epoch: 166, mean loss: 1.0266681618515194\n",
      "Epoch: 167, mean loss: 0.7368771513333051\n",
      "Epoch: 168, mean loss: 1.0058275165759982\n",
      "Epoch: 169, mean loss: 0.7972320405939497\n",
      "Epoch: 170, mean loss: 1.085632611090926\n",
      "Epoch: 171, mean loss: 0.5611793280858596\n",
      "Epoch: 172, mean loss: 0.5319672579613949\n",
      "Epoch: 173, mean loss: 0.6251550012314211\n",
      "Epoch: 174, mean loss: 0.6612324476414664\n",
      "Epoch: 175, mean loss: 0.6564467613540848\n",
      "Epoch: 176, mean loss: 0.6810615968395829\n",
      "Epoch: 177, mean loss: 0.628858127331017\n",
      "Epoch: 178, mean loss: 0.8912771267106617\n",
      "Epoch: 179, mean loss: 0.5497096953284271\n",
      "Epoch: 180, mean loss: 0.6580054984675265\n",
      "Epoch: 181, mean loss: 0.8325452629398827\n",
      "Epoch: 182, mean loss: 1.1058810724173234\n",
      "Epoch: 183, mean loss: 0.6772681357121698\n",
      "Epoch: 184, mean loss: 0.5108563802963402\n",
      "Epoch: 185, mean loss: 0.5605954660939303\n",
      "Epoch: 186, mean loss: 0.7219200210376782\n",
      "Epoch: 187, mean loss: 0.9480397311443726\n",
      "Epoch: 188, mean loss: 0.609439184976164\n",
      "Epoch: 189, mean loss: 0.5117395933750334\n",
      "Epoch: 190, mean loss: 0.5449605167739876\n",
      "Epoch: 191, mean loss: 1.0815648475287634\n",
      "Epoch: 192, mean loss: 0.801462325863782\n",
      "Epoch: 193, mean loss: 0.7089656064728534\n",
      "Epoch: 194, mean loss: 0.9384868702448191\n",
      "Epoch: 195, mean loss: 1.1080167812497732\n",
      "Epoch: 196, mean loss: 0.8523541817419963\n",
      "Epoch: 197, mean loss: 0.6963319423805134\n",
      "Epoch: 198, mean loss: 1.047001314296275\n",
      "Epoch: 199, mean loss: 0.8017572483993674\n",
      "Epoch: 200, mean loss: 0.8687292824608903\n",
      "Epoch: 201, mean loss: 0.679226430421323\n",
      "Epoch: 202, mean loss: 0.7070611911886423\n",
      "Epoch: 203, mean loss: 0.7546792358648162\n",
      "Epoch: 204, mean loss: 0.6850643699038346\n",
      "Epoch: 205, mean loss: 0.8185792234475961\n",
      "Epoch: 206, mean loss: 0.6667359493316766\n",
      "Epoch: 207, mean loss: 0.6235870205006304\n",
      "Epoch: 208, mean loss: 0.5114137369820623\n",
      "Epoch: 209, mean loss: 0.732323130197236\n",
      "Epoch: 210, mean loss: 1.0366106491277023\n",
      "Epoch: 211, mean loss: 0.7555434940065293\n",
      "Epoch: 212, mean loss: 0.7704387541971259\n",
      "Epoch: 213, mean loss: 0.4493109837387042\n",
      "Epoch: 214, mean loss: 0.5306723073852516\n",
      "Epoch: 215, mean loss: 0.4752332876819407\n",
      "Epoch: 216, mean loss: 0.9643951838850724\n",
      "Epoch: 217, mean loss: 0.7361521562010316\n",
      "Epoch: 218, mean loss: 0.6374170839853898\n",
      "Epoch: 219, mean loss: 0.4493474924500377\n",
      "Epoch: 220, mean loss: 0.6224547282957996\n",
      "Epoch: 221, mean loss: 0.7048005989863944\n",
      "Epoch: 222, mean loss: 0.9709240624951603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = NeuralNet(\n",
    "    number_of_neurons_in_layer=150,\n",
    "    hidden_layers=3,\n",
    "    input_dim=28*28,\n",
    "    number_of_outputs=10,\n",
    "    activation_f='relu',\n",
    "    loss_f='crossentropy',\n",
    "    out_activation_f='softmax',\n",
    "    seed=10\n",
    ")\n",
    "classifier.train(X_train, Y_train, learning_rate=0.005, epochs=10000, batch_size=32, momentum=0.9, print_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "12b3489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(nn: NeuralNet, X, Y):\n",
    "    Y_pred = [nn.predict(x) for x in X]\n",
    "    Y_pred = [y.index(max(y)) for y in Y_pred]\n",
    "    return sum([1 if y == y_pred else 0 for y,y_pred in zip(Y.argmax(axis=1),Y_pred)])/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "6529e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "exportNeuralNet(classifier, '95.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "52e71acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9506\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = calculate_accuracy(classifier, X_test, Y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmid, 1 layer, 500 neurons, learning rate 0.1, accuracy 0.92\n",
    "# ReLU, 2 layers, 30 neurons, learning rate 0.01, accuracy 0.88"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
