{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1cb878-5da2-4a15-b73a-7dce723709e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred)**2)\n",
    "\n",
    "def loss_d(y_true, y_pred):\n",
    "    return 2 * y_pred - 2 * y_true\n",
    "\n",
    "def sigm_fun(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigm_d(sigm_val):\n",
    "    return sigm_val * (1 - sigm_val)\n",
    "\n",
    "def relu(x):\n",
    "    return 0 if x < 0 else x\n",
    "def relu_d(relu_val):\n",
    "    return 0 if relu_val <= 0 else 1\n",
    "\n",
    "def softmax(outputs):\n",
    "    exp_outputs = [math.exp(o) for o in outputs]\n",
    "    total = sum(exp_outputs)\n",
    "    return [eo / total for eo in exp_outputs]\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_true = int(y_true - 1)\n",
    "    return - (y_true* np.log(y_pred + 1e-10) + (1 - y_true) * np.log(1 - y_pred + 1e-10))\n",
    "\n",
    "def cross_entropy_loss_d(y_true, y_pred):\n",
    "    y_true = int(y_true - 1)\n",
    "    return y_pred - y_true\n",
    "\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, rng, input_dim, activation_f = sigm_fun, activation_f_d = sigm_d):\n",
    "        self.activation_f = activation_f\n",
    "        self.activation_f_d = activation_f_d\n",
    "        weight_list = []\n",
    "        for i in range(input_dim):\n",
    "            weight_list.append(rng.uniform(-1, 1))\n",
    "        self.weights = np.asarray(weight_list)\n",
    "        self.b = rng.uniform(-1, 1)\n",
    "        self.b_grad = 0\n",
    "        self.w_grad = np.zeros(input_dim)\n",
    "        self.out_grad = 0\n",
    "    \n",
    "    def work(self, inputs):\n",
    "        self.x = np.array(inputs)\n",
    "        sum = np.sum(inputs * self.weights) + self.b\n",
    "        self.output = self.activation_f(sum)\n",
    "        return self.output\n",
    "\n",
    "    def x_grad(self, i):\n",
    "        return self.out_grad * self.activation_f_d(self.output) * self.weights[i]\n",
    "\n",
    "    def generate_param_grad(self):\n",
    "        self.w_grad = self.out_grad * self.activation_f_d(self.output) * self.x\n",
    "        self.b_grad = self.out_grad * self.activation_f_d(self.output)\n",
    "        \n",
    "    def update_weights(self, errors, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * errors[i] * self.x[i]\n",
    "            \n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, number_of_neurons_in_layer, hidden_layers, input_dim, number_of_outputs = 1, seed=10):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.layers = []\n",
    "        self.neurons: list[Neuron] = []\n",
    "        layer_1 = [Neuron(self.rng, input_dim) for _ in range(number_of_neurons_in_layer)]\n",
    "        self.layers.append(layer_1)\n",
    "        self.neurons.extend(layer_1)\n",
    "        for i in range(hidden_layers - 1):\n",
    "            current_layer = [Neuron(self.rng, number_of_neurons_in_layer) \n",
    "                             for _ in range(number_of_neurons_in_layer)]\n",
    "            self.neurons.extend(current_layer)\n",
    "            self.layers.append(current_layer)\n",
    "        output_layer = []\n",
    "        for i in range(number_of_outputs):\n",
    "            output_neuron = Neuron(self.rng, number_of_neurons_in_layer, activation_f=lambda x:x, activation_f_d=lambda x:1)\n",
    "            output_layer.append(output_neuron)\n",
    "            self.neurons.append(output_neuron)\n",
    "        self.layers.append(output_layer)\n",
    "                                     \n",
    "    def predict_regre(self, inputs):\n",
    "        outputs = inputs.copy()\n",
    "        for layer in self.layers:\n",
    "            outputs = [neuron.work(outputs) for neuron in layer]\n",
    "        return outputs[0]\n",
    "    \n",
    "    def predict_class(self, inputs):\n",
    "        outputs = inputs.copy()\n",
    "        for layer in self.layers:\n",
    "            outputs = [neuron.work(outputs) for neuron in layer]\n",
    "        probabilitie = sigm_fun(outputs[0])\n",
    "    \n",
    "        return probabilitie\n",
    "\n",
    "    def backprop_single_input(self, input, expected_value):\n",
    "        y_pred = self.predict_regre(input)\n",
    "        error = loss(expected_value, y_pred)\n",
    "        error_d = loss_d(y_pred=y_pred, y_true=expected_value)\n",
    "        output_neuron = self.layers[-1][0]\n",
    "        output_neuron.out_grad = error_d\n",
    "        output_neuron.generate_param_grad()\n",
    "        next_layer = self.layers[-1]\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            for i, neuron in enumerate(layer):\n",
    "                neuron.w_grad = 0\n",
    "                neuron.out_grad = 0\n",
    "                neuron.b_grad = 0\n",
    "                for next_neuron in next_layer:\n",
    "                    neuron.out_grad += next_neuron.x_grad(i)\n",
    "                neuron.generate_param_grad()\n",
    "        return error\n",
    "    \n",
    "    def train_regre(self, X, Y, learning_rate=0.05, epochs = 50, print_logs = False):\n",
    "        n = len(X)\n",
    "        for k in range(epochs):\n",
    "            tot_error = 0\n",
    "            tot_grad = [(0, 0) for neuron in self.neurons]\n",
    "            for x, y in zip(X, Y):\n",
    "                tot_error+=self.backprop_single_input(x, y)\n",
    "                tot_grad = [(grad[0] + neuron.w_grad, grad[1] + neuron.b_grad) \n",
    "                            for grad, neuron in zip(tot_grad, self.neurons)]\n",
    "            tot_error = tot_error/n\n",
    "            for grad, neuron in zip(tot_grad, self.neurons):\n",
    "                neuron.weights -= grad[0] / n * learning_rate\n",
    "                neuron.b -= grad[1] / n * learning_rate\n",
    "            if print_logs:\n",
    "                print(f\"Epoch: {k}, mean loss: {tot_error}\")\n",
    "    \n",
    "    def backprop_binary_input(self, input, expected_value):\n",
    "        y_pred = self.predict_class(input)\n",
    "        error = cross_entropy_loss(expected_value, y_pred)\n",
    "        error_d = cross_entropy_loss_d(y_pred=y_pred, y_true=expected_value)\n",
    "        output_neuron = self.layers[-1][0]\n",
    "        output_neuron.out_grad = error_d\n",
    "        output_neuron.generate_param_grad()\n",
    "        next_layer = self.layers[-1]\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            for i, neuron in enumerate(layer):\n",
    "                neuron.w_grad = 0\n",
    "                neuron.out_grad = 0\n",
    "                neuron.b_grad = 0\n",
    "                for next_neuron in next_layer:\n",
    "                    neuron.out_grad += next_neuron.x_grad(i)\n",
    "                neuron.generate_param_grad()\n",
    "        return error\n",
    "    \n",
    "                \n",
    "    def train_class(self, X, Y, learning_rate=0.05, epochs=50, print_logs = False):\n",
    "        n = len(X)\n",
    "        for k in range(epochs):\n",
    "            tot_error = 0\n",
    "            tot_grad = [(0, 0) for neuron in self.neurons]\n",
    "            for x, y in zip(X, Y):\n",
    "                tot_error+=self.backprop_binary_input(x, y)\n",
    "                tot_grad = [(grad[0] + neuron.w_grad, grad[1] + neuron.b_grad) \n",
    "                            for grad, neuron in zip(tot_grad, self.neurons)]\n",
    "            tot_error = tot_error/n\n",
    "            for grad, neuron in zip(tot_grad, self.neurons):\n",
    "                neuron.weights -= grad[0] / n * learning_rate\n",
    "                neuron.b -= grad[1] / n * learning_rate\n",
    "            if print_logs:\n",
    "                print(f\"Epoch: {k}, mean loss: {tot_error}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def evaluate_model_regre(model, X, Y):\n",
    "    Y_pred = [model.predict_regre(x) for x in X]\n",
    "    MSE = sum([(y_pred - y)**2 for y_pred, y in zip(Y_pred,Y)]) / len(Y)\n",
    "    return math.sqrt(MSE)\n",
    "\n",
    "\n",
    "def evaluate_model_class(model, X, Y):\n",
    "    Y_pred = [model.predict_regre(x)  for x in X]\n",
    "    MSE = sum([(y_pred - y-1)**2 for y_pred, y in zip(Y_pred,Y)]) / len(Y)\n",
    "    return math.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet(hidden_layers=3,number_of_neurons_in_layer=3,input_dim=3)\n",
    "\n",
    "\n",
    "x = [2,3,6]\n",
    "y = 10\n",
    "net.train_regre([x], [y])\n",
    "net.predict_regre(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a3398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('./projekt1/regression/data.activation.train.100.csv', delimiter=',')\n",
    "test_data = np.genfromtxt('./projekt1/regression/data.activation.test.100.csv', delimiter=',')\n",
    "X_train = train_data[1:, 0]\n",
    "Y_train = train_data[1:, 1]\n",
    "X_test = test_data[1:, 0]\n",
    "Y_test = test_data[1:, 1]\n",
    "\n",
    "net = NeuralNet(hidden_layers=2,number_of_neurons_in_layer=30,input_dim=1)\n",
    "net.train_regre(X_train, Y_train, learning_rate=0.003, epochs=200, print_logs=True)\n",
    "\n",
    "print(f\"RMSE on train data {evaluate_model_regre(net, X_train, Y_train)}\")\n",
    "print(f\"RMSE on test data {evaluate_model_regre(net, X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9248d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ind = np.argsort(X_train)\n",
    "plt.plot(X_test, Y_test, label = 'test')\n",
    "Y_pred = [net.predict_regre(x) for x in X_test]\n",
    "plt.plot(X_test, Y_pred, label = 'pred')\n",
    "plt.plot(X_train[ind], Y_train[ind], label = 'train')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac57271",
   "metadata": {},
   "source": [
    "# Classifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fab11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('./projekt1/classification/data.simple.train.100.csv', delimiter=',')\n",
    "test_data = np.genfromtxt('./projekt1/classification/data.simple.test.100.csv', delimiter=',')\n",
    "X_train = train_data[1:, :2]\n",
    "Y_train = train_data[1:, 2]\n",
    "X_test = test_data[1:, :2]\n",
    "Y_test = test_data[1:, 2]\n",
    "\n",
    "net = NeuralNet(hidden_layers=2,number_of_neurons_in_layer=30,input_dim=2, number_of_outputs = 1)\n",
    "net.train_class(X_train, Y_train, learning_rate=0.003, epochs=200, print_logs=True)\n",
    "\n",
    "print(f\"RMSE on train data {evaluate_model_class(net, X_train, Y_train)}\")\n",
    "print(f\"RMSE on train data {evaluate_model_class(net, X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8f742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b44f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1eeee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
