{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1251484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data.dataloader\n",
    "from neural_net import P1_Net, do_train\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.utils.validation import column_or_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d09939",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = pd.read_csv(\"city_attributes.csv\")\n",
    "df_humidity = pd.read_csv(\"humidity.csv\")\n",
    "df_pressure = pd.read_csv(\"pressure.csv\")\n",
    "df_temper = pd.read_csv(\"temperature.csv\")\n",
    "df_descript = pd.read_csv(\"weather_description.csv\")\n",
    "df_direction = pd.read_csv(\"wind_direction.csv\")\n",
    "df_speed = pd.read_csv(\"wind_speed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca0fa34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774.3513513513514"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([df_humidity.isna().sum().mean(),\n",
    "df_pressure.isna().sum().mean(),\n",
    "df_temper.isna().sum().mean(),\n",
    "df_direction.isna().sum().mean(),\n",
    "df_speed.isna().sum().mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc820297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>Vancouver</th>\n",
       "      <th>Portland</th>\n",
       "      <th>San Francisco</th>\n",
       "      <th>Seattle</th>\n",
       "      <th>Los Angeles</th>\n",
       "      <th>San Diego</th>\n",
       "      <th>Las Vegas</th>\n",
       "      <th>Phoenix</th>\n",
       "      <th>Albuquerque</th>\n",
       "      <th>...</th>\n",
       "      <th>Philadelphia</th>\n",
       "      <th>New York</th>\n",
       "      <th>Montreal</th>\n",
       "      <th>Boston</th>\n",
       "      <th>Beersheba</th>\n",
       "      <th>Tel Aviv District</th>\n",
       "      <th>Eilat</th>\n",
       "      <th>Haifa</th>\n",
       "      <th>Nahariyya</th>\n",
       "      <th>Jerusalem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45253</td>\n",
       "      <td>44458.000000</td>\n",
       "      <td>45252.000000</td>\n",
       "      <td>44460.000000</td>\n",
       "      <td>45250.000000</td>\n",
       "      <td>45250.000000</td>\n",
       "      <td>45252.000000</td>\n",
       "      <td>45252.000000</td>\n",
       "      <td>45250.000000</td>\n",
       "      <td>45252.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>45250.000000</td>\n",
       "      <td>44460.000000</td>\n",
       "      <td>45250.000000</td>\n",
       "      <td>45250.000000</td>\n",
       "      <td>44455.000000</td>\n",
       "      <td>44460.000000</td>\n",
       "      <td>44461.000000</td>\n",
       "      <td>44455.000000</td>\n",
       "      <td>44456.000000</td>\n",
       "      <td>44460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>45253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2012-10-01 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>283.862654</td>\n",
       "      <td>284.992929</td>\n",
       "      <td>288.155821</td>\n",
       "      <td>284.409626</td>\n",
       "      <td>290.846116</td>\n",
       "      <td>290.215044</td>\n",
       "      <td>292.424887</td>\n",
       "      <td>295.493358</td>\n",
       "      <td>285.617856</td>\n",
       "      <td>...</td>\n",
       "      <td>285.374168</td>\n",
       "      <td>285.400406</td>\n",
       "      <td>280.343010</td>\n",
       "      <td>283.779823</td>\n",
       "      <td>291.521986</td>\n",
       "      <td>294.512307</td>\n",
       "      <td>296.497276</td>\n",
       "      <td>295.266398</td>\n",
       "      <td>294.094803</td>\n",
       "      <td>293.184253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.640131</td>\n",
       "      <td>7.452438</td>\n",
       "      <td>5.332862</td>\n",
       "      <td>6.547986</td>\n",
       "      <td>6.460823</td>\n",
       "      <td>5.889992</td>\n",
       "      <td>10.829522</td>\n",
       "      <td>9.916743</td>\n",
       "      <td>9.853484</td>\n",
       "      <td>...</td>\n",
       "      <td>10.242377</td>\n",
       "      <td>10.220932</td>\n",
       "      <td>11.953626</td>\n",
       "      <td>9.802499</td>\n",
       "      <td>7.821815</td>\n",
       "      <td>6.676412</td>\n",
       "      <td>8.852984</td>\n",
       "      <td>6.324566</td>\n",
       "      <td>6.304118</td>\n",
       "      <td>7.093583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>245.150000</td>\n",
       "      <td>262.370000</td>\n",
       "      <td>272.300000</td>\n",
       "      <td>263.780000</td>\n",
       "      <td>266.503667</td>\n",
       "      <td>265.783333</td>\n",
       "      <td>260.561333</td>\n",
       "      <td>266.059000</td>\n",
       "      <td>255.042333</td>\n",
       "      <td>...</td>\n",
       "      <td>250.390000</td>\n",
       "      <td>250.774000</td>\n",
       "      <td>243.300000</td>\n",
       "      <td>249.540000</td>\n",
       "      <td>272.179000</td>\n",
       "      <td>271.049000</td>\n",
       "      <td>271.150000</td>\n",
       "      <td>271.150000</td>\n",
       "      <td>268.682000</td>\n",
       "      <td>272.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>279.160000</td>\n",
       "      <td>279.850000</td>\n",
       "      <td>284.670000</td>\n",
       "      <td>279.830000</td>\n",
       "      <td>286.380000</td>\n",
       "      <td>286.254750</td>\n",
       "      <td>283.920000</td>\n",
       "      <td>287.680000</td>\n",
       "      <td>277.970000</td>\n",
       "      <td>...</td>\n",
       "      <td>277.350636</td>\n",
       "      <td>277.370000</td>\n",
       "      <td>271.971750</td>\n",
       "      <td>276.090000</td>\n",
       "      <td>285.366623</td>\n",
       "      <td>289.450000</td>\n",
       "      <td>289.734000</td>\n",
       "      <td>290.467333</td>\n",
       "      <td>289.881833</td>\n",
       "      <td>287.524279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>283.450000</td>\n",
       "      <td>284.320000</td>\n",
       "      <td>287.610000</td>\n",
       "      <td>283.940000</td>\n",
       "      <td>290.530000</td>\n",
       "      <td>290.118750</td>\n",
       "      <td>292.027486</td>\n",
       "      <td>295.586667</td>\n",
       "      <td>286.120000</td>\n",
       "      <td>...</td>\n",
       "      <td>285.927583</td>\n",
       "      <td>285.870000</td>\n",
       "      <td>281.109000</td>\n",
       "      <td>284.133250</td>\n",
       "      <td>290.932667</td>\n",
       "      <td>294.900000</td>\n",
       "      <td>296.150000</td>\n",
       "      <td>294.820000</td>\n",
       "      <td>294.150000</td>\n",
       "      <td>292.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>288.600785</td>\n",
       "      <td>289.451750</td>\n",
       "      <td>291.015167</td>\n",
       "      <td>288.530000</td>\n",
       "      <td>295.080000</td>\n",
       "      <td>294.107542</td>\n",
       "      <td>300.835000</td>\n",
       "      <td>303.050000</td>\n",
       "      <td>292.835643</td>\n",
       "      <td>...</td>\n",
       "      <td>293.796000</td>\n",
       "      <td>293.760000</td>\n",
       "      <td>290.369583</td>\n",
       "      <td>291.620000</td>\n",
       "      <td>297.270000</td>\n",
       "      <td>299.800000</td>\n",
       "      <td>303.150000</td>\n",
       "      <td>299.660000</td>\n",
       "      <td>298.930000</td>\n",
       "      <td>299.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>312.520000</td>\n",
       "      <td>313.620000</td>\n",
       "      <td>307.300000</td>\n",
       "      <td>315.470000</td>\n",
       "      <td>313.360000</td>\n",
       "      <td>318.640000</td>\n",
       "      <td>321.220000</td>\n",
       "      <td>312.710000</td>\n",
       "      <td>...</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>310.240000</td>\n",
       "      <td>307.880000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>314.820000</td>\n",
       "      <td>320.930000</td>\n",
       "      <td>320.150000</td>\n",
       "      <td>320.930000</td>\n",
       "      <td>313.150000</td>\n",
       "      <td>317.040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime     Vancouver      Portland  San Francisco  \\\n",
       "count                 45253  44458.000000  45252.000000   44460.000000   \n",
       "unique                45253           NaN           NaN            NaN   \n",
       "top     2012-10-01 12:00:00           NaN           NaN            NaN   \n",
       "freq                      1           NaN           NaN            NaN   \n",
       "mean                    NaN    283.862654    284.992929     288.155821   \n",
       "std                     NaN      6.640131      7.452438       5.332862   \n",
       "min                     NaN    245.150000    262.370000     272.300000   \n",
       "25%                     NaN    279.160000    279.850000     284.670000   \n",
       "50%                     NaN    283.450000    284.320000     287.610000   \n",
       "75%                     NaN    288.600785    289.451750     291.015167   \n",
       "max                     NaN    307.000000    312.520000     313.620000   \n",
       "\n",
       "             Seattle   Los Angeles     San Diego     Las Vegas       Phoenix  \\\n",
       "count   45250.000000  45250.000000  45252.000000  45252.000000  45250.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean      284.409626    290.846116    290.215044    292.424887    295.493358   \n",
       "std         6.547986      6.460823      5.889992     10.829522      9.916743   \n",
       "min       263.780000    266.503667    265.783333    260.561333    266.059000   \n",
       "25%       279.830000    286.380000    286.254750    283.920000    287.680000   \n",
       "50%       283.940000    290.530000    290.118750    292.027486    295.586667   \n",
       "75%       288.530000    295.080000    294.107542    300.835000    303.050000   \n",
       "max       307.300000    315.470000    313.360000    318.640000    321.220000   \n",
       "\n",
       "         Albuquerque  ...  Philadelphia      New York      Montreal  \\\n",
       "count   45252.000000  ...  45250.000000  44460.000000  45250.000000   \n",
       "unique           NaN  ...           NaN           NaN           NaN   \n",
       "top              NaN  ...           NaN           NaN           NaN   \n",
       "freq             NaN  ...           NaN           NaN           NaN   \n",
       "mean      285.617856  ...    285.374168    285.400406    280.343010   \n",
       "std         9.853484  ...     10.242377     10.220932     11.953626   \n",
       "min       255.042333  ...    250.390000    250.774000    243.300000   \n",
       "25%       277.970000  ...    277.350636    277.370000    271.971750   \n",
       "50%       286.120000  ...    285.927583    285.870000    281.109000   \n",
       "75%       292.835643  ...    293.796000    293.760000    290.369583   \n",
       "max       312.710000  ...    308.000000    310.240000    307.880000   \n",
       "\n",
       "              Boston     Beersheba  Tel Aviv District         Eilat  \\\n",
       "count   45250.000000  44455.000000       44460.000000  44461.000000   \n",
       "unique           NaN           NaN                NaN           NaN   \n",
       "top              NaN           NaN                NaN           NaN   \n",
       "freq             NaN           NaN                NaN           NaN   \n",
       "mean      283.779823    291.521986         294.512307    296.497276   \n",
       "std         9.802499      7.821815           6.676412      8.852984   \n",
       "min       249.540000    272.179000         271.049000    271.150000   \n",
       "25%       276.090000    285.366623         289.450000    289.734000   \n",
       "50%       284.133250    290.932667         294.900000    296.150000   \n",
       "75%       291.620000    297.270000         299.800000    303.150000   \n",
       "max       308.000000    314.820000         320.930000    320.150000   \n",
       "\n",
       "               Haifa     Nahariyya     Jerusalem  \n",
       "count   44455.000000  44456.000000  44460.000000  \n",
       "unique           NaN           NaN           NaN  \n",
       "top              NaN           NaN           NaN  \n",
       "freq             NaN           NaN           NaN  \n",
       "mean      295.266398    294.094803    293.184253  \n",
       "std         6.324566      6.304118      7.093583  \n",
       "min       271.150000    268.682000    272.974000  \n",
       "25%       290.467333    289.881833    287.524279  \n",
       "50%       294.820000    294.150000    292.996000  \n",
       "75%       299.660000    298.930000    299.150000  \n",
       "max       320.930000    313.150000    317.040000  \n",
       "\n",
       "[11 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temper.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87760b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_days(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"datetime\"]).dt.date\n",
    "    df = df.drop('datetime', axis=1)\n",
    "    df = df.groupby(df[\"date\"]).mean()\n",
    "    return df\n",
    "\n",
    "def group_by_days_descript(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"datetime\"]).dt.date\n",
    "    df = df.fillna('no data')\n",
    "    df = df.drop('datetime', axis=1)\n",
    "    df = df.groupby(df[\"date\"]).agg(lambda x: pd.Series.mode(x)[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b1aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_humidity = group_by_days(df_humidity)\n",
    "df_pressure = group_by_days(df_pressure)\n",
    "df_temper = group_by_days(df_temper)\n",
    "df_descript = group_by_days_descript(df_descript)\n",
    "df_direction = group_by_days(df_direction)\n",
    "df_speed = group_by_days(df_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b104f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df_descript= df_descript.apply(encoder.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a5d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_for_city(city_name):\n",
    "    df =  pd.concat([\n",
    "        df_humidity[city_name], df_pressure[city_name], df_temper[city_name], df_descript[city_name], df_direction[city_name], df_speed[city_name]\n",
    "    ], axis=1)\n",
    "    df.columns = ['humidity', 'pressure', 'temperature', 'description', 'wind_direction','wind_speed' ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d505fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>humidity</th>\n",
       "      <th>pressure</th>\n",
       "      <th>temperature</th>\n",
       "      <th>description</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-10-01</th>\n",
       "      <td>78.727273</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>282.118197</td>\n",
       "      <td>11</td>\n",
       "      <td>57.727273</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-02</th>\n",
       "      <td>65.833333</td>\n",
       "      <td>1023.583333</td>\n",
       "      <td>286.137728</td>\n",
       "      <td>11</td>\n",
       "      <td>214.041667</td>\n",
       "      <td>1.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-03</th>\n",
       "      <td>66.208333</td>\n",
       "      <td>1021.083333</td>\n",
       "      <td>289.599792</td>\n",
       "      <td>2</td>\n",
       "      <td>228.333333</td>\n",
       "      <td>2.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-04</th>\n",
       "      <td>51.166667</td>\n",
       "      <td>1022.875000</td>\n",
       "      <td>286.482500</td>\n",
       "      <td>12</td>\n",
       "      <td>206.750000</td>\n",
       "      <td>4.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-05</th>\n",
       "      <td>40.391304</td>\n",
       "      <td>1022.916667</td>\n",
       "      <td>288.286042</td>\n",
       "      <td>12</td>\n",
       "      <td>182.250000</td>\n",
       "      <td>3.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26</th>\n",
       "      <td>87.625000</td>\n",
       "      <td>1004.500000</td>\n",
       "      <td>281.588333</td>\n",
       "      <td>8</td>\n",
       "      <td>153.750000</td>\n",
       "      <td>4.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>92.875000</td>\n",
       "      <td>1015.625000</td>\n",
       "      <td>279.922500</td>\n",
       "      <td>8</td>\n",
       "      <td>158.083333</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28</th>\n",
       "      <td>88.458333</td>\n",
       "      <td>1019.791667</td>\n",
       "      <td>280.085833</td>\n",
       "      <td>8</td>\n",
       "      <td>144.166667</td>\n",
       "      <td>3.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29</th>\n",
       "      <td>85.500000</td>\n",
       "      <td>1027.625000</td>\n",
       "      <td>279.860833</td>\n",
       "      <td>8</td>\n",
       "      <td>191.125000</td>\n",
       "      <td>2.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>76.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>282.280000</td>\n",
       "      <td>0</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1887 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             humidity     pressure  temperature  description  wind_direction  \\\n",
       "date                                                                           \n",
       "2012-10-01  78.727273  1024.000000   282.118197           11       57.727273   \n",
       "2012-10-02  65.833333  1023.583333   286.137728           11      214.041667   \n",
       "2012-10-03  66.208333  1021.083333   289.599792            2      228.333333   \n",
       "2012-10-04  51.166667  1022.875000   286.482500           12      206.750000   \n",
       "2012-10-05  40.391304  1022.916667   288.286042           12      182.250000   \n",
       "...               ...          ...          ...          ...             ...   \n",
       "2017-11-26  87.625000  1004.500000   281.588333            8      153.750000   \n",
       "2017-11-27  92.875000  1015.625000   279.922500            8      158.083333   \n",
       "2017-11-28  88.458333  1019.791667   280.085833            8      144.166667   \n",
       "2017-11-29  85.500000  1027.625000   279.860833            8      191.125000   \n",
       "2017-11-30  76.000000  1029.000000   282.280000            0      340.000000   \n",
       "\n",
       "            wind_speed  \n",
       "date                    \n",
       "2012-10-01    0.000000  \n",
       "2012-10-02    1.291667  \n",
       "2012-10-03    2.625000  \n",
       "2012-10-04    4.625000  \n",
       "2012-10-05    3.708333  \n",
       "...                ...  \n",
       "2017-11-26    4.958333  \n",
       "2017-11-27    1.666667  \n",
       "2017-11-28    3.041667  \n",
       "2017-11-29    2.166667  \n",
       "2017-11-30    1.000000  \n",
       "\n",
       "[1887 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Portland  =  create_dataframe_for_city(\"Portland\")\n",
    "df_Portland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92385848",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speed_2 = pd.read_csv(\"wind_speed.csv\")\n",
    "df_speed_2[\"date\"] = pd.to_datetime(df_speed_2[\"datetime\"]).dt.date\n",
    "df_speed_2 = df_speed_2.drop(columns=['datetime'])\n",
    "df_speed_2 = df_speed_2.groupby('date').apply(lambda group: (group >= 6).any())\n",
    "strong_wind = pd.concat([\n",
    "    df_speed_2[city] for city in df_city['City']\n",
    "])\n",
    "\n",
    "def add_city_col(df, city):\n",
    "    df['city'] = city\n",
    "    return df\n",
    "all_cities_df = pd.concat([\n",
    "    add_city_col(create_dataframe_for_city(city), city) for city in df_city['City']\n",
    "])\n",
    "all_cities_df['strong_wind'] = strong_wind\n",
    "all_cities_df['strong_wind'] = all_cities_df['strong_wind'].apply(lambda x: 1 if x else 0)\n",
    "all_cities_df = all_cities_df.dropna().sort_values('date', kind='stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce9bd74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>humidity</th>\n",
       "      <th>pressure</th>\n",
       "      <th>temperature</th>\n",
       "      <th>description</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>city</th>\n",
       "      <th>strong_wind</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-10-01</th>\n",
       "      <td>78.727273</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>282.118197</td>\n",
       "      <td>11</td>\n",
       "      <td>57.727273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Portland</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-01</th>\n",
       "      <td>83.000000</td>\n",
       "      <td>1009.727273</td>\n",
       "      <td>289.416642</td>\n",
       "      <td>14</td>\n",
       "      <td>122.363636</td>\n",
       "      <td>1.636364</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-01</th>\n",
       "      <td>78.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>281.767262</td>\n",
       "      <td>1</td>\n",
       "      <td>32.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-01</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>291.846501</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-01</th>\n",
       "      <td>79.909091</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>291.573495</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>1025.000000</td>\n",
       "      <td>279.190000</td>\n",
       "      <td>12</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>274.510000</td>\n",
       "      <td>17</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>283.420000</td>\n",
       "      <td>11</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>271.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>1023.000000</td>\n",
       "      <td>280.650000</td>\n",
       "      <td>0</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>Boston</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67538 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             humidity     pressure  temperature  description  wind_direction  \\\n",
       "date                                                                           \n",
       "2012-10-01  78.727273  1024.000000   282.118197           11       57.727273   \n",
       "2012-10-01  83.000000  1009.727273   289.416642           14      122.363636   \n",
       "2012-10-01  78.000000  1030.000000   281.767262            1       32.272727   \n",
       "2012-10-01  88.000000  1013.000000   291.846501           11        0.000000   \n",
       "2012-10-01  79.909091  1013.000000   291.573495           11        0.000000   \n",
       "...               ...          ...          ...          ...             ...   \n",
       "2017-11-30  42.000000  1025.000000   279.190000           12      360.000000   \n",
       "2017-11-30  60.000000  1027.000000   274.510000           17      330.000000   \n",
       "2017-11-30  32.000000  1024.000000   283.420000           11      360.000000   \n",
       "2017-11-30  58.000000  1027.000000   271.800000            1      300.000000   \n",
       "2017-11-30  56.000000  1023.000000   280.650000            0      320.000000   \n",
       "\n",
       "            wind_speed           city  strong_wind  \n",
       "date                                                \n",
       "2012-10-01    0.000000       Portland            0  \n",
       "2012-10-01    1.636364  San Francisco            0  \n",
       "2012-10-01    0.000000        Seattle            0  \n",
       "2012-10-01    0.000000    Los Angeles            0  \n",
       "2012-10-01    0.000000      San Diego            0  \n",
       "...                ...            ...          ...  \n",
       "2017-11-30    2.000000     Pittsburgh            0  \n",
       "2017-11-30    3.000000        Toronto            0  \n",
       "2017-11-30    4.000000   Philadelphia            0  \n",
       "2017-11-30    4.000000       Montreal            0  \n",
       "2017-11-30    2.000000         Boston            0  \n",
       "\n",
       "[67538 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab310f",
   "metadata": {},
   "source": [
    "# Przewidywanie temperatury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3e1ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_x_y_temperature(df):\n",
    "    X = df.iloc[:-2].copy()\n",
    "    y =  df['temperature'].iloc[4:] \n",
    "\n",
    "    X.reset_index(inplace= True)\n",
    "    X = X.drop(columns = ['strong_wind'])\n",
    "    X['day_of_year'] = pd.to_datetime(X['date']).dt.dayofyear\n",
    "    X['month'] = pd.to_datetime(X['date']).dt.month\n",
    "\n",
    "    # X['wind_dir_sin'] = np.sin(X['wind_direction']/360 * 2 * np.pi)\n",
    "    # X['wind_dir_cos'] = np.cos(X['wind_direction']/360 * 2 * np.pi)\n",
    "    # X['day_sin'] = np.sin(X['day_of_year']/365 * 2 * np.pi)\n",
    "    # X['day_cos'] = np.cos(X['day_of_year']/365 * 2 * np.pi)\n",
    "    # X = X.drop(columns = ['day_of_year'])\n",
    "    # X = X.drop(columns = ['wind_direction'])\n",
    "\n",
    "    column_names = X.columns.to_list()\n",
    "    X = pd.concat([X.iloc[:-2].reset_index(drop=True),X.iloc[1:-1].reset_index(drop=True),X.iloc[2:].reset_index(drop=True)],axis = 1)\n",
    "    X.columns = column_names + [c + '_1' for c in column_names] + [c + '_2' for c in column_names]\n",
    "    if 'city_1' in X.columns:\n",
    "        X = X.drop(columns=['city_1', 'city_2'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc08d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def gen_col_set(col_names):\n",
    "    return [ final_col\n",
    "        for col_name in col_names\n",
    "        for final_col in  [col_name, col_name+\"_1\", col_name+\"_2\"] ]\n",
    "\n",
    "def get_col_transformer_temp(df):\n",
    "    return ColumnTransformer(\n",
    "        [\n",
    "            (\n",
    "                \"standarizer\",\n",
    "                StandardScaler(),\n",
    "                gen_col_set(\n",
    "                    [\"humidity\", \"pressure\", \"temperature\", \"wind_speed\", \"wind_direction\", \"day_of_year\"]\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"one_hot_encoder\", \n",
    "                OneHotEncoder(), \n",
    "                gen_col_set(\n",
    "                    [\"description\", \"month\"]\n",
    "                ) + (['city'] if 'city' in df.columns else [])\n",
    "            ),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "def prepare_temperature_data(df):\n",
    "    groups = df.groupby('city')[df.columns].apply(extract_x_y_temperature)\n",
    "    X_list = [group[0] for group in groups]\n",
    "    y_list = [group[1] for group in groups]\n",
    "    X, y = pd.concat(X_list), pd.concat(y_list)\n",
    "    X['y'] = y.reset_index(drop=True)\n",
    "    X = X.sort_values('date',kind='stable')\n",
    "    y = X['y']\n",
    "    X = X.drop(columns=['date','date_1','date_2', 'y'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size = 0.3, random_state = 0,shuffle=False)\n",
    "    ct = get_col_transformer_temp(X_train)\n",
    "    X_train = ct.fit_transform(X_train)\n",
    "    X_test = ct.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db211bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47175, 150)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_temperature_data(all_cities_df)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "952b1e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1_Net(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=150, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "[1] loss: 80266.916\n",
      "[2] loss: 64860.946\n",
      "[3] loss: 13887.196\n",
      "[4] loss: 869.173\n",
      "[5] loss: 590.549\n",
      "[6] loss: 473.474\n",
      "[7] loss: 398.881\n",
      "[8] loss: 343.455\n",
      "[9] loss: 299.562\n",
      "[10] loss: 263.811\n",
      "[11] loss: 234.247\n",
      "[12] loss: 209.460\n",
      "[13] loss: 188.449\n",
      "[14] loss: 170.477\n",
      "[15] loss: 155.020\n",
      "[16] loss: 141.621\n",
      "[17] loss: 129.922\n",
      "[18] loss: 119.646\n",
      "[19] loss: 110.584\n",
      "[20] loss: 102.534\n",
      "[21] loss: 95.356\n",
      "[22] loss: 88.912\n",
      "[23] loss: 83.107\n",
      "[24] loss: 77.853\n",
      "[25] loss: 73.084\n",
      "[26] loss: 68.748\n",
      "[27] loss: 64.807\n",
      "[28] loss: 61.229\n",
      "[29] loss: 58.001\n",
      "[30] loss: 55.101\n",
      "[31] loss: 52.500\n",
      "[32] loss: 50.199\n",
      "[33] loss: 48.176\n",
      "[34] loss: 46.428\n",
      "[35] loss: 44.953\n",
      "[36] loss: 43.743\n",
      "[37] loss: 42.807\n",
      "[38] loss: 42.150\n",
      "[39] loss: 41.788\n",
      "[40] loss: 41.743\n",
      "[41] loss: 42.031\n",
      "[42] loss: 42.675\n",
      "[43] loss: 43.690\n",
      "[44] loss: 45.058\n",
      "[45] loss: 46.757\n",
      "[46] loss: 48.754\n",
      "[47] loss: 51.019\n",
      "[48] loss: 53.487\n",
      "[49] loss: 55.953\n",
      "[50] loss: 57.967\n",
      "[51] loss: 58.822\n",
      "[52] loss: 57.920\n",
      "[53] loss: 55.317\n",
      "[54] loss: 51.797\n",
      "[55] loss: 48.390\n",
      "[56] loss: 45.833\n",
      "[57] loss: 44.451\n",
      "[58] loss: 44.347\n",
      "[59] loss: 45.589\n",
      "[60] loss: 48.320\n",
      "[61] loss: 52.566\n",
      "[62] loss: 57.576\n",
      "[63] loss: 61.049\n",
      "[64] loss: 60.100\n",
      "[65] loss: 54.343\n",
      "[66] loss: 46.653\n",
      "[67] loss: 40.149\n",
      "[68] loss: 36.050\n",
      "[69] loss: 34.140\n",
      "[70] loss: 33.791\n",
      "[71] loss: 34.473\n",
      "[72] loss: 35.989\n",
      "[73] loss: 39.115\n",
      "[74] loss: 47.289\n",
      "[75] loss: 67.123\n",
      "[76] loss: 92.336\n",
      "[77] loss: 89.208\n",
      "[78] loss: 59.383\n",
      "[79] loss: 37.017\n",
      "[80] loss: 29.927\n",
      "[81] loss: 32.041\n",
      "[82] loss: 38.989\n",
      "[83] loss: 47.805\n",
      "[84] loss: 54.166\n",
      "[85] loss: 53.596\n",
      "[86] loss: 46.365\n",
      "[87] loss: 37.357\n",
      "[88] loss: 30.595\n",
      "[89] loss: 26.983\n",
      "[90] loss: 25.937\n",
      "[91] loss: 27.406\n",
      "[92] loss: 34.450\n",
      "[93] loss: 56.850\n",
      "[94] loss: 94.379\n",
      "[95] loss: 98.043\n",
      "[96] loss: 62.843\n",
      "[97] loss: 36.328\n",
      "[98] loss: 26.649\n",
      "[99] loss: 24.975\n",
      "[100] loss: 26.298\n",
      "[101] loss: 28.735\n",
      "[102] loss: 31.407\n",
      "[103] loss: 33.563\n",
      "[104] loss: 34.547\n",
      "[105] loss: 34.101\n",
      "[106] loss: 32.658\n",
      "[107] loss: 31.896\n",
      "[108] loss: 37.277\n",
      "[109] loss: 61.553\n",
      "[110] loss: 98.268\n",
      "[111] loss: 91.472\n",
      "[112] loss: 54.365\n",
      "[113] loss: 32.262\n",
      "[114] loss: 25.595\n",
      "[115] loss: 25.944\n",
      "[116] loss: 29.349\n",
      "[117] loss: 34.133\n",
      "[118] loss: 38.982\n",
      "[119] loss: 42.478\n",
      "[120] loss: 43.668\n",
      "[121] loss: 42.576\n",
      "[122] loss: 39.928\n",
      "[123] loss: 36.602\n",
      "[124] loss: 33.277\n",
      "[125] loss: 30.335\n",
      "[126] loss: 28.072\n",
      "[127] loss: 27.661\n",
      "[128] loss: 34.474\n",
      "[129] loss: 62.006\n",
      "[130] loss: 99.722\n",
      "[131] loss: 88.470\n",
      "[132] loss: 50.714\n",
      "[133] loss: 30.143\n",
      "[134] loss: 23.750\n",
      "[135] loss: 23.139\n",
      "[136] loss: 24.695\n",
      "[137] loss: 26.952\n",
      "[138] loss: 28.958\n",
      "[139] loss: 29.881\n",
      "[140] loss: 29.429\n",
      "[141] loss: 28.963\n",
      "[142] loss: 34.219\n",
      "[143] loss: 57.300\n",
      "[144] loss: 88.185\n",
      "[145] loss: 79.044\n",
      "[146] loss: 48.264\n",
      "[147] loss: 30.688\n",
      "[148] loss: 24.828\n",
      "[149] loss: 24.567\n",
      "[150] loss: 27.068\n",
      "[151] loss: 30.753\n",
      "[152] loss: 34.129\n",
      "[153] loss: 35.858\n",
      "[154] loss: 35.470\n",
      "[155] loss: 33.460\n",
      "[156] loss: 30.784\n",
      "[157] loss: 29.131\n",
      "[158] loss: 33.477\n",
      "[159] loss: 54.384\n",
      "[160] loss: 83.393\n",
      "[161] loss: 76.423\n",
      "[162] loss: 47.491\n",
      "[163] loss: 30.307\n",
      "[164] loss: 24.644\n",
      "[165] loss: 24.762\n",
      "[166] loss: 27.962\n",
      "[167] loss: 32.620\n",
      "[168] loss: 36.979\n",
      "[169] loss: 39.519\n",
      "[170] loss: 39.933\n",
      "[171] loss: 39.002\n",
      "[172] loss: 37.588\n",
      "[173] loss: 36.057\n",
      "[174] loss: 34.383\n",
      "[175] loss: 32.400\n",
      "[176] loss: 30.087\n",
      "[177] loss: 28.202\n",
      "[178] loss: 30.545\n",
      "[179] loss: 48.080\n",
      "[180] loss: 82.004\n",
      "[181] loss: 84.254\n",
      "[182] loss: 52.781\n",
      "[183] loss: 31.231\n",
      "[184] loss: 23.919\n",
      "[185] loss: 23.483\n",
      "[186] loss: 26.424\n",
      "[187] loss: 30.992\n",
      "[188] loss: 35.459\n",
      "[189] loss: 38.164\n",
      "[190] loss: 38.546\n",
      "[191] loss: 37.276\n",
      "[192] loss: 35.284\n",
      "[193] loss: 33.029\n",
      "[194] loss: 30.579\n",
      "[195] loss: 28.119\n",
      "[196] loss: 27.253\n",
      "[197] loss: 34.405\n",
      "[198] loss: 60.106\n",
      "[199] loss: 83.501\n",
      "[200] loss: 66.097\n",
      "[201] loss: 38.911\n",
      "[202] loss: 26.120\n",
      "[203] loss: 22.620\n",
      "[204] loss: 23.346\n",
      "[205] loss: 26.018\n",
      "[206] loss: 28.902\n",
      "[207] loss: 30.269\n",
      "[208] loss: 29.385\n",
      "[209] loss: 27.790\n",
      "[210] loss: 30.605\n",
      "[211] loss: 45.488\n",
      "[212] loss: 63.513\n",
      "[213] loss: 56.855\n",
      "[214] loss: 37.087\n",
      "[215] loss: 25.712\n",
      "[216] loss: 22.189\n",
      "[217] loss: 22.125\n",
      "[218] loss: 23.131\n",
      "[219] loss: 24.435\n",
      "[220] loss: 27.977\n",
      "[221] loss: 36.214\n",
      "[222] loss: 40.935\n",
      "[223] loss: 31.690\n",
      "[224] loss: 20.188\n",
      "[225] loss: 18.708\n",
      "[226] loss: 24.135\n",
      "[227] loss: 31.882\n",
      "[228] loss: 55.401\n",
      "[229] loss: 59.468\n",
      "[230] loss: 27.451\n",
      "[231] loss: 20.477\n",
      "[232] loss: 36.240\n",
      "[233] loss: 44.116\n",
      "[234] loss: 38.198\n",
      "[235] loss: 39.946\n",
      "[236] loss: 49.450\n",
      "[237] loss: 46.477\n",
      "[238] loss: 32.175\n",
      "[239] loss: 22.820\n",
      "[240] loss: 21.176\n",
      "[241] loss: 23.283\n",
      "[242] loss: 25.486\n",
      "[243] loss: 28.222\n",
      "[244] loss: 35.293\n",
      "[245] loss: 38.886\n",
      "[246] loss: 27.834\n",
      "[247] loss: 18.395\n",
      "[248] loss: 25.146\n",
      "[249] loss: 34.996\n",
      "[250] loss: 43.978\n",
      "[251] loss: 66.854\n",
      "[252] loss: 51.908\n",
      "[253] loss: 22.974\n",
      "[254] loss: 22.102\n",
      "[255] loss: 35.904\n",
      "[256] loss: 43.542\n",
      "[257] loss: 39.061\n",
      "[258] loss: 32.698\n",
      "[259] loss: 30.362\n",
      "[260] loss: 32.055\n",
      "[261] loss: 34.755\n",
      "[262] loss: 33.609\n",
      "[263] loss: 27.759\n",
      "[264] loss: 21.997\n",
      "[265] loss: 20.074\n",
      "[266] loss: 21.504\n",
      "[267] loss: 24.506\n",
      "[268] loss: 30.536\n",
      "[269] loss: 32.720\n",
      "[270] loss: 21.879\n",
      "[271] loss: 28.586\n",
      "[272] loss: 52.374\n",
      "[273] loss: 55.560\n",
      "[274] loss: 73.591\n",
      "[275] loss: 58.360\n",
      "[276] loss: 27.393\n",
      "[277] loss: 23.508\n",
      "[278] loss: 35.286\n",
      "[279] loss: 48.781\n",
      "[280] loss: 55.171\n",
      "[281] loss: 56.286\n",
      "[282] loss: 57.318\n",
      "[283] loss: 57.535\n",
      "[284] loss: 53.153\n",
      "[285] loss: 44.494\n",
      "[286] loss: 35.767\n",
      "[287] loss: 29.769\n",
      "[288] loss: 26.548\n",
      "[289] loss: 25.115\n",
      "[290] loss: 24.548\n",
      "[291] loss: 24.231\n",
      "[292] loss: 23.994\n",
      "[293] loss: 24.695\n",
      "[294] loss: 29.372\n",
      "[295] loss: 41.546\n",
      "[296] loss: 53.054\n",
      "[297] loss: 47.556\n",
      "[298] loss: 33.060\n",
      "[299] loss: 24.198\n",
      "[300] loss: 21.611\n",
      "[301] loss: 21.922\n",
      "[302] loss: 22.992\n",
      "[303] loss: 24.473\n",
      "[304] loss: 27.244\n",
      "[305] loss: 29.313\n",
      "[306] loss: 25.296\n",
      "[307] loss: 18.315\n",
      "[308] loss: 18.675\n",
      "[309] loss: 25.141\n",
      "[310] loss: 37.865\n",
      "[311] loss: 55.322\n",
      "[312] loss: 31.899\n",
      "[313] loss: 34.038\n",
      "[314] loss: 74.652\n",
      "[315] loss: 64.399\n",
      "[316] loss: 38.920\n",
      "[317] loss: 28.728\n",
      "[318] loss: 27.116\n",
      "[319] loss: 28.207\n",
      "[320] loss: 29.082\n",
      "[321] loss: 28.597\n",
      "[322] loss: 27.673\n",
      "[323] loss: 29.432\n",
      "[324] loss: 36.944\n",
      "[325] loss: 44.271\n",
      "[326] loss: 39.032\n",
      "[327] loss: 26.814\n",
      "[328] loss: 20.219\n",
      "[329] loss: 20.432\n",
      "[330] loss: 23.039\n",
      "[331] loss: 25.518\n",
      "[332] loss: 30.065\n",
      "[333] loss: 31.493\n",
      "[334] loss: 21.800\n",
      "[335] loss: 20.320\n",
      "[336] loss: 35.722\n",
      "[337] loss: 43.557\n",
      "[338] loss: 62.228\n",
      "[339] loss: 53.644\n",
      "[340] loss: 22.062\n",
      "[341] loss: 26.653\n",
      "[342] loss: 48.310\n",
      "[343] loss: 52.125\n",
      "[344] loss: 41.634\n",
      "[345] loss: 33.939\n",
      "[346] loss: 31.618\n",
      "[347] loss: 32.153\n",
      "[348] loss: 33.173\n",
      "[349] loss: 33.253\n",
      "[350] loss: 32.097\n",
      "[351] loss: 30.153\n",
      "[352] loss: 28.191\n",
      "[353] loss: 27.471\n",
      "[354] loss: 29.925\n",
      "[355] loss: 35.331\n",
      "[356] loss: 36.511\n",
      "[357] loss: 28.676\n",
      "[358] loss: 20.263\n",
      "[359] loss: 18.983\n",
      "[360] loss: 22.698\n",
      "[361] loss: 26.145\n",
      "[362] loss: 32.454\n",
      "[363] loss: 34.365\n",
      "[364] loss: 21.512\n",
      "[365] loss: 31.680\n",
      "[366] loss: 56.926\n",
      "[367] loss: 49.906\n",
      "[368] loss: 48.608\n",
      "[369] loss: 43.123\n",
      "[370] loss: 25.500\n",
      "[371] loss: 20.153\n",
      "[372] loss: 29.593\n",
      "[373] loss: 39.330\n",
      "[374] loss: 38.290\n",
      "[375] loss: 32.130\n",
      "[376] loss: 27.785\n",
      "[377] loss: 25.793\n",
      "[378] loss: 24.456\n",
      "[379] loss: 22.467\n",
      "[380] loss: 20.064\n",
      "[381] loss: 18.894\n",
      "[382] loss: 19.682\n",
      "[383] loss: 21.435\n",
      "[384] loss: 23.319\n",
      "[385] loss: 20.229\n",
      "[386] loss: 26.406\n",
      "[387] loss: 48.420\n",
      "[388] loss: 74.850\n",
      "[389] loss: 73.832\n",
      "[390] loss: 24.146\n",
      "[391] loss: 43.119\n",
      "[392] loss: 74.797\n",
      "[393] loss: 69.283\n",
      "[394] loss: 55.697\n",
      "[395] loss: 48.286\n",
      "[396] loss: 42.135\n",
      "[397] loss: 36.138\n",
      "[398] loss: 31.511\n",
      "[399] loss: 28.619\n",
      "[400] loss: 27.054\n"
     ]
    }
   ],
   "source": [
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_train.toarray().astype('float32')), \n",
    "    torch.from_numpy(y_train.to_numpy().astype('float32')).unsqueeze(1)\n",
    ")\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "net = P1_Net(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(X_train.shape[1], 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256,256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256,256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256,1)\n",
    "    )\n",
    ")\n",
    "print(net)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "do_train(net, data_loader, optimizer, loss, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06724d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* train\n",
      "\t* Mean absolute error: 7.021524060770603\n",
      "\t* Fraction of predictions with absolute error <= 2: 0.13971383147853733\n",
      "* test\n",
      "\t* Mean absolute error: 7.634810577571083\n",
      "\t* Fraction of predictions with absolute error <= 2: 0.12953162866610612\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "net.eval()\n",
    "\n",
    "def valid_in_margin(y1, y2, margin):\n",
    "    return 1 - np.count_nonzero(abs(y1-y2)>margin) / len(y1)\n",
    "print(\"* train\")\n",
    "with torch.no_grad():\n",
    "    y_pred1 = net(torch.from_numpy(X_train.toarray().astype('float32'))).numpy()\n",
    "    y_pred1 = np.squeeze(y_pred1)\n",
    "\n",
    "mae = mean_absolute_error(y_train, y_pred1)\n",
    "print(f\"\\t* Mean absolute error: {mae}\")\n",
    "margin = 2\n",
    "fraction = valid_in_margin(y_pred1, y_train, margin)\n",
    "print(f\"\\t* Fraction of predictions with absolute error <= {margin}: {fraction}\")\n",
    "\n",
    "print(\"* test\")\n",
    "with torch.no_grad():\n",
    "    y_pred2 = net(torch.from_numpy(X_test.toarray().astype('float32'))).numpy()\n",
    "    y_pred2 = np.squeeze(y_pred2)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "print(f\"\\t* Mean absolute error: {mae}\")\n",
    "margin = 2\n",
    "fraction = valid_in_margin(y_pred2, y_test, margin)\n",
    "print(f\"\\t* Fraction of predictions with absolute error <= {margin}: {fraction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0ded6c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1_Net(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=150, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "[1] loss: 80843.623\n",
      "[2] loss: 80572.665\n",
      "[3] loss: 80200.904\n",
      "[4] loss: 79668.698\n",
      "[5] loss: 78937.602\n",
      "[6] loss: 77981.773\n",
      "[7] loss: 76784.161\n",
      "[8] loss: 75351.741\n",
      "[9] loss: 73685.427\n",
      "[10] loss: 71808.183\n",
      "[11] loss: 69725.653\n",
      "[12] loss: 67444.501\n",
      "[13] loss: 64998.326\n",
      "[14] loss: 62395.939\n",
      "[15] loss: 59641.088\n",
      "[16] loss: 56794.902\n",
      "[17] loss: 53853.550\n",
      "[18] loss: 50844.836\n",
      "[19] loss: 47827.565\n",
      "[20] loss: 44815.552\n",
      "[21] loss: 41788.977\n",
      "[22] loss: 38797.033\n",
      "[23] loss: 35824.233\n",
      "[24] loss: 32920.852\n",
      "[25] loss: 30127.258\n",
      "[26] loss: 27392.433\n",
      "[27] loss: 24776.402\n",
      "[28] loss: 22264.303\n",
      "[29] loss: 19917.438\n",
      "[30] loss: 17673.774\n",
      "[31] loss: 15609.204\n",
      "[32] loss: 13691.510\n",
      "[33] loss: 11911.313\n",
      "[34] loss: 10297.284\n",
      "[35] loss: 8842.249\n",
      "[36] loss: 7498.224\n",
      "[37] loss: 6379.175\n",
      "[38] loss: 5335.919\n",
      "[39] loss: 4451.847\n",
      "[40] loss: 3681.826\n",
      "[41] loss: 3033.431\n",
      "[42] loss: 2476.715\n",
      "[43] loss: 2019.636\n",
      "[44] loss: 1651.987\n",
      "[45] loss: 1363.618\n",
      "[46] loss: 1127.579\n",
      "[47] loss: 937.489\n",
      "[48] loss: 803.535\n",
      "[49] loss: 702.367\n",
      "[50] loss: 624.076\n",
      "[51] loss: 568.162\n",
      "[52] loss: 530.612\n",
      "[53] loss: 502.787\n",
      "[54] loss: 479.780\n",
      "[55] loss: 468.062\n",
      "[56] loss: 457.521\n",
      "[57] loss: 451.869\n",
      "[58] loss: 445.329\n",
      "[59] loss: 441.839\n",
      "[60] loss: 442.934\n",
      "[61] loss: 438.147\n",
      "[62] loss: 435.397\n",
      "[63] loss: 432.171\n",
      "[64] loss: 428.687\n",
      "[65] loss: 420.703\n",
      "[66] loss: 426.659\n",
      "[67] loss: 425.448\n",
      "[68] loss: 425.714\n",
      "[69] loss: 420.783\n",
      "[70] loss: 418.990\n",
      "[71] loss: 415.410\n",
      "[72] loss: 417.940\n",
      "[73] loss: 414.922\n",
      "[74] loss: 412.572\n",
      "[75] loss: 410.372\n",
      "[76] loss: 404.061\n",
      "[77] loss: 404.528\n",
      "[78] loss: 407.164\n",
      "[79] loss: 404.296\n",
      "[80] loss: 408.274\n",
      "[81] loss: 409.209\n",
      "[82] loss: 403.593\n",
      "[83] loss: 404.951\n",
      "[84] loss: 407.898\n",
      "[85] loss: 406.495\n",
      "[86] loss: 400.775\n",
      "[87] loss: 395.707\n",
      "[88] loss: 397.947\n",
      "[89] loss: 400.063\n",
      "[90] loss: 399.525\n",
      "[91] loss: 395.735\n",
      "[92] loss: 395.461\n",
      "[93] loss: 396.162\n",
      "[94] loss: 400.276\n",
      "[95] loss: 399.101\n",
      "[96] loss: 396.138\n",
      "[97] loss: 395.850\n",
      "[98] loss: 393.197\n",
      "[99] loss: 396.231\n",
      "[100] loss: 392.388\n",
      "[101] loss: 388.736\n",
      "[102] loss: 389.015\n",
      "[103] loss: 388.757\n",
      "[104] loss: 390.891\n",
      "[105] loss: 390.650\n",
      "[106] loss: 390.753\n",
      "[107] loss: 389.447\n",
      "[108] loss: 387.659\n",
      "[109] loss: 383.468\n",
      "[110] loss: 385.857\n",
      "[111] loss: 385.544\n",
      "[112] loss: 386.341\n",
      "[113] loss: 387.515\n",
      "[114] loss: 384.606\n",
      "[115] loss: 391.917\n",
      "[116] loss: 387.460\n",
      "[117] loss: 384.542\n",
      "[118] loss: 387.910\n",
      "[119] loss: 384.269\n",
      "[120] loss: 385.801\n",
      "[121] loss: 385.537\n",
      "[122] loss: 387.133\n",
      "[123] loss: 388.402\n",
      "[124] loss: 388.775\n",
      "[125] loss: 384.456\n",
      "[126] loss: 387.008\n",
      "[127] loss: 382.330\n",
      "[128] loss: 386.052\n",
      "[129] loss: 383.665\n",
      "[130] loss: 382.584\n",
      "[131] loss: 378.246\n",
      "[132] loss: 381.095\n",
      "[133] loss: 385.169\n",
      "[134] loss: 382.094\n",
      "[135] loss: 381.458\n",
      "[136] loss: 387.192\n",
      "[137] loss: 382.911\n",
      "[138] loss: 380.723\n",
      "[139] loss: 385.086\n",
      "[140] loss: 386.016\n",
      "[141] loss: 379.185\n",
      "[142] loss: 382.624\n",
      "[143] loss: 381.548\n",
      "[144] loss: 381.713\n",
      "[145] loss: 382.168\n",
      "[146] loss: 382.460\n",
      "[147] loss: 377.562\n",
      "[148] loss: 379.765\n",
      "[149] loss: 382.501\n",
      "[150] loss: 377.991\n",
      "[151] loss: 379.258\n",
      "[152] loss: 384.089\n",
      "[153] loss: 379.383\n",
      "[154] loss: 379.680\n",
      "[155] loss: 384.195\n",
      "[156] loss: 374.549\n",
      "[157] loss: 380.364\n",
      "[158] loss: 377.020\n",
      "[159] loss: 376.924\n",
      "[160] loss: 380.484\n",
      "[161] loss: 377.217\n",
      "[162] loss: 380.603\n",
      "[163] loss: 382.209\n",
      "[164] loss: 378.992\n",
      "[165] loss: 376.778\n",
      "[166] loss: 376.833\n",
      "[167] loss: 381.293\n",
      "[168] loss: 377.732\n",
      "[169] loss: 373.397\n",
      "[170] loss: 380.013\n",
      "[171] loss: 375.220\n",
      "[172] loss: 378.240\n",
      "[173] loss: 380.268\n",
      "[174] loss: 373.166\n",
      "[175] loss: 379.081\n",
      "[176] loss: 372.768\n",
      "[177] loss: 378.403\n",
      "[178] loss: 374.194\n",
      "[179] loss: 376.207\n",
      "[180] loss: 376.968\n",
      "[181] loss: 380.510\n",
      "[182] loss: 378.840\n",
      "[183] loss: 376.624\n",
      "[184] loss: 368.631\n",
      "[185] loss: 377.032\n",
      "[186] loss: 378.748\n",
      "[187] loss: 368.425\n",
      "[188] loss: 379.242\n",
      "[189] loss: 380.958\n",
      "[190] loss: 375.607\n",
      "[191] loss: 375.119\n",
      "[192] loss: 378.028\n",
      "[193] loss: 369.949\n",
      "[194] loss: 376.868\n",
      "[195] loss: 372.938\n",
      "[196] loss: 375.064\n",
      "[197] loss: 375.688\n",
      "[198] loss: 374.633\n",
      "[199] loss: 378.553\n",
      "[200] loss: 372.640\n",
      "[201] loss: 371.923\n",
      "[202] loss: 374.913\n",
      "[203] loss: 371.382\n",
      "[204] loss: 375.433\n",
      "[205] loss: 375.818\n",
      "[206] loss: 368.338\n",
      "[207] loss: 375.454\n",
      "[208] loss: 374.014\n",
      "[209] loss: 372.705\n",
      "[210] loss: 371.264\n",
      "[211] loss: 375.431\n",
      "[212] loss: 374.730\n",
      "[213] loss: 373.675\n",
      "[214] loss: 378.458\n",
      "[215] loss: 373.336\n",
      "[216] loss: 373.918\n",
      "[217] loss: 376.396\n",
      "[218] loss: 373.833\n",
      "[219] loss: 374.018\n",
      "[220] loss: 372.859\n",
      "[221] loss: 370.690\n",
      "[222] loss: 373.051\n",
      "[223] loss: 377.282\n",
      "[224] loss: 373.248\n",
      "[225] loss: 374.873\n",
      "[226] loss: 372.992\n",
      "[227] loss: 376.281\n",
      "[228] loss: 372.098\n",
      "[229] loss: 374.198\n",
      "[230] loss: 374.188\n",
      "[231] loss: 371.412\n",
      "[232] loss: 371.711\n",
      "[233] loss: 372.760\n",
      "[234] loss: 372.274\n",
      "[235] loss: 371.659\n",
      "[236] loss: 371.500\n",
      "[237] loss: 371.114\n",
      "[238] loss: 372.405\n",
      "[239] loss: 370.079\n",
      "[240] loss: 372.315\n",
      "[241] loss: 368.746\n",
      "[242] loss: 371.289\n",
      "[243] loss: 367.586\n",
      "[244] loss: 371.342\n",
      "[245] loss: 371.432\n",
      "[246] loss: 372.026\n",
      "[247] loss: 374.298\n",
      "[248] loss: 374.193\n",
      "[249] loss: 367.819\n",
      "[250] loss: 370.706\n",
      "[251] loss: 366.876\n",
      "[252] loss: 369.653\n",
      "[253] loss: 370.429\n",
      "[254] loss: 369.043\n",
      "[255] loss: 369.676\n",
      "[256] loss: 364.759\n",
      "[257] loss: 365.258\n",
      "[258] loss: 366.828\n",
      "[259] loss: 373.648\n",
      "[260] loss: 371.156\n",
      "[261] loss: 371.583\n",
      "[262] loss: 369.049\n",
      "[263] loss: 371.626\n",
      "[264] loss: 371.650\n",
      "[265] loss: 375.288\n",
      "[266] loss: 370.826\n",
      "[267] loss: 367.014\n",
      "[268] loss: 370.112\n",
      "[269] loss: 366.661\n",
      "[270] loss: 370.381\n",
      "[271] loss: 373.281\n",
      "[272] loss: 369.601\n",
      "[273] loss: 376.796\n",
      "[274] loss: 373.767\n",
      "[275] loss: 368.811\n",
      "[276] loss: 368.864\n",
      "[277] loss: 372.369\n",
      "[278] loss: 368.358\n",
      "[279] loss: 369.391\n",
      "[280] loss: 369.989\n",
      "[281] loss: 370.651\n",
      "[282] loss: 366.766\n",
      "[283] loss: 368.886\n",
      "[284] loss: 373.184\n",
      "[285] loss: 367.914\n",
      "[286] loss: 368.658\n",
      "[287] loss: 368.377\n",
      "[288] loss: 369.013\n",
      "[289] loss: 369.715\n",
      "[290] loss: 367.365\n",
      "[291] loss: 368.536\n",
      "[292] loss: 371.711\n",
      "[293] loss: 369.725\n",
      "[294] loss: 366.326\n",
      "[295] loss: 369.905\n",
      "[296] loss: 370.545\n",
      "[297] loss: 366.289\n",
      "[298] loss: 367.870\n",
      "[299] loss: 371.174\n",
      "[300] loss: 370.260\n",
      "[301] loss: 370.980\n",
      "[302] loss: 370.565\n",
      "[303] loss: 364.697\n",
      "[304] loss: 365.910\n",
      "[305] loss: 368.417\n",
      "[306] loss: 363.080\n",
      "[307] loss: 371.975\n",
      "[308] loss: 367.482\n",
      "[309] loss: 371.063\n",
      "[310] loss: 364.123\n",
      "[311] loss: 365.004\n",
      "[312] loss: 367.704\n",
      "[313] loss: 368.754\n",
      "[314] loss: 365.603\n",
      "[315] loss: 363.614\n",
      "[316] loss: 370.261\n",
      "[317] loss: 368.128\n",
      "[318] loss: 367.448\n",
      "[319] loss: 371.073\n",
      "[320] loss: 365.702\n",
      "[321] loss: 366.363\n",
      "[322] loss: 370.944\n",
      "[323] loss: 366.623\n",
      "[324] loss: 370.174\n",
      "[325] loss: 367.224\n",
      "[326] loss: 365.406\n",
      "[327] loss: 369.859\n",
      "[328] loss: 367.495\n",
      "[329] loss: 367.612\n",
      "[330] loss: 363.707\n",
      "[331] loss: 367.270\n",
      "[332] loss: 367.121\n",
      "[333] loss: 367.423\n",
      "[334] loss: 370.809\n",
      "[335] loss: 365.781\n",
      "[336] loss: 366.416\n",
      "[337] loss: 371.593\n",
      "[338] loss: 368.289\n",
      "[339] loss: 369.105\n",
      "[340] loss: 368.378\n",
      "[341] loss: 369.576\n",
      "[342] loss: 367.533\n",
      "[343] loss: 369.617\n",
      "[344] loss: 366.051\n",
      "[345] loss: 365.857\n",
      "[346] loss: 367.719\n",
      "[347] loss: 370.580\n",
      "[348] loss: 364.784\n",
      "[349] loss: 367.515\n",
      "[350] loss: 369.240\n",
      "[351] loss: 368.234\n",
      "[352] loss: 362.721\n",
      "[353] loss: 366.579\n",
      "[354] loss: 365.638\n",
      "[355] loss: 364.948\n",
      "[356] loss: 365.027\n",
      "[357] loss: 368.770\n",
      "[358] loss: 363.488\n",
      "[359] loss: 366.394\n",
      "[360] loss: 368.468\n",
      "[361] loss: 369.750\n",
      "[362] loss: 369.724\n",
      "[363] loss: 363.132\n",
      "[364] loss: 371.886\n",
      "[365] loss: 368.852\n",
      "[366] loss: 363.564\n",
      "[367] loss: 363.276\n",
      "[368] loss: 365.794\n",
      "[369] loss: 370.998\n",
      "[370] loss: 364.036\n",
      "[371] loss: 368.139\n",
      "[372] loss: 368.708\n",
      "[373] loss: 362.023\n",
      "[374] loss: 366.427\n",
      "[375] loss: 368.646\n",
      "[376] loss: 365.886\n",
      "[377] loss: 368.010\n",
      "[378] loss: 367.377\n",
      "[379] loss: 368.087\n",
      "[380] loss: 366.218\n",
      "[381] loss: 368.275\n",
      "[382] loss: 370.703\n",
      "[383] loss: 366.519\n",
      "[384] loss: 366.404\n",
      "[385] loss: 367.645\n",
      "[386] loss: 364.055\n",
      "[387] loss: 371.258\n",
      "[388] loss: 368.981\n",
      "[389] loss: 366.113\n",
      "[390] loss: 368.621\n",
      "[391] loss: 367.192\n",
      "[392] loss: 362.678\n",
      "[393] loss: 369.105\n",
      "[394] loss: 364.700\n",
      "[395] loss: 368.669\n",
      "[396] loss: 367.139\n",
      "[397] loss: 364.304\n",
      "[398] loss: 368.941\n",
      "[399] loss: 362.446\n",
      "[400] loss: 363.274\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_temperature_data(all_cities_df)\n",
    "X_train.shape\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_train.toarray().astype('float32')), \n",
    "    torch.from_numpy(y_train.to_numpy().astype('float32')).unsqueeze(1)\n",
    ")\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "net = P1_Net(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(X_train.shape[1], 256),\n",
    "        nn.ReLU(),\n",
    "         nn.Dropout(p=0.5),\n",
    "        nn.Linear(256,1)\n",
    "    )\n",
    ")\n",
    "print(net)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "do_train(net, data_loader, optimizer, loss, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cd52af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* train\n",
      "\t* Mean absolute error: 3.1771367505403685\n",
      "\t* Fraction of predictions with absolute error <= 2: 0.3767037625861155\n",
      "* test\n",
      "\t* Mean absolute error: 3.6946716206128265\n",
      "\t* Fraction of predictions with absolute error <= 2: 0.30426826252534744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "net.eval()\n",
    "\n",
    "def valid_in_margin(y1, y2, margin):\n",
    "    return 1 - np.count_nonzero(abs(y1-y2)>margin) / len(y1)\n",
    "print(\"* train\")\n",
    "with torch.no_grad():\n",
    "    y_pred1 = net(torch.from_numpy(X_train.toarray().astype('float32'))).numpy()\n",
    "    y_pred1 = np.squeeze(y_pred1)\n",
    "\n",
    "mae = mean_absolute_error(y_train, y_pred1)\n",
    "print(f\"\\t* Mean absolute error: {mae}\")\n",
    "margin = 2\n",
    "fraction = valid_in_margin(y_pred1, y_train, margin)\n",
    "print(f\"\\t* Fraction of predictions with absolute error <= {margin}: {fraction}\")\n",
    "\n",
    "print(\"* test\")\n",
    "with torch.no_grad():\n",
    "    y_pred2 = net(torch.from_numpy(X_test.toarray().astype('float32'))).numpy()\n",
    "    y_pred2 = np.squeeze(y_pred2)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "print(f\"\\t* Mean absolute error: {mae}\")\n",
    "margin = 2\n",
    "fraction = valid_in_margin(y_pred2, y_test, margin)\n",
    "print(f\"\\t* Fraction of predictions with absolute error <= {margin}: {fraction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d55df70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1_Net(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=150, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "[1] loss: 80897.110\n",
      "[2] loss: 78802.257\n",
      "[3] loss: 66599.003\n",
      "[4] loss: 34937.756\n",
      "[5] loss: 5594.957\n",
      "[6] loss: 456.685\n",
      "[7] loss: 326.158\n",
      "[8] loss: 270.970\n",
      "[9] loss: 235.794\n",
      "[10] loss: 209.604\n",
      "[11] loss: 188.562\n",
      "[12] loss: 170.939\n",
      "[13] loss: 155.824\n",
      "[14] loss: 142.667\n",
      "[15] loss: 131.101\n",
      "[16] loss: 120.856\n",
      "[17] loss: 111.721\n",
      "[18] loss: 103.547\n",
      "[19] loss: 96.195\n",
      "[20] loss: 89.571\n",
      "[21] loss: 83.589\n",
      "[22] loss: 78.188\n",
      "[23] loss: 73.295\n",
      "[24] loss: 68.856\n",
      "[25] loss: 64.828\n",
      "[26] loss: 61.174\n",
      "[27] loss: 57.860\n",
      "[28] loss: 54.852\n",
      "[29] loss: 52.123\n",
      "[30] loss: 49.645\n",
      "[31] loss: 47.389\n",
      "[32] loss: 45.335\n",
      "[33] loss: 43.466\n",
      "[34] loss: 41.757\n",
      "[35] loss: 40.190\n",
      "[36] loss: 38.750\n",
      "[37] loss: 37.423\n",
      "[38] loss: 36.197\n",
      "[39] loss: 35.059\n",
      "[40] loss: 34.000\n",
      "[41] loss: 33.009\n",
      "[42] loss: 32.082\n",
      "[43] loss: 31.211\n",
      "[44] loss: 30.392\n",
      "[45] loss: 29.624\n",
      "[46] loss: 28.906\n",
      "[47] loss: 28.242\n",
      "[48] loss: 27.630\n",
      "[49] loss: 27.072\n",
      "[50] loss: 26.566\n",
      "[51] loss: 26.113\n",
      "[52] loss: 25.716\n",
      "[53] loss: 25.372\n",
      "[54] loss: 25.078\n",
      "[55] loss: 24.834\n",
      "[56] loss: 24.637\n",
      "[57] loss: 24.484\n",
      "[58] loss: 24.373\n",
      "[59] loss: 24.297\n",
      "[60] loss: 24.251\n",
      "[61] loss: 24.235\n",
      "[62] loss: 24.238\n",
      "[63] loss: 24.252\n",
      "[64] loss: 24.276\n",
      "[65] loss: 24.306\n",
      "[66] loss: 24.336\n",
      "[67] loss: 24.359\n",
      "[68] loss: 24.370\n",
      "[69] loss: 24.370\n",
      "[70] loss: 24.363\n",
      "[71] loss: 24.345\n",
      "[72] loss: 24.319\n",
      "[73] loss: 24.284\n",
      "[74] loss: 24.244\n",
      "[75] loss: 24.200\n",
      "[76] loss: 24.153\n",
      "[77] loss: 24.106\n",
      "[78] loss: 24.057\n",
      "[79] loss: 24.006\n",
      "[80] loss: 23.952\n",
      "[81] loss: 23.898\n",
      "[82] loss: 23.847\n",
      "[83] loss: 23.795\n",
      "[84] loss: 23.745\n",
      "[85] loss: 23.695\n",
      "[86] loss: 23.646\n",
      "[87] loss: 23.599\n",
      "[88] loss: 23.553\n",
      "[89] loss: 23.511\n",
      "[90] loss: 23.471\n",
      "[91] loss: 23.430\n",
      "[92] loss: 23.389\n",
      "[93] loss: 23.350\n",
      "[94] loss: 23.312\n",
      "[95] loss: 23.279\n",
      "[96] loss: 23.248\n",
      "[97] loss: 23.217\n",
      "[98] loss: 23.186\n",
      "[99] loss: 23.156\n",
      "[100] loss: 23.127\n",
      "[101] loss: 23.099\n",
      "[102] loss: 23.072\n",
      "[103] loss: 23.046\n",
      "[104] loss: 23.022\n",
      "[105] loss: 22.999\n",
      "[106] loss: 22.977\n",
      "[107] loss: 22.955\n",
      "[108] loss: 22.933\n",
      "[109] loss: 22.912\n",
      "[110] loss: 22.891\n",
      "[111] loss: 22.871\n",
      "[112] loss: 22.852\n",
      "[113] loss: 22.833\n",
      "[114] loss: 22.815\n",
      "[115] loss: 22.797\n",
      "[116] loss: 22.779\n",
      "[117] loss: 22.761\n",
      "[118] loss: 22.746\n",
      "[119] loss: 22.731\n",
      "[120] loss: 22.716\n",
      "[121] loss: 22.702\n",
      "[122] loss: 22.686\n",
      "[123] loss: 22.671\n",
      "[124] loss: 22.657\n",
      "[125] loss: 22.645\n",
      "[126] loss: 22.633\n",
      "[127] loss: 22.620\n",
      "[128] loss: 22.606\n",
      "[129] loss: 22.591\n",
      "[130] loss: 22.577\n",
      "[131] loss: 22.564\n",
      "[132] loss: 22.552\n",
      "[133] loss: 22.541\n",
      "[134] loss: 22.531\n",
      "[135] loss: 22.520\n",
      "[136] loss: 22.508\n",
      "[137] loss: 22.495\n",
      "[138] loss: 22.483\n",
      "[139] loss: 22.473\n",
      "[140] loss: 22.463\n",
      "[141] loss: 22.454\n",
      "[142] loss: 22.443\n",
      "[143] loss: 22.432\n",
      "[144] loss: 22.420\n",
      "[145] loss: 22.408\n",
      "[146] loss: 22.397\n",
      "[147] loss: 22.388\n",
      "[148] loss: 22.382\n",
      "[149] loss: 22.375\n",
      "[150] loss: 22.367\n",
      "[151] loss: 22.358\n",
      "[152] loss: 22.348\n",
      "[153] loss: 22.340\n",
      "[154] loss: 22.332\n",
      "[155] loss: 22.324\n",
      "[156] loss: 22.318\n",
      "[157] loss: 22.308\n",
      "[158] loss: 22.300\n",
      "[159] loss: 22.292\n",
      "[160] loss: 22.283\n",
      "[161] loss: 22.276\n",
      "[162] loss: 22.270\n",
      "[163] loss: 22.263\n",
      "[164] loss: 22.256\n",
      "[165] loss: 22.249\n",
      "[166] loss: 22.241\n",
      "[167] loss: 22.232\n",
      "[168] loss: 22.226\n",
      "[169] loss: 22.218\n",
      "[170] loss: 22.211\n",
      "[171] loss: 22.204\n",
      "[172] loss: 22.197\n",
      "[173] loss: 22.190\n",
      "[174] loss: 22.183\n",
      "[175] loss: 22.175\n",
      "[176] loss: 22.169\n",
      "[177] loss: 22.162\n",
      "[178] loss: 22.157\n",
      "[179] loss: 22.149\n",
      "[180] loss: 22.143\n",
      "[181] loss: 22.137\n",
      "[182] loss: 22.130\n",
      "[183] loss: 22.125\n",
      "[184] loss: 22.120\n",
      "[185] loss: 22.114\n",
      "[186] loss: 22.106\n",
      "[187] loss: 22.099\n",
      "[188] loss: 22.093\n",
      "[189] loss: 22.086\n",
      "[190] loss: 22.081\n",
      "[191] loss: 22.073\n",
      "[192] loss: 22.065\n",
      "[193] loss: 22.058\n",
      "[194] loss: 22.051\n",
      "[195] loss: 22.046\n",
      "[196] loss: 22.040\n",
      "[197] loss: 22.036\n",
      "[198] loss: 22.029\n",
      "[199] loss: 22.022\n",
      "[200] loss: 22.016\n",
      "[201] loss: 22.008\n",
      "[202] loss: 22.003\n",
      "[203] loss: 21.997\n",
      "[204] loss: 21.992\n",
      "[205] loss: 21.983\n",
      "[206] loss: 21.977\n",
      "[207] loss: 21.969\n",
      "[208] loss: 21.961\n",
      "[209] loss: 21.955\n",
      "[210] loss: 21.951\n",
      "[211] loss: 21.945\n",
      "[212] loss: 21.940\n",
      "[213] loss: 21.933\n",
      "[214] loss: 21.928\n",
      "[215] loss: 21.922\n",
      "[216] loss: 21.916\n",
      "[217] loss: 21.909\n",
      "[218] loss: 21.904\n",
      "[219] loss: 21.899\n",
      "[220] loss: 21.893\n",
      "[221] loss: 21.887\n",
      "[222] loss: 21.881\n",
      "[223] loss: 21.876\n",
      "[224] loss: 21.871\n",
      "[225] loss: 21.862\n",
      "[226] loss: 21.857\n",
      "[227] loss: 21.851\n",
      "[228] loss: 21.846\n",
      "[229] loss: 21.840\n",
      "[230] loss: 21.835\n",
      "[231] loss: 21.829\n",
      "[232] loss: 21.822\n",
      "[233] loss: 21.814\n",
      "[234] loss: 21.807\n",
      "[235] loss: 21.803\n",
      "[236] loss: 21.797\n",
      "[237] loss: 21.791\n",
      "[238] loss: 21.785\n",
      "[239] loss: 21.778\n",
      "[240] loss: 21.771\n",
      "[241] loss: 21.764\n",
      "[242] loss: 21.759\n",
      "[243] loss: 21.753\n",
      "[244] loss: 21.747\n",
      "[245] loss: 21.740\n",
      "[246] loss: 21.735\n",
      "[247] loss: 21.729\n",
      "[248] loss: 21.724\n",
      "[249] loss: 21.716\n",
      "[250] loss: 21.710\n",
      "[251] loss: 21.704\n",
      "[252] loss: 21.699\n",
      "[253] loss: 21.691\n",
      "[254] loss: 21.686\n",
      "[255] loss: 21.680\n",
      "[256] loss: 21.673\n",
      "[257] loss: 21.670\n",
      "[258] loss: 21.667\n",
      "[259] loss: 21.658\n",
      "[260] loss: 21.654\n",
      "[261] loss: 21.648\n",
      "[262] loss: 21.638\n",
      "[263] loss: 21.632\n",
      "[264] loss: 21.623\n",
      "[265] loss: 21.619\n",
      "[266] loss: 21.613\n",
      "[267] loss: 21.608\n",
      "[268] loss: 21.602\n",
      "[269] loss: 21.594\n",
      "[270] loss: 21.585\n",
      "[271] loss: 21.579\n",
      "[272] loss: 21.572\n",
      "[273] loss: 21.566\n",
      "[274] loss: 21.561\n",
      "[275] loss: 21.557\n",
      "[276] loss: 21.550\n",
      "[277] loss: 21.546\n",
      "[278] loss: 21.538\n",
      "[279] loss: 21.532\n",
      "[280] loss: 21.526\n",
      "[281] loss: 21.521\n",
      "[282] loss: 21.516\n",
      "[283] loss: 21.511\n",
      "[284] loss: 21.503\n",
      "[285] loss: 21.495\n",
      "[286] loss: 21.488\n",
      "[287] loss: 21.481\n",
      "[288] loss: 21.475\n",
      "[289] loss: 21.467\n",
      "[290] loss: 21.463\n",
      "[291] loss: 21.456\n",
      "[292] loss: 21.450\n",
      "[293] loss: 21.445\n",
      "[294] loss: 21.436\n",
      "[295] loss: 21.430\n",
      "[296] loss: 21.424\n",
      "[297] loss: 21.419\n",
      "[298] loss: 21.413\n",
      "[299] loss: 21.405\n",
      "[300] loss: 21.396\n",
      "[301] loss: 21.389\n",
      "[302] loss: 21.383\n",
      "[303] loss: 21.377\n",
      "[304] loss: 21.372\n",
      "[305] loss: 21.363\n",
      "[306] loss: 21.357\n",
      "[307] loss: 21.350\n",
      "[308] loss: 21.344\n",
      "[309] loss: 21.335\n",
      "[310] loss: 21.329\n",
      "[311] loss: 21.320\n",
      "[312] loss: 21.314\n",
      "[313] loss: 21.308\n",
      "[314] loss: 21.302\n",
      "[315] loss: 21.296\n",
      "[316] loss: 21.290\n",
      "[317] loss: 21.283\n",
      "[318] loss: 21.278\n",
      "[319] loss: 21.272\n",
      "[320] loss: 21.266\n",
      "[321] loss: 21.259\n",
      "[322] loss: 21.253\n",
      "[323] loss: 21.242\n",
      "[324] loss: 21.236\n",
      "[325] loss: 21.230\n",
      "[326] loss: 21.223\n",
      "[327] loss: 21.217\n",
      "[328] loss: 21.208\n",
      "[329] loss: 21.201\n",
      "[330] loss: 21.194\n",
      "[331] loss: 21.188\n",
      "[332] loss: 21.181\n",
      "[333] loss: 21.175\n",
      "[334] loss: 21.169\n",
      "[335] loss: 21.161\n",
      "[336] loss: 21.154\n",
      "[337] loss: 21.146\n",
      "[338] loss: 21.140\n",
      "[339] loss: 21.133\n",
      "[340] loss: 21.128\n",
      "[341] loss: 21.121\n",
      "[342] loss: 21.114\n",
      "[343] loss: 21.108\n",
      "[344] loss: 21.100\n",
      "[345] loss: 21.094\n",
      "[346] loss: 21.087\n",
      "[347] loss: 21.079\n",
      "[348] loss: 21.072\n",
      "[349] loss: 21.065\n",
      "[350] loss: 21.060\n",
      "[351] loss: 21.054\n",
      "[352] loss: 21.049\n",
      "[353] loss: 21.043\n",
      "[354] loss: 21.038\n",
      "[355] loss: 21.034\n",
      "[356] loss: 21.026\n",
      "[357] loss: 21.018\n",
      "[358] loss: 21.011\n",
      "[359] loss: 21.005\n",
      "[360] loss: 20.998\n",
      "[361] loss: 20.991\n",
      "[362] loss: 20.988\n",
      "[363] loss: 20.983\n",
      "[364] loss: 20.978\n",
      "[365] loss: 20.969\n",
      "[366] loss: 20.961\n",
      "[367] loss: 20.955\n",
      "[368] loss: 20.949\n",
      "[369] loss: 20.946\n",
      "[370] loss: 20.941\n",
      "[371] loss: 20.935\n",
      "[372] loss: 20.930\n",
      "[373] loss: 20.923\n",
      "[374] loss: 20.917\n",
      "[375] loss: 20.909\n",
      "[376] loss: 20.906\n",
      "[377] loss: 20.897\n",
      "[378] loss: 20.892\n",
      "[379] loss: 20.888\n",
      "[380] loss: 20.882\n",
      "[381] loss: 20.877\n",
      "[382] loss: 20.871\n",
      "[383] loss: 20.867\n",
      "[384] loss: 20.861\n",
      "[385] loss: 20.858\n",
      "[386] loss: 20.854\n",
      "[387] loss: 20.848\n",
      "[388] loss: 20.845\n",
      "[389] loss: 20.837\n",
      "[390] loss: 20.834\n",
      "[391] loss: 20.830\n",
      "[392] loss: 20.825\n",
      "[393] loss: 20.822\n",
      "[394] loss: 20.820\n",
      "[395] loss: 20.816\n",
      "[396] loss: 20.811\n",
      "[397] loss: 20.805\n",
      "[398] loss: 20.802\n",
      "[399] loss: 20.797\n",
      "[400] loss: 20.795\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_temperature_data(all_cities_df)\n",
    "X_train.shape\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_train.toarray().astype('float32')), \n",
    "    torch.from_numpy(y_train.to_numpy().astype('float32')).unsqueeze(1)\n",
    ")\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "net = P1_Net(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(X_train.shape[1], 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,1)\n",
    "    )\n",
    ")\n",
    "print(net)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "do_train(net, data_loader, optimizer, loss, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "437394dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* train\n",
      "\t* Mean absolute error: 4.862332956343701\n",
      "\t* Fraction of predictions with absolute error <= 2: 0.23249602543720194\n",
      "* test\n",
      "\t* Mean absolute error: 5.384959335249013\n",
      "\t* Fraction of predictions with absolute error <= 2: 0.19872397250111284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "net.eval()\n",
    "\n",
    "def valid_in_margin(y1, y2, margin):\n",
    "    return 1 - np.count_nonzero(abs(y1-y2)>margin) / len(y1)\n",
    "print(\"* train\")\n",
    "with torch.no_grad():\n",
    "    y_pred1 = net(torch.from_numpy(X_train.toarray().astype('float32'))).numpy()\n",
    "    y_pred1 = np.squeeze(y_pred1)\n",
    "\n",
    "mae = mean_absolute_error(y_train, y_pred1)\n",
    "print(f\"\\t* Mean absolute error: {mae}\")\n",
    "margin = 2\n",
    "fraction = valid_in_margin(y_pred1, y_train, margin)\n",
    "print(f\"\\t* Fraction of predictions with absolute error <= {margin}: {fraction}\")\n",
    "\n",
    "print(\"* test\")\n",
    "with torch.no_grad():\n",
    "    y_pred2 = net(torch.from_numpy(X_test.toarray().astype('float32'))).numpy()\n",
    "    y_pred2 = np.squeeze(y_pred2)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "print(f\"\\t* Mean absolute error: {mae}\")\n",
    "margin = 2\n",
    "fraction = valid_in_margin(y_pred2, y_test, margin)\n",
    "print(f\"\\t* Fraction of predictions with absolute error <= {margin}: {fraction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763252a5",
   "metadata": {},
   "source": [
    "## Przewidywanie siły wiatru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73eeb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_x_y_wind(df):\n",
    "    X = df.copy()\n",
    "    X = X.iloc[:-2]\n",
    "    y = df['strong_wind'].iloc[4:] \n",
    "\n",
    "    X.reset_index(inplace= True)\n",
    "    X['day_of_year'] = pd.to_datetime(X['date']).dt.dayofyear\n",
    "    X['month'] = pd.to_datetime(X['date']).dt.month\n",
    "    # X['day_sin'] = np.sin(X['day_of_year']/365 * 2 * np.pi)\n",
    "    # X['day_cos'] = np.cos(X['day_of_year']/365 * 2 * np.pi)\n",
    "    # X = X.drop(columns = ['day_of_year'])\n",
    "\n",
    "\n",
    "    column_names = X.columns.to_list()\n",
    "    X = pd.concat([X.iloc[:-2].reset_index(drop=True),X.iloc[1:-1].reset_index(drop=True),X.iloc[2:].reset_index(drop=True)],axis = 1)\n",
    "    X.columns = column_names + [c + '_1' for c in column_names] + [c + '_2' for c in column_names]\n",
    "    if 'city_1' in X.columns:\n",
    "        X = X.drop(columns=['city_1', 'city_2'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f10d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def get_col_transformer_wind(df):\n",
    "    return ColumnTransformer(\n",
    "        [\n",
    "            (\n",
    "                \"standarizer\",\n",
    "                StandardScaler(),\n",
    "                gen_col_set(\n",
    "                    [\"humidity\", \"pressure\", \"temperature\", \"wind_speed\", \"wind_direction\", \"day_of_year\"]\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"one_hot_encoder\", \n",
    "                OneHotEncoder(), \n",
    "                gen_col_set(\n",
    "                    [\"description\", \"month\"]\n",
    "                ) + (['city'] if 'city' in df.columns else [])\n",
    "            ),\n",
    "            ('pass', 'passthrough', gen_col_set(['strong_wind'])),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "def prepare_wind_data(df):\n",
    "    groups = df.groupby('city')[df.columns].apply(extract_x_y_wind)\n",
    "    X_list = [group[0] for group in groups]\n",
    "    y_list = [group[1] for group in groups]\n",
    "    X, y = pd.concat(X_list), pd.concat(y_list)\n",
    "    X['y'] = y.reset_index(drop=True)\n",
    "    X = X.sort_values('date',kind='stable')\n",
    "    y = X['y']\n",
    "    X = X.drop(columns=['date','date_1','date_2', 'y'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size = 0.3, random_state = 0,shuffle=False)\n",
    "    ct = get_col_transformer_wind(X_train)\n",
    "    X_train = ct.fit_transform(X_train)\n",
    "    X_test = ct.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d139c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47175, 153)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_wind_data(all_cities_df)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e1b5636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1_Net(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=153, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=2, bias=True)\n",
      "    (7): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "[1] loss: 0.709\n",
      "[2] loss: 0.692\n",
      "[3] loss: 0.665\n",
      "[4] loss: 0.633\n",
      "[5] loss: 0.619\n",
      "[6] loss: 0.613\n",
      "[7] loss: 0.608\n",
      "[8] loss: 0.604\n",
      "[9] loss: 0.600\n",
      "[10] loss: 0.598\n",
      "[11] loss: 0.593\n",
      "[12] loss: 0.590\n",
      "[13] loss: 0.587\n",
      "[14] loss: 0.582\n",
      "[15] loss: 0.582\n",
      "[16] loss: 0.579\n",
      "[17] loss: 0.583\n",
      "[18] loss: 0.575\n",
      "[19] loss: 0.572\n",
      "[20] loss: 0.565\n",
      "[21] loss: 0.561\n",
      "[22] loss: 0.561\n",
      "[23] loss: 0.560\n",
      "[24] loss: 0.558\n",
      "[25] loss: 0.556\n",
      "[26] loss: 0.549\n",
      "[27] loss: 0.554\n",
      "[28] loss: 0.543\n",
      "[29] loss: 0.542\n",
      "[30] loss: 0.537\n",
      "[31] loss: 0.542\n",
      "[32] loss: 0.538\n",
      "[33] loss: 0.529\n",
      "[34] loss: 0.521\n",
      "[35] loss: 0.527\n",
      "[36] loss: 0.527\n",
      "[37] loss: 0.533\n",
      "[38] loss: 0.518\n",
      "[39] loss: 0.518\n",
      "[40] loss: 0.511\n",
      "[41] loss: 0.506\n",
      "[42] loss: 0.504\n",
      "[43] loss: 0.513\n",
      "[44] loss: 0.509\n",
      "[45] loss: 0.502\n",
      "[46] loss: 0.496\n",
      "[47] loss: 0.495\n",
      "[48] loss: 0.494\n",
      "[49] loss: 0.497\n",
      "[50] loss: 0.497\n",
      "[51] loss: 0.494\n",
      "[52] loss: 0.489\n",
      "[53] loss: 0.486\n",
      "[54] loss: 0.482\n",
      "[55] loss: 0.476\n",
      "[56] loss: 0.476\n",
      "[57] loss: 0.485\n",
      "[58] loss: 0.476\n",
      "[59] loss: 0.473\n",
      "[60] loss: 0.469\n",
      "[61] loss: 0.466\n",
      "[62] loss: 0.462\n",
      "[63] loss: 0.460\n",
      "[64] loss: 0.462\n",
      "[65] loss: 0.471\n",
      "[66] loss: 0.476\n",
      "[67] loss: 0.462\n",
      "[68] loss: 0.453\n",
      "[69] loss: 0.449\n",
      "[70] loss: 0.447\n",
      "[71] loss: 0.445\n",
      "[72] loss: 0.443\n",
      "[73] loss: 0.442\n",
      "[74] loss: 0.441\n",
      "[75] loss: 0.440\n",
      "[76] loss: 0.447\n",
      "[77] loss: 0.454\n",
      "[78] loss: 0.461\n",
      "[79] loss: 0.450\n",
      "[80] loss: 0.445\n",
      "[81] loss: 0.445\n",
      "[82] loss: 0.445\n",
      "[83] loss: 0.439\n",
      "[84] loss: 0.437\n",
      "[85] loss: 0.434\n",
      "[86] loss: 0.434\n",
      "[87] loss: 0.435\n",
      "[88] loss: 0.439\n",
      "[89] loss: 0.438\n",
      "[90] loss: 0.436\n",
      "[91] loss: 0.433\n",
      "[92] loss: 0.430\n",
      "[93] loss: 0.432\n",
      "[94] loss: 0.430\n",
      "[95] loss: 0.433\n",
      "[96] loss: 0.432\n",
      "[97] loss: 0.432\n",
      "[98] loss: 0.428\n",
      "[99] loss: 0.431\n",
      "[100] loss: 0.427\n",
      "[101] loss: 0.426\n",
      "[102] loss: 0.426\n",
      "[103] loss: 0.425\n",
      "[104] loss: 0.427\n",
      "[105] loss: 0.451\n",
      "[106] loss: 0.447\n",
      "[107] loss: 0.431\n",
      "[108] loss: 0.431\n",
      "[109] loss: 0.426\n",
      "[110] loss: 0.424\n",
      "[111] loss: 0.424\n",
      "[112] loss: 0.423\n",
      "[113] loss: 0.420\n",
      "[114] loss: 0.422\n",
      "[115] loss: 0.422\n",
      "[116] loss: 0.421\n",
      "[117] loss: 0.420\n",
      "[118] loss: 0.422\n",
      "[119] loss: 0.423\n",
      "[120] loss: 0.424\n",
      "[121] loss: 0.420\n",
      "[122] loss: 0.419\n",
      "[123] loss: 0.419\n",
      "[124] loss: 0.419\n",
      "[125] loss: 0.419\n",
      "[126] loss: 0.423\n",
      "[127] loss: 0.423\n",
      "[128] loss: 0.426\n",
      "[129] loss: 0.427\n",
      "[130] loss: 0.424\n",
      "[131] loss: 0.423\n",
      "[132] loss: 0.423\n",
      "[133] loss: 0.420\n",
      "[134] loss: 0.427\n",
      "[135] loss: 0.427\n",
      "[136] loss: 0.420\n",
      "[137] loss: 0.419\n",
      "[138] loss: 0.416\n",
      "[139] loss: 0.416\n",
      "[140] loss: 0.415\n",
      "[141] loss: 0.416\n",
      "[142] loss: 0.417\n",
      "[143] loss: 0.417\n",
      "[144] loss: 0.418\n",
      "[145] loss: 0.418\n",
      "[146] loss: 0.417\n",
      "[147] loss: 0.417\n",
      "[148] loss: 0.420\n",
      "[149] loss: 0.416\n",
      "[150] loss: 0.417\n",
      "[151] loss: 0.415\n",
      "[152] loss: 0.416\n",
      "[153] loss: 0.413\n",
      "[154] loss: 0.412\n",
      "[155] loss: 0.413\n",
      "[156] loss: 0.413\n",
      "[157] loss: 0.414\n",
      "[158] loss: 0.417\n",
      "[159] loss: 0.418\n",
      "[160] loss: 0.417\n",
      "[161] loss: 0.415\n",
      "[162] loss: 0.415\n",
      "[163] loss: 0.413\n",
      "[164] loss: 0.415\n",
      "[165] loss: 0.418\n",
      "[166] loss: 0.413\n",
      "[167] loss: 0.413\n",
      "[168] loss: 0.415\n",
      "[169] loss: 0.414\n",
      "[170] loss: 0.412\n",
      "[171] loss: 0.415\n",
      "[172] loss: 0.413\n",
      "[173] loss: 0.414\n",
      "[174] loss: 0.413\n",
      "[175] loss: 0.413\n",
      "[176] loss: 0.411\n",
      "[177] loss: 0.413\n",
      "[178] loss: 0.413\n",
      "[179] loss: 0.413\n",
      "[180] loss: 0.413\n",
      "[181] loss: 0.415\n",
      "[182] loss: 0.414\n",
      "[183] loss: 0.415\n",
      "[184] loss: 0.412\n",
      "[185] loss: 0.411\n",
      "[186] loss: 0.410\n",
      "[187] loss: 0.409\n",
      "[188] loss: 0.410\n",
      "[189] loss: 0.410\n",
      "[190] loss: 0.410\n",
      "[191] loss: 0.410\n",
      "[192] loss: 0.408\n",
      "[193] loss: 0.409\n",
      "[194] loss: 0.410\n",
      "[195] loss: 0.409\n",
      "[196] loss: 0.410\n",
      "[197] loss: 0.410\n",
      "[198] loss: 0.410\n",
      "[199] loss: 0.411\n",
      "[200] loss: 0.412\n"
     ]
    }
   ],
   "source": [
    "y_0 = np.array([1 if i==0 else 0 for i in y_train])\n",
    "y_t = np.concatenate([np.expand_dims(y_0, axis=1),np.expand_dims(y_train, axis=1)],axis=1)\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_train.toarray().astype('float32')), \n",
    "    torch.from_numpy(y_t.astype('float32'))\n",
    ")\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "net = P1_Net(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(X_train.shape[1], 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256,256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256,256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256,2),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    ")\n",
    "print(net)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "do_train(net, data_loader, optimizer, loss, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efafd146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* train\n",
      "\t* ROC_AUC: 0.8958818590728733\n",
      "* test\n",
      "\t* ROC_AUC: 0.6315882784791245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "net.eval()\n",
    "print(\"* train\")\n",
    "with torch.no_grad():\n",
    "    y_pred1 = net(torch.from_numpy(X_train.toarray().astype('float32')))[:,1]\n",
    "print(f\"\\t* ROC_AUC: {roc_auc_score(y_train, y_pred1)}\")\n",
    "\n",
    "print(\"* test\")\n",
    "with torch.no_grad():\n",
    "    y_pred1 = net(torch.from_numpy(X_test.toarray().astype('float32')))[:,1]\n",
    "print(f\"\\t* ROC_AUC: {roc_auc_score(y_test, y_pred1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa66eeb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1_Net(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=153, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "    (4): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "[1] loss: 0.696\n",
      "[2] loss: 0.654\n",
      "[3] loss: 0.638\n",
      "[4] loss: 0.629\n",
      "[5] loss: 0.622\n",
      "[6] loss: 0.617\n",
      "[7] loss: 0.612\n",
      "[8] loss: 0.610\n",
      "[9] loss: 0.607\n",
      "[10] loss: 0.605\n",
      "[11] loss: 0.604\n",
      "[12] loss: 0.602\n",
      "[13] loss: 0.601\n",
      "[14] loss: 0.600\n",
      "[15] loss: 0.599\n",
      "[16] loss: 0.597\n",
      "[17] loss: 0.597\n",
      "[18] loss: 0.595\n",
      "[19] loss: 0.595\n",
      "[20] loss: 0.593\n",
      "[21] loss: 0.592\n",
      "[22] loss: 0.592\n",
      "[23] loss: 0.590\n",
      "[24] loss: 0.590\n",
      "[25] loss: 0.587\n",
      "[26] loss: 0.587\n",
      "[27] loss: 0.586\n",
      "[28] loss: 0.585\n",
      "[29] loss: 0.584\n",
      "[30] loss: 0.583\n",
      "[31] loss: 0.582\n",
      "[32] loss: 0.581\n",
      "[33] loss: 0.580\n",
      "[34] loss: 0.579\n",
      "[35] loss: 0.579\n",
      "[36] loss: 0.578\n",
      "[37] loss: 0.576\n",
      "[38] loss: 0.576\n",
      "[39] loss: 0.575\n",
      "[40] loss: 0.574\n",
      "[41] loss: 0.574\n",
      "[42] loss: 0.572\n",
      "[43] loss: 0.572\n",
      "[44] loss: 0.571\n",
      "[45] loss: 0.570\n",
      "[46] loss: 0.570\n",
      "[47] loss: 0.568\n",
      "[48] loss: 0.567\n",
      "[49] loss: 0.566\n",
      "[50] loss: 0.567\n",
      "[51] loss: 0.566\n",
      "[52] loss: 0.565\n",
      "[53] loss: 0.563\n",
      "[54] loss: 0.563\n",
      "[55] loss: 0.563\n",
      "[56] loss: 0.562\n",
      "[57] loss: 0.561\n",
      "[58] loss: 0.559\n",
      "[59] loss: 0.560\n",
      "[60] loss: 0.559\n",
      "[61] loss: 0.559\n",
      "[62] loss: 0.558\n",
      "[63] loss: 0.558\n",
      "[64] loss: 0.557\n",
      "[65] loss: 0.556\n",
      "[66] loss: 0.556\n",
      "[67] loss: 0.555\n",
      "[68] loss: 0.554\n",
      "[69] loss: 0.554\n",
      "[70] loss: 0.554\n",
      "[71] loss: 0.552\n",
      "[72] loss: 0.553\n",
      "[73] loss: 0.552\n",
      "[74] loss: 0.552\n",
      "[75] loss: 0.550\n",
      "[76] loss: 0.551\n",
      "[77] loss: 0.550\n",
      "[78] loss: 0.550\n",
      "[79] loss: 0.549\n",
      "[80] loss: 0.549\n",
      "[81] loss: 0.548\n",
      "[82] loss: 0.547\n",
      "[83] loss: 0.547\n",
      "[84] loss: 0.547\n",
      "[85] loss: 0.548\n",
      "[86] loss: 0.545\n",
      "[87] loss: 0.544\n",
      "[88] loss: 0.544\n",
      "[89] loss: 0.545\n",
      "[90] loss: 0.543\n",
      "[91] loss: 0.546\n",
      "[92] loss: 0.542\n",
      "[93] loss: 0.543\n",
      "[94] loss: 0.542\n",
      "[95] loss: 0.542\n",
      "[96] loss: 0.541\n",
      "[97] loss: 0.541\n",
      "[98] loss: 0.542\n",
      "[99] loss: 0.540\n",
      "[100] loss: 0.541\n",
      "[101] loss: 0.539\n",
      "[102] loss: 0.538\n",
      "[103] loss: 0.540\n",
      "[104] loss: 0.538\n",
      "[105] loss: 0.537\n",
      "[106] loss: 0.538\n",
      "[107] loss: 0.537\n",
      "[108] loss: 0.537\n",
      "[109] loss: 0.538\n",
      "[110] loss: 0.536\n",
      "[111] loss: 0.535\n",
      "[112] loss: 0.534\n",
      "[113] loss: 0.534\n",
      "[114] loss: 0.534\n",
      "[115] loss: 0.535\n",
      "[116] loss: 0.534\n",
      "[117] loss: 0.534\n",
      "[118] loss: 0.533\n",
      "[119] loss: 0.533\n",
      "[120] loss: 0.533\n",
      "[121] loss: 0.533\n",
      "[122] loss: 0.531\n",
      "[123] loss: 0.531\n",
      "[124] loss: 0.533\n",
      "[125] loss: 0.532\n",
      "[126] loss: 0.531\n",
      "[127] loss: 0.531\n",
      "[128] loss: 0.531\n",
      "[129] loss: 0.530\n",
      "[130] loss: 0.531\n",
      "[131] loss: 0.530\n",
      "[132] loss: 0.529\n",
      "[133] loss: 0.529\n",
      "[134] loss: 0.529\n",
      "[135] loss: 0.529\n",
      "[136] loss: 0.529\n",
      "[137] loss: 0.528\n",
      "[138] loss: 0.527\n",
      "[139] loss: 0.528\n",
      "[140] loss: 0.526\n",
      "[141] loss: 0.526\n",
      "[142] loss: 0.527\n",
      "[143] loss: 0.526\n",
      "[144] loss: 0.525\n",
      "[145] loss: 0.525\n",
      "[146] loss: 0.525\n",
      "[147] loss: 0.526\n",
      "[148] loss: 0.525\n",
      "[149] loss: 0.524\n",
      "[150] loss: 0.525\n",
      "[151] loss: 0.524\n",
      "[152] loss: 0.524\n",
      "[153] loss: 0.523\n",
      "[154] loss: 0.524\n",
      "[155] loss: 0.522\n",
      "[156] loss: 0.524\n",
      "[157] loss: 0.524\n",
      "[158] loss: 0.522\n",
      "[159] loss: 0.523\n",
      "[160] loss: 0.523\n",
      "[161] loss: 0.522\n",
      "[162] loss: 0.522\n",
      "[163] loss: 0.521\n",
      "[164] loss: 0.522\n",
      "[165] loss: 0.520\n",
      "[166] loss: 0.522\n",
      "[167] loss: 0.521\n",
      "[168] loss: 0.521\n",
      "[169] loss: 0.520\n",
      "[170] loss: 0.520\n",
      "[171] loss: 0.519\n",
      "[172] loss: 0.521\n",
      "[173] loss: 0.521\n",
      "[174] loss: 0.520\n",
      "[175] loss: 0.519\n",
      "[176] loss: 0.519\n",
      "[177] loss: 0.519\n",
      "[178] loss: 0.520\n",
      "[179] loss: 0.519\n",
      "[180] loss: 0.518\n",
      "[181] loss: 0.518\n",
      "[182] loss: 0.518\n",
      "[183] loss: 0.517\n",
      "[184] loss: 0.518\n",
      "[185] loss: 0.518\n",
      "[186] loss: 0.517\n",
      "[187] loss: 0.516\n",
      "[188] loss: 0.518\n",
      "[189] loss: 0.518\n",
      "[190] loss: 0.516\n",
      "[191] loss: 0.518\n",
      "[192] loss: 0.517\n",
      "[193] loss: 0.516\n",
      "[194] loss: 0.515\n",
      "[195] loss: 0.516\n",
      "[196] loss: 0.517\n",
      "[197] loss: 0.515\n",
      "[198] loss: 0.515\n",
      "[199] loss: 0.515\n",
      "[200] loss: 0.514\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_wind_data(all_cities_df)\n",
    "X_train.shape\n",
    "y_0 = np.array([1 if i==0 else 0 for i in y_train])\n",
    "y_t = np.concatenate([np.expand_dims(y_0, axis=1),np.expand_dims(y_train, axis=1)],axis=1)\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_train.toarray().astype('float32')), \n",
    "    torch.from_numpy(y_t.astype('float32'))\n",
    ")\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "net = P1_Net(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(X_train.shape[1], 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(256,2),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    ")\n",
    "print(net)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "do_train(net, data_loader, optimizer, loss, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f87ad4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* train\n",
      "\t* ROC_AUC: 0.8472582955958052\n",
      "* test\n",
      "\t* ROC_AUC: 0.6839878636916268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "net.eval()\n",
    "print(\"* train\")\n",
    "with torch.no_grad():\n",
    "    y_pred1 = net(torch.from_numpy(X_train.toarray().astype('float32')))[:,1]\n",
    "print(f\"\\t* ROC_AUC: {roc_auc_score(y_train, y_pred1)}\")\n",
    "\n",
    "print(\"* test\")\n",
    "with torch.no_grad():\n",
    "    y_pred1 = net(torch.from_numpy(X_test.toarray().astype('float32')))[:,1]\n",
    "print(f\"\\t* ROC_AUC: {roc_auc_score(y_test, y_pred1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af5e8bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1_Net(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=153, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=2, bias=True)\n",
      "    (7): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "[1] loss: 0.703\n",
      "[2] loss: 0.688\n",
      "[3] loss: 0.660\n",
      "[4] loss: 0.633\n",
      "[5] loss: 0.618\n",
      "[6] loss: 0.612\n",
      "[7] loss: 0.608\n",
      "[8] loss: 0.605\n",
      "[9] loss: 0.602\n",
      "[10] loss: 0.599\n",
      "[11] loss: 0.597\n",
      "[12] loss: 0.595\n",
      "[13] loss: 0.592\n",
      "[14] loss: 0.589\n",
      "[15] loss: 0.592\n",
      "[16] loss: 0.587\n",
      "[17] loss: 0.582\n",
      "[18] loss: 0.581\n",
      "[19] loss: 0.584\n",
      "[20] loss: 0.577\n",
      "[21] loss: 0.573\n",
      "[22] loss: 0.573\n",
      "[23] loss: 0.569\n",
      "[24] loss: 0.576\n",
      "[25] loss: 0.568\n",
      "[26] loss: 0.562\n",
      "[27] loss: 0.559\n",
      "[28] loss: 0.556\n",
      "[29] loss: 0.552\n",
      "[30] loss: 0.550\n",
      "[31] loss: 0.545\n",
      "[32] loss: 0.543\n",
      "[33] loss: 0.545\n",
      "[34] loss: 0.561\n",
      "[35] loss: 0.543\n",
      "[36] loss: 0.536\n",
      "[37] loss: 0.527\n",
      "[38] loss: 0.535\n",
      "[39] loss: 0.527\n",
      "[40] loss: 0.534\n",
      "[41] loss: 0.521\n",
      "[42] loss: 0.516\n",
      "[43] loss: 0.509\n",
      "[44] loss: 0.512\n",
      "[45] loss: 0.515\n",
      "[46] loss: 0.512\n",
      "[47] loss: 0.511\n",
      "[48] loss: 0.500\n",
      "[49] loss: 0.495\n",
      "[50] loss: 0.497\n",
      "[51] loss: 0.501\n",
      "[52] loss: 0.497\n",
      "[53] loss: 0.502\n",
      "[54] loss: 0.492\n",
      "[55] loss: 0.490\n",
      "[56] loss: 0.483\n",
      "[57] loss: 0.483\n",
      "[58] loss: 0.481\n",
      "[59] loss: 0.478\n",
      "[60] loss: 0.477\n",
      "[61] loss: 0.486\n",
      "[62] loss: 0.496\n",
      "[63] loss: 0.481\n",
      "[64] loss: 0.480\n",
      "[65] loss: 0.472\n",
      "[66] loss: 0.465\n",
      "[67] loss: 0.464\n",
      "[68] loss: 0.459\n",
      "[69] loss: 0.463\n",
      "[70] loss: 0.464\n",
      "[71] loss: 0.458\n",
      "[72] loss: 0.459\n",
      "[73] loss: 0.456\n",
      "[74] loss: 0.465\n",
      "[75] loss: 0.474\n",
      "[76] loss: 0.462\n",
      "[77] loss: 0.456\n",
      "[78] loss: 0.447\n",
      "[79] loss: 0.446\n",
      "[80] loss: 0.443\n",
      "[81] loss: 0.444\n",
      "[82] loss: 0.443\n",
      "[83] loss: 0.447\n",
      "[84] loss: 0.444\n",
      "[85] loss: 0.451\n",
      "[86] loss: 0.452\n",
      "[87] loss: 0.446\n",
      "[88] loss: 0.449\n",
      "[89] loss: 0.441\n",
      "[90] loss: 0.439\n",
      "[91] loss: 0.434\n",
      "[92] loss: 0.433\n",
      "[93] loss: 0.432\n",
      "[94] loss: 0.431\n",
      "[95] loss: 0.432\n",
      "[96] loss: 0.435\n",
      "[97] loss: 0.457\n",
      "[98] loss: 0.450\n",
      "[99] loss: 0.442\n",
      "[100] loss: 0.438\n",
      "[101] loss: 0.436\n",
      "[102] loss: 0.432\n",
      "[103] loss: 0.431\n",
      "[104] loss: 0.432\n",
      "[105] loss: 0.430\n",
      "[106] loss: 0.429\n",
      "[107] loss: 0.430\n",
      "[108] loss: 0.429\n",
      "[109] loss: 0.426\n",
      "[110] loss: 0.426\n",
      "[111] loss: 0.426\n",
      "[112] loss: 0.426\n",
      "[113] loss: 0.428\n",
      "[114] loss: 0.446\n",
      "[115] loss: 0.460\n",
      "[116] loss: 0.448\n",
      "[117] loss: 0.433\n",
      "[118] loss: 0.429\n",
      "[119] loss: 0.428\n",
      "[120] loss: 0.428\n",
      "[121] loss: 0.431\n",
      "[122] loss: 0.427\n",
      "[123] loss: 0.427\n",
      "[124] loss: 0.425\n",
      "[125] loss: 0.421\n",
      "[126] loss: 0.424\n",
      "[127] loss: 0.425\n",
      "[128] loss: 0.424\n",
      "[129] loss: 0.423\n",
      "[130] loss: 0.422\n",
      "[131] loss: 0.421\n",
      "[132] loss: 0.421\n",
      "[133] loss: 0.422\n",
      "[134] loss: 0.424\n",
      "[135] loss: 0.424\n",
      "[136] loss: 0.422\n",
      "[137] loss: 0.423\n",
      "[138] loss: 0.431\n",
      "[139] loss: 0.431\n",
      "[140] loss: 0.430\n",
      "[141] loss: 0.425\n",
      "[142] loss: 0.423\n",
      "[143] loss: 0.421\n",
      "[144] loss: 0.420\n",
      "[145] loss: 0.422\n",
      "[146] loss: 0.422\n",
      "[147] loss: 0.421\n",
      "[148] loss: 0.422\n",
      "[149] loss: 0.427\n",
      "[150] loss: 0.431\n",
      "[151] loss: 0.427\n",
      "[152] loss: 0.426\n",
      "[153] loss: 0.425\n",
      "[154] loss: 0.419\n",
      "[155] loss: 0.419\n",
      "[156] loss: 0.417\n",
      "[157] loss: 0.417\n",
      "[158] loss: 0.419\n",
      "[159] loss: 0.420\n",
      "[160] loss: 0.422\n",
      "[161] loss: 0.420\n",
      "[162] loss: 0.422\n",
      "[163] loss: 0.420\n",
      "[164] loss: 0.421\n",
      "[165] loss: 0.426\n",
      "[166] loss: 0.432\n",
      "[167] loss: 0.423\n",
      "[168] loss: 0.422\n",
      "[169] loss: 0.417\n",
      "[170] loss: 0.416\n",
      "[171] loss: 0.416\n",
      "[172] loss: 0.418\n",
      "[173] loss: 0.417\n",
      "[174] loss: 0.419\n",
      "[175] loss: 0.419\n",
      "[176] loss: 0.420\n",
      "[177] loss: 0.418\n",
      "[178] loss: 0.417\n",
      "[179] loss: 0.418\n",
      "[180] loss: 0.418\n",
      "[181] loss: 0.421\n",
      "[182] loss: 0.418\n",
      "[183] loss: 0.419\n",
      "[184] loss: 0.418\n",
      "[185] loss: 0.417\n",
      "[186] loss: 0.416\n",
      "[187] loss: 0.417\n",
      "[188] loss: 0.415\n",
      "[189] loss: 0.414\n",
      "[190] loss: 0.416\n",
      "[191] loss: 0.415\n",
      "[192] loss: 0.414\n",
      "[193] loss: 0.416\n",
      "[194] loss: 0.416\n",
      "[195] loss: 0.414\n",
      "[196] loss: 0.414\n",
      "[197] loss: 0.413\n",
      "[198] loss: 0.414\n",
      "[199] loss: 0.414\n",
      "[200] loss: 0.415\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_wind_data(all_cities_df)\n",
    "X_train.shape\n",
    "y_0 = np.array([1 if i==0 else 0 for i in y_train])\n",
    "y_t = np.concatenate([np.expand_dims(y_0, axis=1),np.expand_dims(y_train, axis=1)],axis=1)\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_train.toarray().astype('float32')), \n",
    "    torch.from_numpy(y_t.astype('float32'))\n",
    ")\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "net = P1_Net(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(X_train.shape[1], 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256,128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128,64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,2),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    ")\n",
    "print(net)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "do_train(net, data_loader, optimizer, loss, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f02ebe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* train\n",
      "\t* ROC_AUC: 0.887167637953014\n",
      "* test\n",
      "\t* ROC_AUC: 0.628390203745488\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "net.eval()\n",
    "print(\"* train\")\n",
    "with torch.no_grad():\n",
    "    y_pred1 = net(torch.from_numpy(X_train.toarray().astype('float32')))[:,1]\n",
    "print(f\"\\t* ROC_AUC: {roc_auc_score(y_train, y_pred1)}\")\n",
    "\n",
    "print(\"* test\")\n",
    "with torch.no_grad():\n",
    "    y_pred1 = net(torch.from_numpy(X_test.toarray().astype('float32')))[:,1]\n",
    "print(f\"\\t* ROC_AUC: {roc_auc_score(y_test, y_pred1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93d193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
